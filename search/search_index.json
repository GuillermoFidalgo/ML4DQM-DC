{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Some utilities and example notebooks for ML4DQM/DC This repository contains example code for the ML4DQM/DC project. It was developed with the idea of training autoencoders on the per-lumisection histograms stored in dedicated DQMIO files in order to (partially) automate the DQM and/or DC process. However, it is intended to be generally useful for any ML4DQM/DC study (i.e. any subsystem, any type of histogram, any classification method). In more detail: The framework has been developed based on a number of 1D histograms related to the status of the pixel tracker. Support for 2D histograms was added later. Likewise, most of the development effort went into using an autoencoder as classification algorithm, looking at the mean-squared-difference between a histogram and its reconstruction as a measure for anomality. Support for other algorithms is also present, one just needs to define a class deriving from src/classifiers/HistogramClassifier . Working examples for NMF, PCA and a couple of other classifiers are present in the tutorials. Furthermore, the idea is to train a separate classifier on each histogram type, and then combine the output scores in some way. Using instead only one histogram type is of course also possible (examples are present in the tutorials). Combining histograms in different ways (e.g. appending them together and training an autoencoder on the result) is not explicitly supported yet, though many parts of the code should still be perfectly usable. Structure of this repository There are six important directories: dqmio , tutorials , utils , src , runswan and omsapi . The other directories in the repository contain either data or meta-information (e.g. for documentation). In more detail (in order of most likely appearance): dqmio : contains tools and scripts to read (nano)DQMIO files from DAS and put them in a more useful format. Before starting with further steps, it is recommended to generate small files with only one monitor element per file. See the README in the dqmio folder for more info. Please use the conversion into csv files for now, as this is the expected input for the downstream steps (for historical reasons). This might be made more flexible in the future. Note 1: the nanoDQMIO files are intended to be the new starting point for any ML4DQM-DC study, as of 2022 data-taking. Previously, for 2017 and 2018 data, we had large csv files stored on \\eos that basically contained the full DQMIO content in csv format. They can also still be used as input, for studies on legacy data. But in that case you also need to run a prepocessing step. See the tutorial read_and_write_data.ipynb (more info below). Note 2: The difficulties with using the nanoDQMIO files directly are: that they still contain a relatively large number of monitoring elements (even though reduced with respect to the standard DQMIO format). This makes reading and operating on these files rather slow. Furthermore, many of the tools in the downstream steps assume one monitoring element per file as an input (at least for now, might be made more flexible in the future). that the lumisections belonging to a given data-taking period are split (arbitrarily?) over multiple files. It is often more useful to have all lumisections of a data-taking period in one file. tutorials : contains a number of notebooks that can be used to get familiar with the code and its capabilities (see more info below). utils : contains a number of python files with static utility functions for general use. They are being called and used in various places throughout the code. See the tutorials for examples. src : contains a number of useful classes for this project. See the tutorials for examples of how to use them. DataLoader : class for convenient loading of histograms from the input csv files. HistStruct : a histogram container for easy handling of multiple types of histograms simultaneously and consistently. Model and ModelInterface : a model container that can hold classifiers acting on several histogram types and a combination method to provide a global per-lumisection score. classifiers : folder containing an abstract base HistogramClassifier class and derived classes representing concrete examples of histogram classification algorithms. cloudfitters : folder containing an abstract base CloudFitter class and derived classes representing concrete examples of point cloud fitting algorithms (used when working with multiple types of histograms simultaneously). runswan : contains code for a graphical interface (see more info below). omsapi : standalone API for retrieving information from OMS. Tutorials Some tutorials are located in the tutorials folder in this repository, that should help you get started with the code. They can be grouped into different steps: Put the data in a more manageable format. This step is no longer needed if you start from the (nano)DQMIO files and if you have prepared the data with the scripts in the dqmio folder of this repository. If you start from the legacy csv files however, follow these steps. The raw csv files that are (were) our common input are not very easy to work with. Therefore you would probably first want to do something similar to what's done in the notebook read_and_write_data.ipynb . See the code and inline comments in that script and the functions it refers to for more detailed explanation. Its output is one single csv file per histogram type and per year, which is often much more convenient than the original csv files (which contain all histogram types together and are split per number of lines, not per run). All other functions and notebooks presuppose this first step. Plot the data. Next, you can run plot_histograms.ipynb and plot_histograms_loop.ipynb . These notebooks should help you get a feeling of what your histogram looks like in general, and perhaps help you find some anomalies that you can use for testing. For 2D histograms, look at plot_histograms_2d.ipynb instead. Generate some artificial data for model training and testing, as exemplified in generate_data.ipynb . Train an autoencoder (or any other type of classifier). The scripts autoencoder.ipynb and autoencoder_iterative.ipynb are used to train an autoencoder on the whole dataset or a particular subset respectively. maxpull.ipynb , nmf_1d.ipynb and nmf_2.ipynb are showcases of some other classifiers (a maximum pull with respect to a reference histogram, and an NMF model in 1D and in 2D respectively). Finally, global_combined_training.ipynb shows a more complete example of training a model on several histogram types and combine the output. Graphical interface See more info in the dedicated README in the corresponding folder. Other remarks The repository contains no data files. I was planning to put some example data files in a data folder, but the files are too big for github. You can produce your own input files starting from the (new) nanoDQMIO or the (legacy) csv files as explained above. Another way to get started is to get some example files from my CERNBox Disclaimer: the whole repository is still in development stage. Feel free to contact me (at luka.lambrecht@cern.ch ) in case you found bugs or if you have other suggestions. To get the tutorial notebooks running in SWAN (preferred method): Log in to SWAN. Go to Projects. Click the cloud icon that says 'Download Project from git' Paste the following url: https://github.com/LukaLambrecht/ML4DQM-DC.git . (alternative method): Log in to SWAN. Click on the leftmost icon on the top right ('new terminal'). Navigate to where you want this repository (the starting place is your CERNBox home directory). Paste this command: git clone https://github.com/LukaLambrecht/ML4DQM-DC.git (or however you usually clone a repository). Exit the terminal. The folder should now be where you cloned it, and you can open and run the notebooks in it in SWAN. Further documentation Documentation for all the class definitions and functions in the relevant code directories can be found here . The documentation is generated automatically from annotations in the source code, so the formatting might behave oddly in some cases. Note that the website above does not include documentation for the tutorials (yet?). However, some comments in the tutorial notebooks should provide (enough?) explanation to follow along.","title":"Home"},{"location":"#some-utilities-and-example-notebooks-for-ml4dqmdc","text":"This repository contains example code for the ML4DQM/DC project. It was developed with the idea of training autoencoders on the per-lumisection histograms stored in dedicated DQMIO files in order to (partially) automate the DQM and/or DC process. However, it is intended to be generally useful for any ML4DQM/DC study (i.e. any subsystem, any type of histogram, any classification method). In more detail: The framework has been developed based on a number of 1D histograms related to the status of the pixel tracker. Support for 2D histograms was added later. Likewise, most of the development effort went into using an autoencoder as classification algorithm, looking at the mean-squared-difference between a histogram and its reconstruction as a measure for anomality. Support for other algorithms is also present, one just needs to define a class deriving from src/classifiers/HistogramClassifier . Working examples for NMF, PCA and a couple of other classifiers are present in the tutorials. Furthermore, the idea is to train a separate classifier on each histogram type, and then combine the output scores in some way. Using instead only one histogram type is of course also possible (examples are present in the tutorials). Combining histograms in different ways (e.g. appending them together and training an autoencoder on the result) is not explicitly supported yet, though many parts of the code should still be perfectly usable.","title":"Some utilities and example notebooks for ML4DQM/DC"},{"location":"#structure-of-this-repository","text":"There are six important directories: dqmio , tutorials , utils , src , runswan and omsapi . The other directories in the repository contain either data or meta-information (e.g. for documentation). In more detail (in order of most likely appearance): dqmio : contains tools and scripts to read (nano)DQMIO files from DAS and put them in a more useful format. Before starting with further steps, it is recommended to generate small files with only one monitor element per file. See the README in the dqmio folder for more info. Please use the conversion into csv files for now, as this is the expected input for the downstream steps (for historical reasons). This might be made more flexible in the future. Note 1: the nanoDQMIO files are intended to be the new starting point for any ML4DQM-DC study, as of 2022 data-taking. Previously, for 2017 and 2018 data, we had large csv files stored on \\eos that basically contained the full DQMIO content in csv format. They can also still be used as input, for studies on legacy data. But in that case you also need to run a prepocessing step. See the tutorial read_and_write_data.ipynb (more info below). Note 2: The difficulties with using the nanoDQMIO files directly are: that they still contain a relatively large number of monitoring elements (even though reduced with respect to the standard DQMIO format). This makes reading and operating on these files rather slow. Furthermore, many of the tools in the downstream steps assume one monitoring element per file as an input (at least for now, might be made more flexible in the future). that the lumisections belonging to a given data-taking period are split (arbitrarily?) over multiple files. It is often more useful to have all lumisections of a data-taking period in one file. tutorials : contains a number of notebooks that can be used to get familiar with the code and its capabilities (see more info below). utils : contains a number of python files with static utility functions for general use. They are being called and used in various places throughout the code. See the tutorials for examples. src : contains a number of useful classes for this project. See the tutorials for examples of how to use them. DataLoader : class for convenient loading of histograms from the input csv files. HistStruct : a histogram container for easy handling of multiple types of histograms simultaneously and consistently. Model and ModelInterface : a model container that can hold classifiers acting on several histogram types and a combination method to provide a global per-lumisection score. classifiers : folder containing an abstract base HistogramClassifier class and derived classes representing concrete examples of histogram classification algorithms. cloudfitters : folder containing an abstract base CloudFitter class and derived classes representing concrete examples of point cloud fitting algorithms (used when working with multiple types of histograms simultaneously). runswan : contains code for a graphical interface (see more info below). omsapi : standalone API for retrieving information from OMS.","title":"Structure of this repository"},{"location":"#tutorials","text":"Some tutorials are located in the tutorials folder in this repository, that should help you get started with the code. They can be grouped into different steps: Put the data in a more manageable format. This step is no longer needed if you start from the (nano)DQMIO files and if you have prepared the data with the scripts in the dqmio folder of this repository. If you start from the legacy csv files however, follow these steps. The raw csv files that are (were) our common input are not very easy to work with. Therefore you would probably first want to do something similar to what's done in the notebook read_and_write_data.ipynb . See the code and inline comments in that script and the functions it refers to for more detailed explanation. Its output is one single csv file per histogram type and per year, which is often much more convenient than the original csv files (which contain all histogram types together and are split per number of lines, not per run). All other functions and notebooks presuppose this first step. Plot the data. Next, you can run plot_histograms.ipynb and plot_histograms_loop.ipynb . These notebooks should help you get a feeling of what your histogram looks like in general, and perhaps help you find some anomalies that you can use for testing. For 2D histograms, look at plot_histograms_2d.ipynb instead. Generate some artificial data for model training and testing, as exemplified in generate_data.ipynb . Train an autoencoder (or any other type of classifier). The scripts autoencoder.ipynb and autoencoder_iterative.ipynb are used to train an autoencoder on the whole dataset or a particular subset respectively. maxpull.ipynb , nmf_1d.ipynb and nmf_2.ipynb are showcases of some other classifiers (a maximum pull with respect to a reference histogram, and an NMF model in 1D and in 2D respectively). Finally, global_combined_training.ipynb shows a more complete example of training a model on several histogram types and combine the output.","title":"Tutorials"},{"location":"#graphical-interface","text":"See more info in the dedicated README in the corresponding folder.","title":"Graphical interface"},{"location":"#other-remarks","text":"The repository contains no data files. I was planning to put some example data files in a data folder, but the files are too big for github. You can produce your own input files starting from the (new) nanoDQMIO or the (legacy) csv files as explained above. Another way to get started is to get some example files from my CERNBox Disclaimer: the whole repository is still in development stage. Feel free to contact me (at luka.lambrecht@cern.ch ) in case you found bugs or if you have other suggestions.","title":"Other remarks"},{"location":"#to-get-the-tutorial-notebooks-running-in-swan","text":"","title":"To get the tutorial notebooks running in SWAN"},{"location":"#preferred-method","text":"Log in to SWAN. Go to Projects. Click the cloud icon that says 'Download Project from git' Paste the following url: https://github.com/LukaLambrecht/ML4DQM-DC.git .","title":"(preferred method):"},{"location":"#alternative-method","text":"Log in to SWAN. Click on the leftmost icon on the top right ('new terminal'). Navigate to where you want this repository (the starting place is your CERNBox home directory). Paste this command: git clone https://github.com/LukaLambrecht/ML4DQM-DC.git (or however you usually clone a repository). Exit the terminal. The folder should now be where you cloned it, and you can open and run the notebooks in it in SWAN.","title":"(alternative method):"},{"location":"#further-documentation","text":"Documentation for all the class definitions and functions in the relevant code directories can be found here . The documentation is generated automatically from annotations in the source code, so the formatting might behave oddly in some cases. Note that the website above does not include documentation for the tutorials (yet?). However, some comments in the tutorial notebooks should provide (enough?) explanation to follow along.","title":"Further documentation"},{"location":"dqmio/","text":"Tools for reading (nano)DQMIO files Use cases The tools in this folder can be used to: read a DQMIO or nanoDQMIO files and extract the monitoring elements. access CMS DAS and retrieve all files in a given dataset. convert a collection of nanoDQMIO files to other formats, both interactively and in job submission on lxplus and your local T2 cluster. Where to start The scripts harvest_nanodqmio_to_*.py read one or more (nano)DQMIO files, select a single monitoring element, and write it to a different format. You can run each of these scripts with the option -h to see the available options. The script harvest_nanodqmio_submit.py is a job submission script that wraps any of the scripts above in a Condor job. Run with the option -h to see the available options. The script harvest_nanodqmio_submitmultiple.py has similar functionality to harvest_nanodqmio_submit.py but you can specify multiple monitoring elements at once that will each be written to their own output file. Run with the option -h to see the available options. The folder copydastolocal contains a few scripts to copy remote files or even entire datasets to a local directory, to be used as a backup in case the remote file reading does not work. See the README there for more info. The folder jsons contains an example json file needed as input for harvest_nanodqmio_submitmultiple.py , specifying the monitoring elements to read and their respective output files. You can put your own files with the monitoring elements of your choice in the same directory and specify the file as an input argument to harvest_nanodqmio_submitmultiple.py . The folder src contains the actual (nano)DQMIO reader class and some other tools. You would probably not need to go here unless you found a bug. The folder 'test' contains some testing notebooks that can run e.g. on SWAN. Not needed anymore. Things to keep in mind when trying to read files from DAS: You will need a valid grid certificate. Create one using voms-proxy-init --voms cms . The scripts in this folder should contain the correct export command. If you still get an error concerning X509_USER_PROXY, you can run the command export X509_USER_PROXY=path (where path should be replaced by the path to where you stored the proxy created in the previous step) and try again. Special instructions for job submission: You will need a valid grid certificate if accessing remote files via DAS (see above). Copy the proxy to a location that is accessible from the cluster nodes (e.g. somewhere in you home folder) and make sure to pass the path to it as the proxy argument to the job submission script. You might also need to set a CMSSW environment, depending on the configuration of your cluster. At least on lxplus this appears to be needed. You can do this using the cmssw argument to the job submission script.","title":"README"},{"location":"dqmio/#tools-for-reading-nanodqmio-files","text":"","title":"Tools for reading (nano)DQMIO files"},{"location":"dqmio/#use-cases","text":"The tools in this folder can be used to: read a DQMIO or nanoDQMIO files and extract the monitoring elements. access CMS DAS and retrieve all files in a given dataset. convert a collection of nanoDQMIO files to other formats, both interactively and in job submission on lxplus and your local T2 cluster.","title":"Use cases"},{"location":"dqmio/#where-to-start","text":"The scripts harvest_nanodqmio_to_*.py read one or more (nano)DQMIO files, select a single monitoring element, and write it to a different format. You can run each of these scripts with the option -h to see the available options. The script harvest_nanodqmio_submit.py is a job submission script that wraps any of the scripts above in a Condor job. Run with the option -h to see the available options. The script harvest_nanodqmio_submitmultiple.py has similar functionality to harvest_nanodqmio_submit.py but you can specify multiple monitoring elements at once that will each be written to their own output file. Run with the option -h to see the available options. The folder copydastolocal contains a few scripts to copy remote files or even entire datasets to a local directory, to be used as a backup in case the remote file reading does not work. See the README there for more info. The folder jsons contains an example json file needed as input for harvest_nanodqmio_submitmultiple.py , specifying the monitoring elements to read and their respective output files. You can put your own files with the monitoring elements of your choice in the same directory and specify the file as an input argument to harvest_nanodqmio_submitmultiple.py . The folder src contains the actual (nano)DQMIO reader class and some other tools. You would probably not need to go here unless you found a bug. The folder 'test' contains some testing notebooks that can run e.g. on SWAN. Not needed anymore.","title":"Where to start"},{"location":"dqmio/#things-to-keep-in-mind-when-trying-to-read-files-from-das","text":"You will need a valid grid certificate. Create one using voms-proxy-init --voms cms . The scripts in this folder should contain the correct export command. If you still get an error concerning X509_USER_PROXY, you can run the command export X509_USER_PROXY=path (where path should be replaced by the path to where you stored the proxy created in the previous step) and try again.","title":"Things to keep in mind when trying to read files from DAS:"},{"location":"dqmio/#special-instructions-for-job-submission","text":"You will need a valid grid certificate if accessing remote files via DAS (see above). Copy the proxy to a location that is accessible from the cluster nodes (e.g. somewhere in you home folder) and make sure to pass the path to it as the proxy argument to the job submission script. You might also need to set a CMSSW environment, depending on the configuration of your cluster. At least on lxplus this appears to be needed. You can do this using the cmssw argument to the job submission script.","title":"Special instructions for job submission:"},{"location":"dqmio/harvest_nanodqmio_submit/","text":"harvest nanodqmio submit Submitter for DQMIO conversion scripts This script wraps conversion scripts ( harvest_nanodqmio_to_*.py ) in a job. Run with python harvest_nanodqmio_submit.py -h for a list of available options.","title":"harvest_nanodqmio_submit"},{"location":"dqmio/harvest_nanodqmio_submit/#harvest-nanodqmio-submit","text":"Submitter for DQMIO conversion scripts This script wraps conversion scripts ( harvest_nanodqmio_to_*.py ) in a job. Run with python harvest_nanodqmio_submit.py -h for a list of available options.","title":"harvest nanodqmio submit"},{"location":"dqmio/harvest_nanodqmio_submitmultiple/","text":"harvest nanodqmio submitmultiple Submitter for DQMIO conversion scripts This script wraps conversion scripts ( harvest_nanodqmio_to_*.py ) in a job. The difference with respect to harvest_nanodqmio_submit.py is that this script makes it more easy to harvest multiple monitoring elements in one go (instead of modifying and resubmitting harvest_nanodqmio_submit.py sequentially). Run with python harvest_nanodqmio_submitmultiple.py -h for a list of available options.","title":"harvest_nanodqmio_submitmultiple"},{"location":"dqmio/harvest_nanodqmio_submitmultiple/#harvest-nanodqmio-submitmultiple","text":"Submitter for DQMIO conversion scripts This script wraps conversion scripts ( harvest_nanodqmio_to_*.py ) in a job. The difference with respect to harvest_nanodqmio_submit.py is that this script makes it more easy to harvest multiple monitoring elements in one go (instead of modifying and resubmitting harvest_nanodqmio_submit.py sequentially). Run with python harvest_nanodqmio_submitmultiple.py -h for a list of available options.","title":"harvest nanodqmio submitmultiple"},{"location":"dqmio/harvest_nanodqmio_to_csv/","text":"harvest nanodqmio to csv A script for reading (nano)DQMIO files and storing a ME in a CSV file format Run with python harvest_nanodqmio_to_csv.py -h for a list of available options. The output is stored in a CSV file similar to the ones for the RunII legacy campaign. The file format is targeted to be as close as possible to the RunII legacy files, with the same columns, data types and naming conventions. The only difference is that there are no duplicate columns. While this file format may be far from optimal, it has the advantage that much of the existing code was developed to run on those files, so this is implemented to at least have the option to run on new DQMIO files without any code change. It was tested that the output files from this script can indeed be read correctly by the already existing part of the framework without any code change. Note: need to do definitive check (both for 1D and 2D) with collision data in order to verify that the shapes are correct (hard to tell with cosmics...)","title":"harvest_nanodqmio_to_csv"},{"location":"dqmio/harvest_nanodqmio_to_csv/#harvest-nanodqmio-to-csv","text":"A script for reading (nano)DQMIO files and storing a ME in a CSV file format Run with python harvest_nanodqmio_to_csv.py -h for a list of available options. The output is stored in a CSV file similar to the ones for the RunII legacy campaign. The file format is targeted to be as close as possible to the RunII legacy files, with the same columns, data types and naming conventions. The only difference is that there are no duplicate columns. While this file format may be far from optimal, it has the advantage that much of the existing code was developed to run on those files, so this is implemented to at least have the option to run on new DQMIO files without any code change. It was tested that the output files from this script can indeed be read correctly by the already existing part of the framework without any code change. Note: need to do definitive check (both for 1D and 2D) with collision data in order to verify that the shapes are correct (hard to tell with cosmics...)","title":"harvest nanodqmio to csv"},{"location":"dqmio/harvest_nanodqmio_to_parquet/","text":"harvest nanodqmio to parquet A script for reading (nano)DQMIO files and storing a ME in a parquet file format Run with python harvest_nanodqmio_to_parquet.py -h for a list of available options. Very similar to harvest_nanodqmio_to_csv.py , but store the dataframe as a parquet file instead of a csv file.","title":"harvest_nanodqmio_to_parquet"},{"location":"dqmio/harvest_nanodqmio_to_parquet/#harvest-nanodqmio-to-parquet","text":"A script for reading (nano)DQMIO files and storing a ME in a parquet file format Run with python harvest_nanodqmio_to_parquet.py -h for a list of available options. Very similar to harvest_nanodqmio_to_csv.py , but store the dataframe as a parquet file instead of a csv file.","title":"harvest nanodqmio to parquet"},{"location":"dqmio/harvest_nanodqmio_to_root/","text":"harvest nanodqmio to root A script for reading (nano)DQMIO files and storing a ME in a ROOT file format Run with python harvest_nanodqmio_to_root.py -h for a list of available options. The output is stored in a plain ROOT file, containing only the raw histograms. Run and lumisection information is written to the name of the histogram within the ROOT file.","title":"harvest_nanodqmio_to_root"},{"location":"dqmio/harvest_nanodqmio_to_root/#harvest-nanodqmio-to-root","text":"A script for reading (nano)DQMIO files and storing a ME in a ROOT file format Run with python harvest_nanodqmio_to_root.py -h for a list of available options. The output is stored in a plain ROOT file, containing only the raw histograms. Run and lumisection information is written to the name of the histogram within the ROOT file.","title":"harvest nanodqmio to root"},{"location":"dqmio/copydastolocal/","text":"Tools for copying files or datasets from DAS to local To be used in case remote file access does not work or is too slow, or for debugging. The scripts in this folder can be used to copy a single file or all files in a dataset from DAS to a specified local folder. Then, some ME's can be extracted from them and put in whatever data format is preferred, using the tools in the parent folder with local file access instead of remote file access. When done, the downloaded datasets from DAS can be removed again.","title":"README"},{"location":"dqmio/copydastolocal/copy_das_to_local_file/","text":"copy das to local file Copy a file from DAS to a local area This script copies a file from DAS to a local area. Note: check the file size first, not suitable for extremely large files. Run with python copy_das_to_local_file.py -h to get a list of available command line options.","title":"copy_das_to_local_file"},{"location":"dqmio/copydastolocal/copy_das_to_local_file/#copy-das-to-local-file","text":"Copy a file from DAS to a local area This script copies a file from DAS to a local area. Note: check the file size first, not suitable for extremely large files. Run with python copy_das_to_local_file.py -h to get a list of available command line options.","title":"copy das to local file"},{"location":"dqmio/copydastolocal/copy_das_to_local_set/","text":"copy das to local set Copy an entire dataset from DAS to a local area This script copies a dataset from DAS to a local area. Note: check the size and number of files first, not suitable for large sets. Run with python copy_das_to_local_set.py -h for a list of available command line options.","title":"copy_das_to_local_set"},{"location":"dqmio/copydastolocal/copy_das_to_local_set/#copy-das-to-local-set","text":"Copy an entire dataset from DAS to a local area This script copies a dataset from DAS to a local area. Note: check the size and number of files first, not suitable for large sets. Run with python copy_das_to_local_set.py -h for a list of available command line options.","title":"copy das to local set"},{"location":"dqmio/utils/","text":"Some tools for (nanoDQMIO) file diagnostics","title":"README"},{"location":"dqmio/utils/print_dataset_files/","text":"print dataset files Get the file names belonging to a dataset on DAS Run with python print_dataset_files.py -h for a list of available options.","title":"print_dataset_files"},{"location":"dqmio/utils/print_dataset_files/#print-dataset-files","text":"Get the file names belonging to a dataset on DAS Run with python print_dataset_files.py -h for a list of available options.","title":"print dataset files"},{"location":"dqmio/utils/print_mes/","text":"print mes Print the available monitoring elements in a file Run with python print_mes.py -h for a list of available options.","title":"print_mes"},{"location":"dqmio/utils/print_mes/#print-mes","text":"Print the available monitoring elements in a file Run with python print_mes.py -h for a list of available options.","title":"print mes"},{"location":"omsapi/","text":"OMS API: retrieve information from the OMS database Collection of tools for obtaining OMS information in json-like format. Note: this functionality supersedes the older version in the omsinterface folder! References The code is based on the oms api repository here: https://gitlab.cern.ch/cmsoms/oms-api-client . The file omsapi.py in this folder is a direct copy of the omsapi/__init__.py file in that repository, as recommended by the developers to get it running on SWAN. See also these slides for further info on the setup of the app and this site for the available endpoints. How to use You will need to authenticate through an application registered with the OMS developer team. Either contact me on luka.lambrecht@cern.ch so I can send you my application ID and client secret, or create your own as explained below. Open example.ipynb for some examples. You need to import get_oms_api.py , then create an OMSAPI instance via get_oms_api() (only once, can be re-used for multiple queries) and then query the information via get_oms_data( <arguments> ) . See example.ipynb or get_oms_data.py for details. How to create a personal application for authentication You will need to register a personal application ID and client secret with the OMS developer team. See the slides linked above on how to do that (only slide 4-6 are relevant, the rest has been taken care of). You will receive an application ID and client secret (both are just string-like variables). Create a new python file in this folder called clientid.py and define two variables in there: API_CLIENT_ID = <your application ID> API_CLIENT_SECRET = <your client secret> That should be all!","title":"README"},{"location":"omsapi/#oms-api-retrieve-information-from-the-oms-database","text":"Collection of tools for obtaining OMS information in json-like format. Note: this functionality supersedes the older version in the omsinterface folder!","title":"OMS API: retrieve information from the OMS database"},{"location":"omsapi/#references","text":"The code is based on the oms api repository here: https://gitlab.cern.ch/cmsoms/oms-api-client . The file omsapi.py in this folder is a direct copy of the omsapi/__init__.py file in that repository, as recommended by the developers to get it running on SWAN. See also these slides for further info on the setup of the app and this site for the available endpoints.","title":"References"},{"location":"omsapi/#how-to-use","text":"You will need to authenticate through an application registered with the OMS developer team. Either contact me on luka.lambrecht@cern.ch so I can send you my application ID and client secret, or create your own as explained below. Open example.ipynb for some examples. You need to import get_oms_api.py , then create an OMSAPI instance via get_oms_api() (only once, can be re-used for multiple queries) and then query the information via get_oms_data( <arguments> ) . See example.ipynb or get_oms_data.py for details.","title":"How to use"},{"location":"omsapi/#how-to-create-a-personal-application-for-authentication","text":"You will need to register a personal application ID and client secret with the OMS developer team. See the slides linked above on how to do that (only slide 4-6 are relevant, the rest has been taken care of). You will receive an application ID and client secret (both are just string-like variables). Create a new python file in this folder called clientid.py and define two variables in there: API_CLIENT_ID = <your application ID> API_CLIENT_SECRET = <your client secret> That should be all!","title":"How to create a personal application for authentication"},{"location":"omsapi/get_oms_data/","text":"get oms data Functionality to call the OMS API with the correct query based on input parameters How to use? Check the readme file in this directory for the required setup! In particular, you will need an application ID and client secret to authenticate. Once this is ready, you can do the following: Import this module, for example via from get_oms_data import get_oms_api, get_oms_data, get_oms_response_attribute Create an instance of the OMS API class using omsapi = get_oms_api() This instance can be re-used for all consecutive calls to OMS, no need to recreate it for every call. Make a call to get_oms_data , where the first argument is the instance you just created. Other arguments: see the function documentation below. The returned object is a complicated dictionary containing all information. Simply print it to find out its exact structure and how to access exactly the values you need. The function get_oms_response_attribute is a small helper function to retrieve a specific attribute from this dictionary. See the notebook example.ipynb in this directory for some examples! get_oms_api full signature: def get_oms_api() comments: get an OMSAPI instance takes no input arguments, as the configuration parameters are unlikely to change very often if needed, these parameters can be changed in the file urls.py get_oms_data full signature: def get_oms_data( omsapi, api_endpoint, runnb=None, fillnb=None, extrafilters=[], extraargs={}, sort=None, attributes=[], limit_entries=1000) comments: query some data from OMS input arguments: - omsapi: an OMSAPI instance, e.g. created by get_oms_api() - api_endpoint: string, target information, e.g. 'runs' or 'lumisections' (see the readme for a link where the available endpoints are listed) - runnb: run number(s) to retrieve the info for, either integer (for single run) or tuple or list of two elements (first run and last run) (can also be None to not filter on run number but this is not recommended) - fillnb: runnb but for fill number instead of run number - extrafilters: list of extra filters (apart from run number), each filter is supposed to be a dict of the form {'attribute_name':<name>,'value':<value>,'operator':<operator>} where <name> must be a valid field name in the OMS data, <value> its value, and <operator> chosen from \"EQ\", \"NEQ\", \"LT\", \"GT\", \"LE\", \"GE\" or \"LIKE\" - extraargs: dict of custom key/value pairs to add to the query (still experimental, potentially usable for changing the granularity from 'run' to 'lumisection' for e.g. L1 trigger rates, see example.ipynb) - sort: valid field name in the OMS data by which to sort - attributes: list of valid field names in the OMS data to return (if not specified, all information is returned) - limit_entries: entry limit for output json object get_oms_response_attribute full signature: def get_oms_response_attribute( omsresponse, attribute ) comments: small helper function to retrieve a list of values for a single attribute input arguments: - omsresponse: the json-like object returned by get_oms_data - attribute: name of one of the attributes present in omsresponse","title":"get_oms_data"},{"location":"omsapi/get_oms_data/#get-oms-data","text":"Functionality to call the OMS API with the correct query based on input parameters How to use? Check the readme file in this directory for the required setup! In particular, you will need an application ID and client secret to authenticate. Once this is ready, you can do the following: Import this module, for example via from get_oms_data import get_oms_api, get_oms_data, get_oms_response_attribute Create an instance of the OMS API class using omsapi = get_oms_api() This instance can be re-used for all consecutive calls to OMS, no need to recreate it for every call. Make a call to get_oms_data , where the first argument is the instance you just created. Other arguments: see the function documentation below. The returned object is a complicated dictionary containing all information. Simply print it to find out its exact structure and how to access exactly the values you need. The function get_oms_response_attribute is a small helper function to retrieve a specific attribute from this dictionary. See the notebook example.ipynb in this directory for some examples!","title":"get oms data"},{"location":"omsapi/get_oms_data/#get95oms95api","text":"full signature: def get_oms_api() comments: get an OMSAPI instance takes no input arguments, as the configuration parameters are unlikely to change very often if needed, these parameters can be changed in the file urls.py","title":"get_oms_api"},{"location":"omsapi/get_oms_data/#get95oms95data","text":"full signature: def get_oms_data( omsapi, api_endpoint, runnb=None, fillnb=None, extrafilters=[], extraargs={}, sort=None, attributes=[], limit_entries=1000) comments: query some data from OMS input arguments: - omsapi: an OMSAPI instance, e.g. created by get_oms_api() - api_endpoint: string, target information, e.g. 'runs' or 'lumisections' (see the readme for a link where the available endpoints are listed) - runnb: run number(s) to retrieve the info for, either integer (for single run) or tuple or list of two elements (first run and last run) (can also be None to not filter on run number but this is not recommended) - fillnb: runnb but for fill number instead of run number - extrafilters: list of extra filters (apart from run number), each filter is supposed to be a dict of the form {'attribute_name':<name>,'value':<value>,'operator':<operator>} where <name> must be a valid field name in the OMS data, <value> its value, and <operator> chosen from \"EQ\", \"NEQ\", \"LT\", \"GT\", \"LE\", \"GE\" or \"LIKE\" - extraargs: dict of custom key/value pairs to add to the query (still experimental, potentially usable for changing the granularity from 'run' to 'lumisection' for e.g. L1 trigger rates, see example.ipynb) - sort: valid field name in the OMS data by which to sort - attributes: list of valid field names in the OMS data to return (if not specified, all information is returned) - limit_entries: entry limit for output json object","title":"get_oms_data"},{"location":"omsapi/get_oms_data/#get95oms95response95attribute","text":"full signature: def get_oms_response_attribute( omsresponse, attribute ) comments: small helper function to retrieve a list of values for a single attribute input arguments: - omsresponse: the json-like object returned by get_oms_data - attribute: name of one of the attributes present in omsresponse","title":"get_oms_response_attribute"},{"location":"omsapi/omsapi/","text":"omsapi OMS API class Copied from the OMS developers [class] OMSApiException comments: (no valid documentation found) [class] OMSQuery comments: (no valid documentation found) \u2937 __init__ full signature: def __init__(self, base_url, resource, verbose, cookies, oms_auth, cert_verify, retry_on_err_sec, proxies) comments: (no valid documentation found) \u2937 _attr_exists full signature: def _attr_exists(self, attr) comments: (no valid documentation found) \u2937 _load_meta full signature: def _load_meta(self) comments: (no valid documentation found) \u2937 _warn full signature: def _warn(self, message, raise_exc=False) comments: (no valid documentation found) \u2937 set_verbose full signature: def set_verbose(self, verbose) comments: (no valid documentation found) \u2937 set_validation full signature: def set_validation(self, attribute_validation) comments: (no valid documentation found) \u2937 attrs full signature: def attrs(self, attributes=None) comments: (no valid documentation found) \u2937 filters full signature: def filters(self, filters) comments: (no valid documentation found) \u2937 filter full signature: def filter(self, attribute, value, operator=\"EQ\") comments: (no valid documentation found) \u2937 clear_filter full signature: def clear_filter(self) comments: (no valid documentation found) \u2937 sort full signature: def sort(self, attribute, asc=True) comments: (no valid documentation found) \u2937 paginate full signature: def paginate(self, page=1, per_page=10) comments: (no valid documentation found) \u2937 include full signature: def include(self, key) comments: (no valid documentation found) \u2937 custom full signature: def custom(self, key, value=None) comments: (no valid documentation found) \u2937 data_query full signature: def data_query(self) comments: (no valid documentation found) \u2937 data full signature: def data(self) comments: (no valid documentation found) \u2937 meta full signature: def meta(self) comments: (no valid documentation found) \u2937 get_request full signature: def get_request(self, url, verify=False) comments: (no valid documentation found) [class] OMSAPIOAuth comments: (no valid documentation found) \u2937 __init__ full signature: def __init__(self, client_id, client_secret, audience=\"cmsoms-prod\", cert_verify=True, proxies={}, retry_on_err_sec=0) comments: (no valid documentation found) \u2937 auth_oidc full signature: def auth_oidc(self) comments: (no valid documentation found) \u2937 auth_oidc_req full signature: def auth_oidc_req(self) comments: (no valid documentation found) [class] OMSAPI comments: (no valid documentation found) \u2937 __init__ full signature: def __init__(self, api_url=\"https://cmsoms.cern.ch/agg/api\", api_version=\"v1\", verbose=True, cert_verify=True, retry_on_err_sec=0, proxies={}) comments: (no valid documentation found) \u2937 query full signature: def query(self, resource, query_validation=True) comments: (no valid documentation found) \u2937 auth_oidc full signature: def auth_oidc(self, client_id, client_secret, audience=\"cmsoms-prod\", proxies={}) comments: (no valid documentation found) \u2937 auth_krb full signature: def auth_krb(self, cookie_path=\"ssocookies.txt\") comments: (no valid documentation found) rm_file full signature: def rm_file(filename) comments: (no valid documentation found)","title":"omsapi"},{"location":"omsapi/omsapi/#omsapi","text":"OMS API class Copied from the OMS developers","title":"omsapi"},{"location":"omsapi/omsapi/#class-omsapiexception","text":"comments: (no valid documentation found)","title":"[class] OMSApiException"},{"location":"omsapi/omsapi/#class-omsquery","text":"comments: (no valid documentation found)","title":"[class] OMSQuery"},{"location":"omsapi/omsapi/#9595init9595","text":"full signature: def __init__(self, base_url, resource, verbose, cookies, oms_auth, cert_verify, retry_on_err_sec, proxies) comments: (no valid documentation found)","title":"&#10551; __init__"},{"location":"omsapi/omsapi/#95attr95exists","text":"full signature: def _attr_exists(self, attr) comments: (no valid documentation found)","title":"&#10551; _attr_exists"},{"location":"omsapi/omsapi/#95load95meta","text":"full signature: def _load_meta(self) comments: (no valid documentation found)","title":"&#10551; _load_meta"},{"location":"omsapi/omsapi/#95warn","text":"full signature: def _warn(self, message, raise_exc=False) comments: (no valid documentation found)","title":"&#10551; _warn"},{"location":"omsapi/omsapi/#set95verbose","text":"full signature: def set_verbose(self, verbose) comments: (no valid documentation found)","title":"&#10551; set_verbose"},{"location":"omsapi/omsapi/#set95validation","text":"full signature: def set_validation(self, attribute_validation) comments: (no valid documentation found)","title":"&#10551; set_validation"},{"location":"omsapi/omsapi/#attrs","text":"full signature: def attrs(self, attributes=None) comments: (no valid documentation found)","title":"&#10551; attrs"},{"location":"omsapi/omsapi/#filters","text":"full signature: def filters(self, filters) comments: (no valid documentation found)","title":"&#10551; filters"},{"location":"omsapi/omsapi/#filter","text":"full signature: def filter(self, attribute, value, operator=\"EQ\") comments: (no valid documentation found)","title":"&#10551; filter"},{"location":"omsapi/omsapi/#clear95filter","text":"full signature: def clear_filter(self) comments: (no valid documentation found)","title":"&#10551; clear_filter"},{"location":"omsapi/omsapi/#sort","text":"full signature: def sort(self, attribute, asc=True) comments: (no valid documentation found)","title":"&#10551; sort"},{"location":"omsapi/omsapi/#paginate","text":"full signature: def paginate(self, page=1, per_page=10) comments: (no valid documentation found)","title":"&#10551; paginate"},{"location":"omsapi/omsapi/#include","text":"full signature: def include(self, key) comments: (no valid documentation found)","title":"&#10551; include"},{"location":"omsapi/omsapi/#custom","text":"full signature: def custom(self, key, value=None) comments: (no valid documentation found)","title":"&#10551; custom"},{"location":"omsapi/omsapi/#data95query","text":"full signature: def data_query(self) comments: (no valid documentation found)","title":"&#10551; data_query"},{"location":"omsapi/omsapi/#data","text":"full signature: def data(self) comments: (no valid documentation found)","title":"&#10551; data"},{"location":"omsapi/omsapi/#meta","text":"full signature: def meta(self) comments: (no valid documentation found)","title":"&#10551; meta"},{"location":"omsapi/omsapi/#get95request","text":"full signature: def get_request(self, url, verify=False) comments: (no valid documentation found)","title":"&#10551; get_request"},{"location":"omsapi/omsapi/#class-omsapioauth","text":"comments: (no valid documentation found)","title":"[class] OMSAPIOAuth"},{"location":"omsapi/omsapi/#9595init9595_1","text":"full signature: def __init__(self, client_id, client_secret, audience=\"cmsoms-prod\", cert_verify=True, proxies={}, retry_on_err_sec=0) comments: (no valid documentation found)","title":"&#10551; __init__"},{"location":"omsapi/omsapi/#auth95oidc","text":"full signature: def auth_oidc(self) comments: (no valid documentation found)","title":"&#10551; auth_oidc"},{"location":"omsapi/omsapi/#auth95oidc95req","text":"full signature: def auth_oidc_req(self) comments: (no valid documentation found)","title":"&#10551; auth_oidc_req"},{"location":"omsapi/omsapi/#class-omsapi","text":"comments: (no valid documentation found)","title":"[class] OMSAPI"},{"location":"omsapi/omsapi/#9595init9595_2","text":"full signature: def __init__(self, api_url=\"https://cmsoms.cern.ch/agg/api\", api_version=\"v1\", verbose=True, cert_verify=True, retry_on_err_sec=0, proxies={}) comments: (no valid documentation found)","title":"&#10551; __init__"},{"location":"omsapi/omsapi/#query","text":"full signature: def query(self, resource, query_validation=True) comments: (no valid documentation found)","title":"&#10551; query"},{"location":"omsapi/omsapi/#auth95oidc_1","text":"full signature: def auth_oidc(self, client_id, client_secret, audience=\"cmsoms-prod\", proxies={}) comments: (no valid documentation found)","title":"&#10551; auth_oidc"},{"location":"omsapi/omsapi/#auth95krb","text":"full signature: def auth_krb(self, cookie_path=\"ssocookies.txt\") comments: (no valid documentation found)","title":"&#10551; auth_krb"},{"location":"omsapi/omsapi/#rm95file","text":"full signature: def rm_file(filename) comments: (no valid documentation found)","title":"rm_file"},{"location":"omsapi/urls/","text":"urls Definition of URLs to be used for the API queries","title":"urls"},{"location":"omsapi/urls/#urls","text":"Definition of URLs to be used for the API queries","title":"urls"},{"location":"run/","text":"Run the ML4DQM functionality using the GUI !WARNING: superseded by a newer version of the GUI running in SWAN! See the folder 'runswan' as opposed to here. The GUI here is not maintained (for now) and will not work anymore due to changes in the source code. All information below is kept for reference but is outdated. How to start the program The GUI cannot be run in SWAN, it needs to be run locally on your computer. You can either download the full repository, or a single executable file. Option 1: downloading the repository Go to the GitHub page and clone or download the repository via git. In the main project directory, avigate to the run folder and start the program via python3 gui.py . Dependencies: - You need to have python installed. The GUI will not work with python 2, it is required to use python 3. - Apart from tools within this repository, there are quite a few other python dependencies, such as matplotlib, numpy, math, pandas, etc. To do: make complete list of dependencies. Option 2: download executable file Go to this CERNBox location and download the executable file with the most recent date (format: gui_vYYYYMMDD ). Run via ./gui_<version> . Drawbacks: - The file is rather large (about 600MB) since it includes a complete python interpreter. - So far only a Linux version is available; it will not work on Windows or MacOS. Where to start? The starting screen of the GUI shows a number of buttons and text boxes. Usually one would start by loading a previously stored HistStruct (a structure containing all relevant histograms and classifiers) via the 'Load' button, or create a new one via the 'New' button. Many of the other buttons will not work if you have not loaded or created a valid HistStruct first. To create a HistStruct, the GUI expects input files in a specific format, see below. Input files Some example input files are provided on this CERNBox location . They are skimmed versions of the centrally provided csv files with monintoring elements (both 1D and 2D histograms) with a per-lumisection granularity. The central files are available on EOS: /eos/project/c/cmsml4dc/ML_2020 . How to produce your own skimmed input files from the centrally provided csv files is still under discussion. The code to do so is available in this repository (if run on SWAN, which has direct access to EOS), but not in the executable file (which is run locally). To do: update documentation when other options are available. The HistStruct object The 'HistStruct' is the central object used in this program. It contains all relevant histograms (depending on the target run, training runs and histogram types) and their respective classifiers in a structured way. Selecting subsets of histograms, e.g. for classifier training or evaluation, is managed by using masks, which you can initialize when creating the HistStruct. There are three types of masks: run masks: select lumisections belonging to a given run number. statistics masks: select lumisections with high or low statistics. Currently the criterion is simply based on the number of histogram entries divided by the number of bins. Might be extended in the future. json masks: select lumisections with a custom json file in the same format as the typical so-called golden json file. Any json file in the correct format can be uploaded, which allows full flexibility in selecting lumisections. Typical workflow After having loaded or created a new HistStruct object, you can start the training and evaluation of histogram classifiers. A typical workflow would consist of (most of) the following steps: preprocessing the histograms (i.e. rebinning, normalizing and/or cropping). See the 'Preprocessing' button. make plots of the training set, target set and other histograms that might be relevant. See the 'Plotting' button. resample the training set to increase statistics (might not be needed for some types of classifiers). See the 'Resampling' button. adding classifiers for each histogram type. See the 'Add classifiers' button. train all classifiers. See the 'Train classifiers' button. fit a probability density function to the resulting classifier scores in order to get from a per-histogram score to a per-lumisection score. See the 'Fit' button. evaluate the model. See 'Evaluate model' button. take a closer look at some lumisections. For example in the case of an autoencoder one might be interested to compare the original histogram to its autoencoder reconstruction. Or more generally one could be interested to see what histogram types are causing the high lumisection score. See 'Plot lumisection' button. Example usage Some examples on how to use the GUI are put in slides and collected on this CERNBox location","title":"README"},{"location":"run/#run-the-ml4dqm-functionality-using-the-gui","text":"!WARNING: superseded by a newer version of the GUI running in SWAN! See the folder 'runswan' as opposed to here. The GUI here is not maintained (for now) and will not work anymore due to changes in the source code. All information below is kept for reference but is outdated.","title":"Run the ML4DQM functionality using the GUI"},{"location":"run/#how-to-start-the-program","text":"The GUI cannot be run in SWAN, it needs to be run locally on your computer. You can either download the full repository, or a single executable file.","title":"How to start the program"},{"location":"run/#option-1-downloading-the-repository","text":"Go to the GitHub page and clone or download the repository via git. In the main project directory, avigate to the run folder and start the program via python3 gui.py . Dependencies: - You need to have python installed. The GUI will not work with python 2, it is required to use python 3. - Apart from tools within this repository, there are quite a few other python dependencies, such as matplotlib, numpy, math, pandas, etc. To do: make complete list of dependencies.","title":"Option 1: downloading the repository"},{"location":"run/#option-2-download-executable-file","text":"Go to this CERNBox location and download the executable file with the most recent date (format: gui_vYYYYMMDD ). Run via ./gui_<version> . Drawbacks: - The file is rather large (about 600MB) since it includes a complete python interpreter. - So far only a Linux version is available; it will not work on Windows or MacOS.","title":"Option 2: download executable file"},{"location":"run/#where-to-start","text":"The starting screen of the GUI shows a number of buttons and text boxes. Usually one would start by loading a previously stored HistStruct (a structure containing all relevant histograms and classifiers) via the 'Load' button, or create a new one via the 'New' button. Many of the other buttons will not work if you have not loaded or created a valid HistStruct first. To create a HistStruct, the GUI expects input files in a specific format, see below.","title":"Where to start?"},{"location":"run/#input-files","text":"Some example input files are provided on this CERNBox location . They are skimmed versions of the centrally provided csv files with monintoring elements (both 1D and 2D histograms) with a per-lumisection granularity. The central files are available on EOS: /eos/project/c/cmsml4dc/ML_2020 . How to produce your own skimmed input files from the centrally provided csv files is still under discussion. The code to do so is available in this repository (if run on SWAN, which has direct access to EOS), but not in the executable file (which is run locally). To do: update documentation when other options are available.","title":"Input files"},{"location":"run/#the-histstruct-object","text":"The 'HistStruct' is the central object used in this program. It contains all relevant histograms (depending on the target run, training runs and histogram types) and their respective classifiers in a structured way. Selecting subsets of histograms, e.g. for classifier training or evaluation, is managed by using masks, which you can initialize when creating the HistStruct. There are three types of masks: run masks: select lumisections belonging to a given run number. statistics masks: select lumisections with high or low statistics. Currently the criterion is simply based on the number of histogram entries divided by the number of bins. Might be extended in the future. json masks: select lumisections with a custom json file in the same format as the typical so-called golden json file. Any json file in the correct format can be uploaded, which allows full flexibility in selecting lumisections.","title":"The HistStruct object"},{"location":"run/#typical-workflow","text":"After having loaded or created a new HistStruct object, you can start the training and evaluation of histogram classifiers. A typical workflow would consist of (most of) the following steps: preprocessing the histograms (i.e. rebinning, normalizing and/or cropping). See the 'Preprocessing' button. make plots of the training set, target set and other histograms that might be relevant. See the 'Plotting' button. resample the training set to increase statistics (might not be needed for some types of classifiers). See the 'Resampling' button. adding classifiers for each histogram type. See the 'Add classifiers' button. train all classifiers. See the 'Train classifiers' button. fit a probability density function to the resulting classifier scores in order to get from a per-histogram score to a per-lumisection score. See the 'Fit' button. evaluate the model. See 'Evaluate model' button. take a closer look at some lumisections. For example in the case of an autoencoder one might be interested to compare the original histogram to its autoencoder reconstruction. Or more generally one could be interested to see what histogram types are causing the high lumisection score. See 'Plot lumisection' button.","title":"Typical workflow"},{"location":"run/#example-usage","text":"Some examples on how to use the GUI are put in slides and collected on this CERNBox location","title":"Example usage"},{"location":"runswan/","text":"Run the ML4DQM functionality using the GUI on SWAN How to start the program Simply download the repository and open the notebook nbgui.ipynb in SWAN. Downloading the repository See the README in the parent folder for more explanation. Dependencies: All of the required dependencies are installed by default on SWAN! To do: make list of optional dependencies that are not installed by default (e.g. imageio?) Where to start? The starting screen of the GUI shows a number of tabs for several actions one can perform. Usually one would start by loading a previously stored HistStruct (a structure containing all relevant histograms and classifiers) via the 'Load' tab, or create a new one via the 'New' tab. Many of the other tabs will not work if you have not loaded or created a valid HistStruct first. To create a HistStruct, the GUI expects input files in a specific format, see below. Input files Some example input files are provided on this CERNBox location . They are skimmed versions of the centrally provided (legacy) csv files with monintoring elements (both 1D and 2D histograms) with a per-lumisection granularity. The central files are available on EOS: /eos/project/c/cmsml4dc/ML_2020 . Starting from 2022 data-taking, nanoDQMIO files are supposed to be the main central input. They can however be preprocessed into small csv files fully compatible with the legacy ones. See the main README and the folder dqmio in this repository for more information. The HistStruct object The 'HistStruct' is the central object used in this program. It contains all relevant histograms (depending on the target run, training runs and histogram types) and their respective classifiers in a structured way. Selecting subsets of histograms, e.g. for classifier training or evaluation, is managed by using masks, which you can initialize when creating the HistStruct. There are three types of masks: run masks: select lumisections belonging to a given run number. statistics masks: select lumisections with high or low statistics. Currently the criterion is simply based on the number of histogram entries divided by the number of bins. Might be extended in the future. json masks: select lumisections with a custom json file in the same format as the typical so-called golden json file. Any json file in the correct format can be uploaded, which allows full flexibility in selecting lumisections. Typical workflow After having loaded or created a new HistStruct object, you can start the training and evaluation of histogram classifiers. A typical workflow would consist of (most of) the following steps: preprocessing the histograms (i.e. rebinning, normalizing and/or cropping). See the 'Preprocessing' button. make plots of the training set, target set and other histograms that might be relevant. See the 'Plotting' button. resample the training set to increase statistics (might not be needed for some types of classifiers). See the 'Resampling' button. adding classifiers for each histogram type. See the 'Add classifiers' button. train all classifiers. See the 'Train classifiers' button. fit a probability density function to the resulting classifier scores in order to get from a per-histogram score to a per-lumisection score. See the 'Fit' button. evaluate the model. See 'Evaluate model' button. take a closer look at some lumisections. For example in the case of an autoencoder one might be interested to compare the original histogram to its autoencoder reconstruction. Or more generally one could be interested to see what histogram types are causing the high lumisection score. See 'Plot lumisection' button. Example usage Some examples on how to use the GUI are put in slides and collected on this CERNBox location . To do: update!","title":"README"},{"location":"runswan/#run-the-ml4dqm-functionality-using-the-gui-on-swan","text":"","title":"Run the ML4DQM functionality using the GUI on SWAN"},{"location":"runswan/#how-to-start-the-program","text":"Simply download the repository and open the notebook nbgui.ipynb in SWAN.","title":"How to start the program"},{"location":"runswan/#downloading-the-repository","text":"See the README in the parent folder for more explanation. Dependencies: All of the required dependencies are installed by default on SWAN! To do: make list of optional dependencies that are not installed by default (e.g. imageio?)","title":"Downloading the repository"},{"location":"runswan/#where-to-start","text":"The starting screen of the GUI shows a number of tabs for several actions one can perform. Usually one would start by loading a previously stored HistStruct (a structure containing all relevant histograms and classifiers) via the 'Load' tab, or create a new one via the 'New' tab. Many of the other tabs will not work if you have not loaded or created a valid HistStruct first. To create a HistStruct, the GUI expects input files in a specific format, see below.","title":"Where to start?"},{"location":"runswan/#input-files","text":"Some example input files are provided on this CERNBox location . They are skimmed versions of the centrally provided (legacy) csv files with monintoring elements (both 1D and 2D histograms) with a per-lumisection granularity. The central files are available on EOS: /eos/project/c/cmsml4dc/ML_2020 . Starting from 2022 data-taking, nanoDQMIO files are supposed to be the main central input. They can however be preprocessed into small csv files fully compatible with the legacy ones. See the main README and the folder dqmio in this repository for more information.","title":"Input files"},{"location":"runswan/#the-histstruct-object","text":"The 'HistStruct' is the central object used in this program. It contains all relevant histograms (depending on the target run, training runs and histogram types) and their respective classifiers in a structured way. Selecting subsets of histograms, e.g. for classifier training or evaluation, is managed by using masks, which you can initialize when creating the HistStruct. There are three types of masks: run masks: select lumisections belonging to a given run number. statistics masks: select lumisections with high or low statistics. Currently the criterion is simply based on the number of histogram entries divided by the number of bins. Might be extended in the future. json masks: select lumisections with a custom json file in the same format as the typical so-called golden json file. Any json file in the correct format can be uploaded, which allows full flexibility in selecting lumisections.","title":"The HistStruct object"},{"location":"runswan/#typical-workflow","text":"After having loaded or created a new HistStruct object, you can start the training and evaluation of histogram classifiers. A typical workflow would consist of (most of) the following steps: preprocessing the histograms (i.e. rebinning, normalizing and/or cropping). See the 'Preprocessing' button. make plots of the training set, target set and other histograms that might be relevant. See the 'Plotting' button. resample the training set to increase statistics (might not be needed for some types of classifiers). See the 'Resampling' button. adding classifiers for each histogram type. See the 'Add classifiers' button. train all classifiers. See the 'Train classifiers' button. fit a probability density function to the resulting classifier scores in order to get from a per-histogram score to a per-lumisection score. See the 'Fit' button. evaluate the model. See 'Evaluate model' button. take a closer look at some lumisections. For example in the case of an autoencoder one might be interested to compare the original histogram to its autoencoder reconstruction. Or more generally one could be interested to see what histogram types are causing the high lumisection score. See 'Plot lumisection' button.","title":"Typical workflow"},{"location":"runswan/#example-usage","text":"Some examples on how to use the GUI are put in slides and collected on this CERNBox location . To do: update!","title":"Example usage"},{"location":"src/DataLoader/","text":"DataLoader Class for loading histograms from disk into a pandas dataframe. Typically, the input consists of a single file per histogram type, prepared from the nanoDQMIO format (accessed via DAS) and converted into another file format. see the tools in the 'dqmio' folder for more info on preparing the input files. Currently supported input file formats: csv parquet Example usage: from DataLoader import DataLoader dl = DataLoader() df = dl.get_dataframe_from_file( <path to input file> ) Alternatively, support is available to read the legacy per-LS csv files (deprecated approach for run-II data, before nanoDQMIO in run-III). In this case, the needed input consists of: a set of histogram names to load a specification in terms of eras or years Example usage: from DataLoader import DataLoader dl = DataLoader() csvfiles = dl.get_default_csv_files( year=<year>, dim=<histogram dimension> ) df = dl.get_dataframe_from_files( csvfiles, histnames=<histogram names> ) The output consists of a pandas dataframe containing the requested histograms. [class] DataLoader comments: (no valid documentation found) \u2937 __init__ full signature: def __init__( self ) comments: initializer initialization of valid years and eras for which legacy csv files exist (note: only relevant for legacy csv approach, else empty initializer) \u2937 check_year full signature: def check_year( self, year ) comments: check if a provided year is valid (note: only relevant for legacy csv approach) (note: internal helper function, no need to call) input arguments: - year: year in string format \u2937 check_eras full signature: def check_eras( self, eras, year ) comments: check if a list of provided eras is valid (note: only relevant for legacy csv approach) (note: internal helper function, no need to call) input arguments: - eras: list of eras in string format, e.g. ['B','C'] - year: year in string format \u2937 check_dim full signature: def check_dim( self, dim ) comments: check if a histogram dimension is valid (note: only 1D and 2D histograms are supported for now) (note: internal helper function, no need to call) \u2937 check_eos full signature: def check_eos( self ) comments: check if the /eos directory exists and is accessible (note: only relevant for legacy csv approach) (note: internal helper function, no need to call) \u2937 get_default_data_dirs full signature: def get_default_data_dirs( self, year='2017', eras=[], dim=1 ) comments: get the default data directories for the data for this project (note: only relevant for legacy csv approach) (note: internal helper function, no need to call) note: this returns the directories where the data is currently stored; might change in future reprocessings of the data, and should be extended for upcoming Run-III data. note: default directories are on the /eos file system. this function will throw an exception if it does not have access to /eos. input arguments: - year: data-taking year, should be '2017' or '2018' so far (default: 2017) - eras: list of valid eras for the given data-taking year (default: all eras) - dim: dimension of requested histograms (1 or 2) note: need to provide the dimension at this stage since the files for 1D and 2D histograms are stored in different directories. returns: a list of directories containing the legacy csv files with the requested data. \u2937 get_csv_files_in_dir full signature: def get_csv_files_in_dir( self, inputdir, sort=True ) comments: get a (optionally sorted) list of csv files in a given input directory (note: only relevant for legacy csv approach) (note: internal helper function, no need to call) input arguments: - inputdir: directory to scan for csv files - sort: boolean whether to sort the files returns: a list of csv files in the given directory. \u2937 get_csv_files_in_dirs full signature: def get_csv_files_in_dirs( self, inputdirs, sort=True ) comments: find the csv files in a set of input directories and return them in one list. (note: only relevant for legacy csv approach) (note: internal helper function, no need to call) this function simply loops over the directories given in inputdirs, calls get_csv_files_in_dir for each of them, and concatenates the results. input arguments: - list of input directories where to look for csv files - sort: see get_csv_files_in_dir returns: a list of csv files in the given directories. \u2937 get_default_csv_files full signature: def get_default_csv_files( self, year='2017', eras=[], dim=1, sort=True ) comments: read the csv files from the default directories with input data for this project (note: only relevant for legacy csv approach) note: default directories are on the /eos file system. this function will throw an exception if it has not access to /eos. input arguments: - year, eras, dim: see get_default_data_dirs! - sort: see get_csv_files_in_dir! returns: a list of csv files with the data corresponding to the provided year, eras and dimension. \u2937 get_dataframe_from_file full signature: def get_dataframe_from_file( self, dfile, histnames=[], sort=True, verbose=True ) comments: load histograms from a given file into a dataframe input arguments: - dfile: file containing the data. currently supported formats: csv, parquet. - histnames: list of histogram names to keep (default: keep all histograms present in the input file). - sort: whether to sort the dataframe by run and lumisection number (note: requires keys 'fromrun' and 'fromlumi' to be present in the dataframe). - verbose: whether to print info messages. returns: a pandas dataframe \u2937 get_dataframe_from_files full signature: def get_dataframe_from_files( self, dfiles, histnames=[], sort=True, verbose=True ) comments: load histograms from a given set of files into a single dataframe input arguments: - dfiles: list of files containing the data. currently supported formats: csv, parquet. - histnames: list of histogram names to keep (default: keep all histograms present in the input file). - sort: whether to sort the dataframe by run and lumisection number (note: requires keys 'fromrun' and 'fromlumi' to be present in the dataframe). - verbose: whether to print info messages. returns: a pandas dataframe \u2937 write_dataframe_to_file full signature: def write_dataframe_to_file( self, df, dfile, overwrite=False, verbose=True ) comments: write a dataframe to a file input arguments: - df: a pandas dataframe. - dfile: file name to write. currently supported formats: csv, parquet. - overwrite: whether to overwrite if a file with the given name already exists. - verbose: whether to print info messages.","title":"DataLoader"},{"location":"src/DataLoader/#dataloader","text":"Class for loading histograms from disk into a pandas dataframe. Typically, the input consists of a single file per histogram type, prepared from the nanoDQMIO format (accessed via DAS) and converted into another file format. see the tools in the 'dqmio' folder for more info on preparing the input files. Currently supported input file formats: csv parquet Example usage: from DataLoader import DataLoader dl = DataLoader() df = dl.get_dataframe_from_file( <path to input file> ) Alternatively, support is available to read the legacy per-LS csv files (deprecated approach for run-II data, before nanoDQMIO in run-III). In this case, the needed input consists of: a set of histogram names to load a specification in terms of eras or years Example usage: from DataLoader import DataLoader dl = DataLoader() csvfiles = dl.get_default_csv_files( year=<year>, dim=<histogram dimension> ) df = dl.get_dataframe_from_files( csvfiles, histnames=<histogram names> ) The output consists of a pandas dataframe containing the requested histograms.","title":"DataLoader"},{"location":"src/DataLoader/#class-dataloader","text":"comments: (no valid documentation found)","title":"[class] DataLoader"},{"location":"src/DataLoader/#9595init9595","text":"full signature: def __init__( self ) comments: initializer initialization of valid years and eras for which legacy csv files exist (note: only relevant for legacy csv approach, else empty initializer)","title":"&#10551; __init__"},{"location":"src/DataLoader/#check95year","text":"full signature: def check_year( self, year ) comments: check if a provided year is valid (note: only relevant for legacy csv approach) (note: internal helper function, no need to call) input arguments: - year: year in string format","title":"&#10551; check_year"},{"location":"src/DataLoader/#check95eras","text":"full signature: def check_eras( self, eras, year ) comments: check if a list of provided eras is valid (note: only relevant for legacy csv approach) (note: internal helper function, no need to call) input arguments: - eras: list of eras in string format, e.g. ['B','C'] - year: year in string format","title":"&#10551; check_eras"},{"location":"src/DataLoader/#check95dim","text":"full signature: def check_dim( self, dim ) comments: check if a histogram dimension is valid (note: only 1D and 2D histograms are supported for now) (note: internal helper function, no need to call)","title":"&#10551; check_dim"},{"location":"src/DataLoader/#check95eos","text":"full signature: def check_eos( self ) comments: check if the /eos directory exists and is accessible (note: only relevant for legacy csv approach) (note: internal helper function, no need to call)","title":"&#10551; check_eos"},{"location":"src/DataLoader/#get95default95data95dirs","text":"full signature: def get_default_data_dirs( self, year='2017', eras=[], dim=1 ) comments: get the default data directories for the data for this project (note: only relevant for legacy csv approach) (note: internal helper function, no need to call) note: this returns the directories where the data is currently stored; might change in future reprocessings of the data, and should be extended for upcoming Run-III data. note: default directories are on the /eos file system. this function will throw an exception if it does not have access to /eos. input arguments: - year: data-taking year, should be '2017' or '2018' so far (default: 2017) - eras: list of valid eras for the given data-taking year (default: all eras) - dim: dimension of requested histograms (1 or 2) note: need to provide the dimension at this stage since the files for 1D and 2D histograms are stored in different directories. returns: a list of directories containing the legacy csv files with the requested data.","title":"&#10551; get_default_data_dirs"},{"location":"src/DataLoader/#get95csv95files95in95dir","text":"full signature: def get_csv_files_in_dir( self, inputdir, sort=True ) comments: get a (optionally sorted) list of csv files in a given input directory (note: only relevant for legacy csv approach) (note: internal helper function, no need to call) input arguments: - inputdir: directory to scan for csv files - sort: boolean whether to sort the files returns: a list of csv files in the given directory.","title":"&#10551; get_csv_files_in_dir"},{"location":"src/DataLoader/#get95csv95files95in95dirs","text":"full signature: def get_csv_files_in_dirs( self, inputdirs, sort=True ) comments: find the csv files in a set of input directories and return them in one list. (note: only relevant for legacy csv approach) (note: internal helper function, no need to call) this function simply loops over the directories given in inputdirs, calls get_csv_files_in_dir for each of them, and concatenates the results. input arguments: - list of input directories where to look for csv files - sort: see get_csv_files_in_dir returns: a list of csv files in the given directories.","title":"&#10551; get_csv_files_in_dirs"},{"location":"src/DataLoader/#get95default95csv95files","text":"full signature: def get_default_csv_files( self, year='2017', eras=[], dim=1, sort=True ) comments: read the csv files from the default directories with input data for this project (note: only relevant for legacy csv approach) note: default directories are on the /eos file system. this function will throw an exception if it has not access to /eos. input arguments: - year, eras, dim: see get_default_data_dirs! - sort: see get_csv_files_in_dir! returns: a list of csv files with the data corresponding to the provided year, eras and dimension.","title":"&#10551; get_default_csv_files"},{"location":"src/DataLoader/#get95dataframe95from95file","text":"full signature: def get_dataframe_from_file( self, dfile, histnames=[], sort=True, verbose=True ) comments: load histograms from a given file into a dataframe input arguments: - dfile: file containing the data. currently supported formats: csv, parquet. - histnames: list of histogram names to keep (default: keep all histograms present in the input file). - sort: whether to sort the dataframe by run and lumisection number (note: requires keys 'fromrun' and 'fromlumi' to be present in the dataframe). - verbose: whether to print info messages. returns: a pandas dataframe","title":"&#10551; get_dataframe_from_file"},{"location":"src/DataLoader/#get95dataframe95from95files","text":"full signature: def get_dataframe_from_files( self, dfiles, histnames=[], sort=True, verbose=True ) comments: load histograms from a given set of files into a single dataframe input arguments: - dfiles: list of files containing the data. currently supported formats: csv, parquet. - histnames: list of histogram names to keep (default: keep all histograms present in the input file). - sort: whether to sort the dataframe by run and lumisection number (note: requires keys 'fromrun' and 'fromlumi' to be present in the dataframe). - verbose: whether to print info messages. returns: a pandas dataframe","title":"&#10551; get_dataframe_from_files"},{"location":"src/DataLoader/#write95dataframe95to95file","text":"full signature: def write_dataframe_to_file( self, df, dfile, overwrite=False, verbose=True ) comments: write a dataframe to a file input arguments: - df: a pandas dataframe. - dfile: file name to write. currently supported formats: csv, parquet. - overwrite: whether to overwrite if a file with the given name already exists. - verbose: whether to print info messages.","title":"&#10551; write_dataframe_to_file"},{"location":"src/HistStruct/","text":"HistStruct HistStruct: consistent treatment of multiple histogram types The HistStruct class is the main data structure used within this framework. A HistStruct object basically consists of a mutually consistent collection of numpy arrays, where each numpy array corresponds to one histogram type, with dimensions (number of histograms, number of bins). The HistStruct has functions to easily perform the following common tasks (among others): select a subset of runs and/or lumisections (e.g. using a custom or predefined json file formatted selector), prepare the data for machine learning training, with all kinds of preprocessing, evaluate classifiers (machine learning types or other), go from per-histogram scores to per-lumisection scores. When only processing a single histogram type, the HistStruct might be a bit of an overkill. One could instead choose to operate on the dataframe directly. However, especially when using multiple histogram types, the HistStruct is very handy to keep everything consistent. [class] HistStruct comments: (no valid documentation found) \u2937 __init__ full signature: def __init__( self ) comments: empty initializer, setting all containers to empty defaults a HistStruct object has the following properties: histnames: list of histogram names histograms: dict mapping histogram name to 2D numpy array of histograms (shape (nhists,nbins)) nentries: dict mapping histogram name to 1D numpy array of number of entries per histogram (same length as histograms) histranges: dict mapping histogram name to tuple with (xmin, xmax) runnbs: 1D numpy array of run numbers (same length as histograms) lsnbs: 1D numpy array of lumisection numbers (same length as histograms) masks: dict mapping name to 1D numpy array of booleans (same length as histograms) that can be used for masking exthistograms: dict of dicts similar to histograms for additional (e.g. artificially generated) histograms setnames: list of names of extended sets models: dict mapping model names to ModelInterfaces modelnames: list of model names \u2937 __str__ full signature: def __str__( self ) comments: get a printable representation of a HistStruct \u2937 save full signature: def save( self, path, save_models=False, save_classifiers=True, save_fitter=True ) comments: save a HistStruct object to a pkl file input arguments: - path where to store the file (appendix .zip is automatically appended) - save_models: a boolean whether to include the models if present in the HistStruct - save_classifiers: a boolean whether to include the classifiers if present in the ModelInterfaces - save_fitter: a boolean whether to include the fitter if present in the ModelInterfaces \u2937 load full signature: def load( self, path, load_models=True, load_classifiers=True, load_fitter=True, verbose=False ) comments: load a HistStruct object input arguments: - path to a zip file containing a HistStruct object - load_models: a boolean whether to load the models if present - load_classifiers: a boolean whether to load the classifiers if present - load_fitter: a boolean whether to load the fitter if present - verbose: boolean whether to print some information \u2937 add_dataframe full signature: def add_dataframe( self, df, cropslices=None, rebinningfactor=None, smoothinghalfwindow=None, smoothingweights=None, averagewindow=None, averageweights=None, donormalize=True ) comments: add a dataframe to a HistStruct input arguments: - df: a pandas dataframe as read from the input csv files - cropslices: list of slices (one per dimension) by which to crop the histograms see hist_utils.py / crophists for more info. - rebinningfactor: factor by which to group bins together see hist_utils.py / rebinhists for more info. - smoothinghalfwindow: half window (int for 1D, tuple for 2D) for doing smoothing of histograms - smoothingweights: weight array (1D for 1D, 2D for 2D) for smoothing of histograms see hist_utils.py / smoothhists for more info. - averagewindow: window (int or tuple) for averaging each histogram with its neighbours - averageweights: weights for averaging each histogram with its neighbours see hist_utils.py / running_average_hists for more info. - donormalize: boolean whether to normalize the histograms see hist_utils.py / normalizehists for more info. notes: - the new dataframe can contain one or multiple histogram types - the new dataframe must contain the same run and lumisection numbers (for each histogram type in it) as already present in the HistStruct, except if it is the first one to be added - alternative to adding the dataframe with the preprocessing options, one can also apply the preprocessing at a later stage using the preprocess() function with the same arguments. \u2937 add_histograms full signature: def add_histograms( self, histname, histograms, runnbs, lsnbs, nentries=None ) comments: add a set of histograms to a HistStruct input arguments: - histname: name of the histogram type to be added - histograms: a numpy array of shape (nhistograms,nbins), assumed to be of a single type - runnbs: a 1D list or array of length nhistograms containing the run number per histogram - lsnbs: a 1D list or array of length nhistograms containing the lumisection number per histogram - nentries: a 1D list or array of length nhistograms containing the number of entries per histogram notes: - must be provided explicitly since histograms might be normalized, in which case the number of entries cannot be determined from the sum of bin contents. - used for (de-)selecting histograms with sufficient statistics; if you don't need that type of selection, nentries can be left at default. - default is None, meaning all entries will be set to zero. notes: - no preprocessing is performed, this is assumed to have been done manually (if needed) before adding the histograms - runnbs and lsnbs must correspond to what is already in the current HistStruct, except if this is the first set of histogram to be added - see also add_dataframe for an alternative way of adding histograms \u2937 preprocess full signature: def preprocess( self, masknames=None, cropslices=None, rebinningfactor=None, smoothinghalfwindow=None, smoothingweights=None, averagewindow=None, averageweights=None, donormalize=False ) comments: do preprocessing input arguments: - masknames: names of masks to select histograms to which to apply the preprocessing (histograms not passing the masks are simply copied) the other input arguments are equivalent to those given in add_dataframe, but this function allows to do preprocessing after the dataframes have already been loaded note: does not work on extended histograms sets! one needs to apply preprocessing before generating extra histograms. \u2937 add_globalscores full signature: def add_globalscores( self, globalscores ) comments: add an array of global scores (one per lumisection) DEPRECATED, DO NOT USE ANYMORE input arguments: - globalscores: 1D numpy array of scores (must have same length as lumisection and run numbers) \u2937 add_extglobalscores full signature: def add_extglobalscores( self, extname, globalscores ) comments: add an array of global scores (one per lumisection) for a specified extra set of histograms in the HistStruct DEPRECATED, DO NOT USE ANYMORE input arguments: - extname: name of extra histogram set - globalscores: 1D numpy array of scores note: this function checks if all histogram types in this set contain the same number of histograms, (and that this number corresponds to the length of globalscores) else adding globalscores is meaningless \u2937 add_exthistograms full signature: def add_exthistograms( self, setname, histname, histograms, overwrite=False ) comments: add a set of extra histograms to a HistStruct these histograms are not assumed to correspond to physical run/lumisections numbers (e.g. resampled ones), and no consistency checks are done input arguments: - setname: name of the extra histogram set (you can add multiple, e.g. resampled_good, resampled_bad and/or resampled_training) - histname: name of the histogram type - histograms: a numpy array of shape (nhistograms,nbins) - overwrite: boolean whether to overwrite a set of histograms of the same name if present (default: raise exception) \u2937 add_mask full signature: def add_mask( self, name, mask ) comments: add a mask to a HistStruct input arguments: - name: a name for the mask - mask: a 1D np array of booleans with same length as number of lumisections in HistStruct \u2937 remove_mask full signature: def remove_mask( self, name ) comments: inverse operation of add_mask \u2937 add_index_mask full signature: def add_index_mask( self, name, indices ) comments: add a mask corresponding to predefined indices input arguments: - name: a name for the mask - indices: a 1D np array of integer indices, between 0 and the number of lumisections in HistStruct \u2937 add_run_mask full signature: def add_run_mask( self, name, runnb ) comments: add a mask corresponding to a given run number input arguments: - name: a name for the mask - runnb: run number \u2937 add_multirun_mask full signature: def add_multirun_mask( self, name, runnbs ) comments: add a mask corresponding to a given list of run numbers input arguments: - name: a name for the mask - runnbs: a list of run numbers \u2937 add_json_mask full signature: def add_json_mask( self, name, jsondict ) comments: add a mask corresponding to a json dict input arguments: - name: a name for the mask - jsondict: a dictionary in typical json format (see the golden json file for inspiration) all lumisections present in the jsondict will be masked True, the others False. \u2937 add_goldenjson_mask full signature: def add_goldenjson_mask( self, name ) comments: add a mask corresponding to the golden json file input arguments: - name: a name for the mask \u2937 add_dcsonjson_mask full signature: def add_dcsonjson_mask( self, name ) comments: add a mask corresponding to the DCS-bit on json file input arguments: - name: a name for the mask \u2937 add_stat_mask full signature: def add_stat_mask( self, name, histnames=None, min_entries_to_bins_ratio=-1, max_entries_to_bins_ratio=-1 ) comments: add a mask corresponding to lumisections where all histograms have statistics within given bounds input arguments: - histnames: list of histogram names to take into account for making the mask (default: all in the HistStruct) - min_entries_to_bins_ratio: number of entries divided by number of bins, lower boundary for statistics (default: no lower boundary) - max_entries_to_bins_ratio: same but upper boundary instead of lower boundary (default: no upper boundary) \u2937 add_highstat_mask full signature: def add_highstat_mask( self, name, histnames=None, entries_to_bins_ratio=100 ) comments: shorthand call to add_stat_mask with only lower boundary and no upper boundary for statistics input arguments: - entries_to_bins_ratio: number of entries divided by number of bins, lower boundary for statistics others: see add_stat_mask \u2937 pass_masks full signature: def pass_masks( self, masknames, runnbs=None, lsnbs=None ) comments: get a list of booleans of lumisections whether they pass a given set of masks input arguments: - masknames: list of mask names - runnbs: list of run numbers (default: all in histstruct) - lsnbs: list of lumisection numbers (equally long as runnbs) (default: al in histstruct) \u2937 get_masknames full signature: def get_masknames( self ) comments: return a list of all mask names in the current HistStruct \u2937 get_mask full signature: def get_mask( self, name ) comments: return a mask in the current HistStruct \u2937 get_combined_mask full signature: def get_combined_mask( self, names ) comments: get a combined (intersection) mask given multiple mask names mostly for internal use; externally you can use get_histograms( histname, <list of mask names>) directly \u2937 get_union_mask full signature: def get_union_mask( self, names ) comments: get a combined (union) mask given multiple mask names mostly for internal use \u2937 get_runnbs full signature: def get_runnbs( self, masknames=None ) comments: get the array of run numbers, optionally after masking input arguments: - masknames: list of names of masks (default: no masking, return full array) \u2937 get_runnbs_unique full signature: def get_runnbs_unique( self, masknames=None ) comments: get a list of unique run numbers \u2937 get_lsnbs full signature: def get_lsnbs( self, masknames=None ) comments: get the array of lumisection numbers, optionally after masking input arguments: - masknames: list of names of masks (default: no masking, return full array) \u2937 get_index full signature: def get_index( self, runnb, lsnb ) comments: get the index in the current HistStruct of a given run and lumisection number input arguments: - runnb and lsnb: run and lumisection number respectively \u2937 get_scores full signature: def get_scores( self, modelname, histname=None, setnames=None, masknames=None ) comments: get the array of scores for a given model and for a given histogram type, optionally after masking input arguments: - modelname: name of the model for which to retrieve the scores - histname: name of the histogram type for which to retrieve the score. if None, return a dict matching histnames to arrays of scores - setnames: list of names of the histogram sets (use None for standard set) - masknames: list of names of masks (default: no masking, return full array) notes: - do not use setnames and masknames simultaneously, this is not defined - if multiple masks are given, the intersection is taken; if multiple sets are given, the union is taken - the classifiers in the appropriate model must have been evaluated before calling this method! \u2937 get_scores_array full signature: def get_scores_array( self, modelname, setnames=None, masknames=None ) comments: similar to get_scores, but with different return type: np array of shape (nhistograms, nhistogramtypes) \u2937 get_extscores full signature: def get_extscores( self, extname, histname=None ) comments: get the array of scores for a given histogram type in a given extra set. DEPRECATED, DO NOT USE ANYMORE input arguments: - extname: name of the extra set (see also add_exthistograms) - histname: name of the histogram type for which to retrieve the score. if None, return a dict matching histnames to arrays of scores notes: - this method takes the scores from the HistStruct.extscores attribute; make sure to have evaluated the classifiers before calling this method, else an exception will be thrown. \u2937 get_extscores_array full signature: def get_extscores_array( self, extname ) comments: similar to get_extscores, but with different return type: np array of shape (nhistograms, nhistogramtypes) DEPRECATED, DO NOT USE ANYMORE \u2937 get_scores_ls full signature: def get_scores_ls( self, modelname, runnb, lsnb, histnames=None ) comments: get the scores for a given run/lumisection number and for given histogram names input arguments: - modelname: name of the model for which to retrieve the score - runnb: run number - lsnb: lumisection number - histnames: names of the histogram types for which to retrieve the score. returns: - a dict matching each name in histnames to a score (or None if no valid score) \u2937 get_globalscores full signature: def get_globalscores( self, modelname, setnames=None, masknames=None ) comments: get the array of global scores, optionally after masking input arguments: - modelname: name of the model for which to retrieve the global score - setnames: list of names of the histogram sets (use None for standard set) - masknames: list of names of masks (default: no masking, return full array) notes: - do not use setnames and masknames simultaneously, this is not defined - if multiple masks are given, the intersection is taken; if multiple sets are given, the union is taken - the classifiers in the appropriate model must have been evaluated before calling this method! \u2937 get_globalscores_jsonformat full signature: def get_globalscores_jsonformat( self, modelname=None ) comments: make a json format listing all lumisections in this histstruct the output list has entries for global scores and masks input arguments: - modelname: name of the model for wich to retrieve the global score if None, all available models will be used \u2937 get_globalscore_ls full signature: def get_globalscore_ls( self, modelname, runnb, lsnb ) comments: get the global score for a given run/lumisection number input arguments: - modelname: name of the model for which to retrieve the global score - runnb: run number - lsnb: lumisection number - histnames: names of the histogram types for which to retrieve the score. returns: - a dict matching each name in histnames to a score (or None if no valid score) \u2937 get_globalscores_mask full signature: def get_globalscores_mask( self, modelname, masknames=None, setnames=None, score_up=None, score_down=None ) comments: get the mask for global score between specified boundaries input arguments: - modelname: name of the model for which to consider the global scores - masknames: list of additional masks (on top of score boundaries) to consider - setnames: list of set names for which to retrieve the global scores - score_up and score_down are upper and lower thresholds if both are not None, the mask for global scores between the boundaries are returned if score_up is None, the mask for global score > score_down are returned if score_down is None, the mask for global score < score_up are returned \u2937 get_globalscores_indices full signature: def get_globalscores_indices( self, modelname, masknames=None, setnames=None, score_up=None, score_down=None ) comments: get the indices with a global score between specified boundaries input arguments: see get_globalscore_mask \u2937 get_globalscores_runsls full signature: def get_globalscores_runsls( self, modelname, masknames=None, setnames=None, score_up=None, score_down=None ) comments: get the run and lumisection numbers with a global score between specified boundaries input arguments: see get_globalscore_mask \u2937 get_extglobalscores full signature: def get_extglobalscores( self, extname ) comments: get the array of global scores for one of the extra histogram sets DEPRECATED, DO NOT USE ANYMORE input arguments: - extname: name of the extra histogram set notes: - this method takes the scores from the HistStruct.extglobalscores attribute; make sure to have set this attribute with add_extglobalscores, else an exception will be thrown. \u2937 get_histograms full signature: def get_histograms( self, histname=None, masknames=None, setnames=None ) comments: get the array of histograms for a given type, optionally after masking input arguments: - histname: name of the histogram type to retrieve if None, return a dict matching histnames to arrays of histograms - masknames: list of names of masks (default: no masking, return full array) - setnames: list of names of the sets of extra histograms (see also add_exthistograms) if multiple setnames are provided, the union/concatenation is returned \u2937 get_histogramsandscores full signature: def get_histogramsandscores( self, modelname=None, setnames=None, masknames=None, nrandoms=-1, nfirst=-1 ) comments: combination of get_histograms, get_scores and get_globalscores with additional options - modelname: name of the model for which to retrieve the score if None, no scores will be retrieved (only histograms) - setnames: list of names of histogram sets (use None for default set) - masknames: list of names of masks - nrandoms: if > 0, number of random instances to draw - nfirst: if > 0, number of first instances to keep return type: dict with keys 'histograms', 'scores' and 'globalscores' note that the values of scores and globalscores may be None if not initialized \u2937 add_model full signature: def add_model( self, modelname, model ) comments: add a model to the HistStruct input arguments: - modelname: a name for the model - model: an instance of ModelInterface class with histnames corresponding to the ones for this HistStruct \u2937 check_model full signature: def check_model( self, modelname ) comments: check if a given model name is present in the HistStruct input arguments: - modelname: name of the model to check \u2937 remove_model full signature: def remove_model( self, modelname ) comments: remove a model input arguments: - modelname: name of the model to remove \u2937 train_classifier full signature: def train_classifier( self, modelname, histname, masknames=None, setnames=None, **kwargs ) comments: train a histogram classifier input arguments: - modelname: name of the model for which to train the classifiers - histname: a valid histogram name present in the HistStruct for which to train the classifier - masknames: list of masks the classifiers should be trained on - setnames: list of names of sets of extra histograms on which the classifiers should be trained - kwargs: additional keyword arguments for training \u2937 train_classifiers full signature: def train_classifiers( self, modelname, masknames=None, setnames=None, **kwargs ) comments: train histogram classifiers for all histogram types input arguments: - modelname: name of the model for which to train the classifiers - masknames: list of masks the classifiers should be trained on - setnames: list of names of sets of extra histograms on which the classifiers should be trained - kwargs: additional keyword arguments for training \u2937 evaluate_classifier full signature: def evaluate_classifier( self, modelname, histname, masknames=None, setnames=None ) comments: evaluate a histogram classifier input arguments: - modelname: name of the model for wich to evaluate the classifiers - histname: a valid histogram name present in the HistStruct for which to evaluate the classifier - masknames: list of masks if the classifiers should be evaluated on a subset only (e.g. for speed) - setnames: list of names of sets of extra histograms for which the classifiers should be evaluated \u2937 evaluate_classifiers full signature: def evaluate_classifiers( self, modelname, masknames=None, setnames=None ) comments: evaluate histogram classifiers for all histogram types input arguments: - modelname: name of the model for wich to evaluate the classifiers - masknames: list of masks if the classifiers should be evaluated on a subset only (e.g. for speed) - setnames: list of names of a set of extra histograms for which the classifiers should be evaluated \u2937 set_fitter full signature: def set_fitter( self, modelname, fitter ) comments: set the fitter for a given model \u2937 train_fitter full signature: def train_fitter( self, modelname, masknames=None, setnames=None, verbose=False, **kwargs ) comments: train the fitter for a given model input arguments: - modelname: name of the model to train - masknames: list of mask names for training set - setnames: list of set names for training set - kwargs: additional keyword arguments for fitting note: use either masksnames or setnames, not both! \u2937 train_partial_fitters full signature: def train_partial_fitters( self, modelname, dimslist, masknames=None, setnames=None, **kwargs ) comments: train partial fitters for a given model input arguments: - modelname: name of the model to train - dimslist: list of tuples with integer dimension numbers - masknames: list of mask names for training set - setnames: list of set names for training set - kwargs: additional keyword arguments for fitting note: use either masksnames or setnames, not both! note: see also plot_partial_fit for a convenient plotting method! \u2937 evaluate_fitter full signature: def evaluate_fitter( self, modelname, masknames=None, setnames=None, verbose=False ) comments: evaluate the fitter for a given model input arguments: - modelname: name of the model for which to evaluate the fitter - masknames: list of mask names if the fitter should be evaluated on a subset only (e.g. for speed) - setnames: list of set names of extra histograms for which the fitter should be evaluated \u2937 evaluate_fitter_on_point full signature: def evaluate_fitter_on_point( self, modelname, point ) comments: evaluate the fitter on a given points input arguments: - modelname: name of the model for which to evaluate the fitter - points: dict matching histnames to scores (one float per histogram type) (e.g. as returned by get_scores_ls) returns: - the global score for the provided point (a float) \u2937 evaluate_fitter_on_points full signature: def evaluate_fitter_on_points( self, modelname, points ) comments: evaluate the fitter on a given set of points input arguments: - modelname: name of the model for which to evaluate the fitter - points: dict matching histnames to scores (np array of shape (nhistograms)) returns: - the global scores for the provided points \u2937 plot_histograms full signature: def plot_histograms( self, histnames=None, masknames=None, histograms=None, ncols=4, colorlist=[], labellist=[], transparencylist=[], titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs ) comments: plot the histograms in a HistStruct, optionally after masking input arguments: - histnames: list of names of the histogram types to plot (default: all) - masknames: list of list of mask names note: each element in masknames represents a set of masks to apply; the histograms passing different sets of masks are plotted in different colors - histograms: list of dicts of histnames to 2D arrays of histograms, can be used to plot a given collection of histograms directly, and bypass the histnames and masknames arguments (note: for use in the gui, not recommended outside of it) - ncols: number of columns (only relevant for 1D histograms) - colorlist: list of matplotlib colors, must have same length as masknames - labellist: list of labels for the legend, must have same legnth as masknames - transparencylist: list of transparency values, must have same length as masknames - titledict: dict mapping histogram names to titles for the subplots (default: title = histogram name) - xaxtitledict: dict mapping histogram names to x-axis titles for the subplots (default: no x-axis title) - yaxtitledict: dict mapping histogram names to y-axis titles for the subplots (default: no y-axis title) - physicalxax: bool whether to use physical x-axis range or simply use bin number (default) - kwargs: keyword arguments passed down to plot_utils.plot_sets \u2937 plot_histograms_1d full signature: def plot_histograms_1d( self, histnames=None, masknames=None, histograms=None, ncols=4, colorlist=[], labellist=[], transparencylist=[], titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs ) comments: plot the histograms in a histstruct, optionally after masking internal helper function, use only via plot_histograms \u2937 plot_histograms_2d full signature: def plot_histograms_2d( self, histnames=None, masknames=None, histograms=None, labellist=[], titledict=None, xaxtitledict=None, yaxtitledict=None, **kwargs ) comments: plot the histograms in a histstruct, optionally after masking internal helper function, use only via plot_histograms \u2937 plot_histograms_run full signature: def plot_histograms_run( self, histnames=None, masknames=None, histograms=None, ncols=4, titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs ) comments: plot a set of histograms in a HistStruct with a smooth color gradient. typical use case: plot a single run. note: only for 1D histograms! input arguments: - histnames: list of names of the histogram types to plot (default: all) - masknames: list mask names (typically should contain a run number mask) - histograms: dict of histnames to 2D arrays of histograms, can be used to plot a given collection of histograms directly, and bypass the histnames and masknames arguments (note: for use in the gui, not recommended outside of it. - titledict: dict mapping histogram names to titles for the subplots (default: title = histogram name) - xaxtitledict: dict mapping histogram names to x-axis titles for the subplots (default: no x-axis title) - yaxtitledict: dict mapping histogram names to y-axis titles for the subplots (default: no y-axis title) - physicalxax: bool whether to use physical x-axis range or simply use bin number (default) - kwargs: keyword arguments passed down to plot_utils.plot_hists_multi \u2937 plot_histograms_run_1d full signature: def plot_histograms_run_1d( self, histnames=None, masknames=None, histograms=None, ncols=4, titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs ) comments: plot the histograms in a histstruct, optionally after masking internal helper function, use only via plot_histograms_run \u2937 plot_ls full signature: def plot_ls( self, runnb, lsnb, histnames=None, histlabel=None, ncols=4, recohist=None, recohistlabel='Reconstruction', refhists=None, refhistslabel='Reference histograms', refhiststransparency=None, titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs) comments: plot the histograms in a HistStruct for a given run/ls number versus their references and/or their reconstruction input arguments: - runnb: run number - lsnb: lumisection number - histnames: names of histogram types to plot (default: all) - histlabel: legend entry for the histogram (default: run and lumisection number) - recohist: dict matching histogram names to reconstructed histograms notes: - 'reconstructed histograms' refers to e.g. autoencoder or NMF reconstructions; some models (e.g. simply looking at histogram moments) might not have this kind of reconstruction - in principle one histogram per key is expected, but still the the shape must be 2D (i.e. (1,nbins)) - in case recohist is set to a valid model name present in the current HistStruct, the reconstruction is calculated on the fly for the input histograms - recohistlabel: legend entry for the reco histograms - refhists: dict matching histogram names to reference histograms notes: - multiple histograms (i.e. a 2D array) per key are expected; in case there is only one reference histogram, it must be reshaped into (1,nbins) - refhistslabel: legend entry for the reference histograms - titledict: dict mapping histogram names to titles for the subplots (default: title = histogram name) - xaxtitledict: dict mapping histogram names to x-axis titles for the subplots (default: no x-axis title) - yaxtitledict: dict mapping histogram names to y-axis titles for the subplots (default: no y-axis title) - physicalxax: bool whether to use physical x-axis range or simply use bin number (default) - kwargs: keyword arguments passed down to plot_utils.plot_sets \u2937 plot_run full signature: def plot_run( self, runnb, masknames=None, ncols=4, recohist=None, recohistlabel='reco', refhists=None, refhistslabel='reference', doprint=False) comments: call plot_ls for all lumisections in a given run \u2937 plot_ls_1d full signature: def plot_ls_1d( self, runnb, lsnb, histnames=None, histlabel=None, ncols=4, recohist=None, recohistlabel='Reconstruction', refhists=None, refhistslabel='Reference histograms', refhiststransparency=None, titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs) comments: plot the histograms in a HistStruct for a given run/ls number versus their references and/or their reconstruction internal helper function, use only via plot_ls \u2937 plot_ls_2d full signature: def plot_ls_2d( self, runnb, lsnb, histnames=None, histlabel=None, recohist=None, recohistlabel='Reconstruction', titledict=None, xaxtitledict=None, yaxtitledict=None, **kwargs) comments: plot the histograms in a HistStruct for a given run/ls number versus their reconstruction internal helper function, use only via plot_ls \u2937 plot_ls_score full signature: def plot_ls_score( self, modelname, runnb, lsnb, ncols=4, masknames=None, setnames=None, **kwargs ) comments: plot the score of a given lumisection for each histogram type compared to reference scores input arguments: - modelname: name of the model for which to retrieve the score - runnb: run number - lsnb: lumisection number - masknames: list of mask names for the reference scores - setnames: list of set names for the reference scores - kwargs: additional keyword arguments passed down to pu.plot_score_dist \u2937 plot_partial_fit full signature: def plot_partial_fit( self, modelname, dims, clusters, **kwargs) comments: plot a partial fit calculated with train_partial_fitters input arguments: - modelname: name of the model for which to plot the partial fits - dims: a tuple of length 1 or 2 with integer dimension indices note: the partial fit for this dimension must have been calculated with train_partial_fitters first - clusters: a list of the different point clusters to plot each element in the list should be a dict of the form {'masknames': [list of mask names]} or {'setnames': [list of set names]} - kwargs: plot options passed down to pu.plot_fit_1d_clusters or pu.plot_fit_2d_clusters; some of them have to have the same length as clusters (e.g. colors and labels) \u2937 plot_score_dist full signature: def plot_score_dist( self, modelname, histname=None, masknames_sig=None, setnames_sig=None, masknames_bkg=None, setnames_bkg=None, **kwargs ) comments: plot a 1D score distribution input arguments: - modelname: name of the model for which to retrieve the scores - histname: type of histogram for which to retrieve the scores if None, the global scores will be retrieved - masknames_sig, setnames_sig: lists of mask or set names for signal distribution - masknames_bkg, setnames_bkg: lists of mask or set names for background distribution note: in case of multiple masks, the intersection is taken (as usual); in case of multiple sets, the union is taken! - kwargs: additional keyword arguments passed down to pu.plot_score_dist","title":"HistStruct"},{"location":"src/HistStruct/#histstruct","text":"HistStruct: consistent treatment of multiple histogram types The HistStruct class is the main data structure used within this framework. A HistStruct object basically consists of a mutually consistent collection of numpy arrays, where each numpy array corresponds to one histogram type, with dimensions (number of histograms, number of bins). The HistStruct has functions to easily perform the following common tasks (among others): select a subset of runs and/or lumisections (e.g. using a custom or predefined json file formatted selector), prepare the data for machine learning training, with all kinds of preprocessing, evaluate classifiers (machine learning types or other), go from per-histogram scores to per-lumisection scores. When only processing a single histogram type, the HistStruct might be a bit of an overkill. One could instead choose to operate on the dataframe directly. However, especially when using multiple histogram types, the HistStruct is very handy to keep everything consistent.","title":"HistStruct"},{"location":"src/HistStruct/#class-histstruct","text":"comments: (no valid documentation found)","title":"[class] HistStruct"},{"location":"src/HistStruct/#9595init9595","text":"full signature: def __init__( self ) comments: empty initializer, setting all containers to empty defaults a HistStruct object has the following properties: histnames: list of histogram names histograms: dict mapping histogram name to 2D numpy array of histograms (shape (nhists,nbins)) nentries: dict mapping histogram name to 1D numpy array of number of entries per histogram (same length as histograms) histranges: dict mapping histogram name to tuple with (xmin, xmax) runnbs: 1D numpy array of run numbers (same length as histograms) lsnbs: 1D numpy array of lumisection numbers (same length as histograms) masks: dict mapping name to 1D numpy array of booleans (same length as histograms) that can be used for masking exthistograms: dict of dicts similar to histograms for additional (e.g. artificially generated) histograms setnames: list of names of extended sets models: dict mapping model names to ModelInterfaces modelnames: list of model names","title":"&#10551; __init__"},{"location":"src/HistStruct/#9595str9595","text":"full signature: def __str__( self ) comments: get a printable representation of a HistStruct","title":"&#10551; __str__"},{"location":"src/HistStruct/#save","text":"full signature: def save( self, path, save_models=False, save_classifiers=True, save_fitter=True ) comments: save a HistStruct object to a pkl file input arguments: - path where to store the file (appendix .zip is automatically appended) - save_models: a boolean whether to include the models if present in the HistStruct - save_classifiers: a boolean whether to include the classifiers if present in the ModelInterfaces - save_fitter: a boolean whether to include the fitter if present in the ModelInterfaces","title":"&#10551; save"},{"location":"src/HistStruct/#load","text":"full signature: def load( self, path, load_models=True, load_classifiers=True, load_fitter=True, verbose=False ) comments: load a HistStruct object input arguments: - path to a zip file containing a HistStruct object - load_models: a boolean whether to load the models if present - load_classifiers: a boolean whether to load the classifiers if present - load_fitter: a boolean whether to load the fitter if present - verbose: boolean whether to print some information","title":"&#10551; load"},{"location":"src/HistStruct/#add95dataframe","text":"full signature: def add_dataframe( self, df, cropslices=None, rebinningfactor=None, smoothinghalfwindow=None, smoothingweights=None, averagewindow=None, averageweights=None, donormalize=True ) comments: add a dataframe to a HistStruct input arguments: - df: a pandas dataframe as read from the input csv files - cropslices: list of slices (one per dimension) by which to crop the histograms see hist_utils.py / crophists for more info. - rebinningfactor: factor by which to group bins together see hist_utils.py / rebinhists for more info. - smoothinghalfwindow: half window (int for 1D, tuple for 2D) for doing smoothing of histograms - smoothingweights: weight array (1D for 1D, 2D for 2D) for smoothing of histograms see hist_utils.py / smoothhists for more info. - averagewindow: window (int or tuple) for averaging each histogram with its neighbours - averageweights: weights for averaging each histogram with its neighbours see hist_utils.py / running_average_hists for more info. - donormalize: boolean whether to normalize the histograms see hist_utils.py / normalizehists for more info. notes: - the new dataframe can contain one or multiple histogram types - the new dataframe must contain the same run and lumisection numbers (for each histogram type in it) as already present in the HistStruct, except if it is the first one to be added - alternative to adding the dataframe with the preprocessing options, one can also apply the preprocessing at a later stage using the preprocess() function with the same arguments.","title":"&#10551; add_dataframe"},{"location":"src/HistStruct/#add95histograms","text":"full signature: def add_histograms( self, histname, histograms, runnbs, lsnbs, nentries=None ) comments: add a set of histograms to a HistStruct input arguments: - histname: name of the histogram type to be added - histograms: a numpy array of shape (nhistograms,nbins), assumed to be of a single type - runnbs: a 1D list or array of length nhistograms containing the run number per histogram - lsnbs: a 1D list or array of length nhistograms containing the lumisection number per histogram - nentries: a 1D list or array of length nhistograms containing the number of entries per histogram notes: - must be provided explicitly since histograms might be normalized, in which case the number of entries cannot be determined from the sum of bin contents. - used for (de-)selecting histograms with sufficient statistics; if you don't need that type of selection, nentries can be left at default. - default is None, meaning all entries will be set to zero. notes: - no preprocessing is performed, this is assumed to have been done manually (if needed) before adding the histograms - runnbs and lsnbs must correspond to what is already in the current HistStruct, except if this is the first set of histogram to be added - see also add_dataframe for an alternative way of adding histograms","title":"&#10551; add_histograms"},{"location":"src/HistStruct/#preprocess","text":"full signature: def preprocess( self, masknames=None, cropslices=None, rebinningfactor=None, smoothinghalfwindow=None, smoothingweights=None, averagewindow=None, averageweights=None, donormalize=False ) comments: do preprocessing input arguments: - masknames: names of masks to select histograms to which to apply the preprocessing (histograms not passing the masks are simply copied) the other input arguments are equivalent to those given in add_dataframe, but this function allows to do preprocessing after the dataframes have already been loaded note: does not work on extended histograms sets! one needs to apply preprocessing before generating extra histograms.","title":"&#10551; preprocess"},{"location":"src/HistStruct/#add95globalscores","text":"full signature: def add_globalscores( self, globalscores ) comments: add an array of global scores (one per lumisection) DEPRECATED, DO NOT USE ANYMORE input arguments: - globalscores: 1D numpy array of scores (must have same length as lumisection and run numbers)","title":"&#10551; add_globalscores"},{"location":"src/HistStruct/#add95extglobalscores","text":"full signature: def add_extglobalscores( self, extname, globalscores ) comments: add an array of global scores (one per lumisection) for a specified extra set of histograms in the HistStruct DEPRECATED, DO NOT USE ANYMORE input arguments: - extname: name of extra histogram set - globalscores: 1D numpy array of scores note: this function checks if all histogram types in this set contain the same number of histograms, (and that this number corresponds to the length of globalscores) else adding globalscores is meaningless","title":"&#10551; add_extglobalscores"},{"location":"src/HistStruct/#add95exthistograms","text":"full signature: def add_exthistograms( self, setname, histname, histograms, overwrite=False ) comments: add a set of extra histograms to a HistStruct these histograms are not assumed to correspond to physical run/lumisections numbers (e.g. resampled ones), and no consistency checks are done input arguments: - setname: name of the extra histogram set (you can add multiple, e.g. resampled_good, resampled_bad and/or resampled_training) - histname: name of the histogram type - histograms: a numpy array of shape (nhistograms,nbins) - overwrite: boolean whether to overwrite a set of histograms of the same name if present (default: raise exception)","title":"&#10551; add_exthistograms"},{"location":"src/HistStruct/#add95mask","text":"full signature: def add_mask( self, name, mask ) comments: add a mask to a HistStruct input arguments: - name: a name for the mask - mask: a 1D np array of booleans with same length as number of lumisections in HistStruct","title":"&#10551; add_mask"},{"location":"src/HistStruct/#remove95mask","text":"full signature: def remove_mask( self, name ) comments: inverse operation of add_mask","title":"&#10551; remove_mask"},{"location":"src/HistStruct/#add95index95mask","text":"full signature: def add_index_mask( self, name, indices ) comments: add a mask corresponding to predefined indices input arguments: - name: a name for the mask - indices: a 1D np array of integer indices, between 0 and the number of lumisections in HistStruct","title":"&#10551; add_index_mask"},{"location":"src/HistStruct/#add95run95mask","text":"full signature: def add_run_mask( self, name, runnb ) comments: add a mask corresponding to a given run number input arguments: - name: a name for the mask - runnb: run number","title":"&#10551; add_run_mask"},{"location":"src/HistStruct/#add95multirun95mask","text":"full signature: def add_multirun_mask( self, name, runnbs ) comments: add a mask corresponding to a given list of run numbers input arguments: - name: a name for the mask - runnbs: a list of run numbers","title":"&#10551; add_multirun_mask"},{"location":"src/HistStruct/#add95json95mask","text":"full signature: def add_json_mask( self, name, jsondict ) comments: add a mask corresponding to a json dict input arguments: - name: a name for the mask - jsondict: a dictionary in typical json format (see the golden json file for inspiration) all lumisections present in the jsondict will be masked True, the others False.","title":"&#10551; add_json_mask"},{"location":"src/HistStruct/#add95goldenjson95mask","text":"full signature: def add_goldenjson_mask( self, name ) comments: add a mask corresponding to the golden json file input arguments: - name: a name for the mask","title":"&#10551; add_goldenjson_mask"},{"location":"src/HistStruct/#add95dcsonjson95mask","text":"full signature: def add_dcsonjson_mask( self, name ) comments: add a mask corresponding to the DCS-bit on json file input arguments: - name: a name for the mask","title":"&#10551; add_dcsonjson_mask"},{"location":"src/HistStruct/#add95stat95mask","text":"full signature: def add_stat_mask( self, name, histnames=None, min_entries_to_bins_ratio=-1, max_entries_to_bins_ratio=-1 ) comments: add a mask corresponding to lumisections where all histograms have statistics within given bounds input arguments: - histnames: list of histogram names to take into account for making the mask (default: all in the HistStruct) - min_entries_to_bins_ratio: number of entries divided by number of bins, lower boundary for statistics (default: no lower boundary) - max_entries_to_bins_ratio: same but upper boundary instead of lower boundary (default: no upper boundary)","title":"&#10551; add_stat_mask"},{"location":"src/HistStruct/#add95highstat95mask","text":"full signature: def add_highstat_mask( self, name, histnames=None, entries_to_bins_ratio=100 ) comments: shorthand call to add_stat_mask with only lower boundary and no upper boundary for statistics input arguments: - entries_to_bins_ratio: number of entries divided by number of bins, lower boundary for statistics others: see add_stat_mask","title":"&#10551; add_highstat_mask"},{"location":"src/HistStruct/#pass95masks","text":"full signature: def pass_masks( self, masknames, runnbs=None, lsnbs=None ) comments: get a list of booleans of lumisections whether they pass a given set of masks input arguments: - masknames: list of mask names - runnbs: list of run numbers (default: all in histstruct) - lsnbs: list of lumisection numbers (equally long as runnbs) (default: al in histstruct)","title":"&#10551; pass_masks"},{"location":"src/HistStruct/#get95masknames","text":"full signature: def get_masknames( self ) comments: return a list of all mask names in the current HistStruct","title":"&#10551; get_masknames"},{"location":"src/HistStruct/#get95mask","text":"full signature: def get_mask( self, name ) comments: return a mask in the current HistStruct","title":"&#10551; get_mask"},{"location":"src/HistStruct/#get95combined95mask","text":"full signature: def get_combined_mask( self, names ) comments: get a combined (intersection) mask given multiple mask names mostly for internal use; externally you can use get_histograms( histname, <list of mask names>) directly","title":"&#10551; get_combined_mask"},{"location":"src/HistStruct/#get95union95mask","text":"full signature: def get_union_mask( self, names ) comments: get a combined (union) mask given multiple mask names mostly for internal use","title":"&#10551; get_union_mask"},{"location":"src/HistStruct/#get95runnbs","text":"full signature: def get_runnbs( self, masknames=None ) comments: get the array of run numbers, optionally after masking input arguments: - masknames: list of names of masks (default: no masking, return full array)","title":"&#10551; get_runnbs"},{"location":"src/HistStruct/#get95runnbs95unique","text":"full signature: def get_runnbs_unique( self, masknames=None ) comments: get a list of unique run numbers","title":"&#10551; get_runnbs_unique"},{"location":"src/HistStruct/#get95lsnbs","text":"full signature: def get_lsnbs( self, masknames=None ) comments: get the array of lumisection numbers, optionally after masking input arguments: - masknames: list of names of masks (default: no masking, return full array)","title":"&#10551; get_lsnbs"},{"location":"src/HistStruct/#get95index","text":"full signature: def get_index( self, runnb, lsnb ) comments: get the index in the current HistStruct of a given run and lumisection number input arguments: - runnb and lsnb: run and lumisection number respectively","title":"&#10551; get_index"},{"location":"src/HistStruct/#get95scores","text":"full signature: def get_scores( self, modelname, histname=None, setnames=None, masknames=None ) comments: get the array of scores for a given model and for a given histogram type, optionally after masking input arguments: - modelname: name of the model for which to retrieve the scores - histname: name of the histogram type for which to retrieve the score. if None, return a dict matching histnames to arrays of scores - setnames: list of names of the histogram sets (use None for standard set) - masknames: list of names of masks (default: no masking, return full array) notes: - do not use setnames and masknames simultaneously, this is not defined - if multiple masks are given, the intersection is taken; if multiple sets are given, the union is taken - the classifiers in the appropriate model must have been evaluated before calling this method!","title":"&#10551; get_scores"},{"location":"src/HistStruct/#get95scores95array","text":"full signature: def get_scores_array( self, modelname, setnames=None, masknames=None ) comments: similar to get_scores, but with different return type: np array of shape (nhistograms, nhistogramtypes)","title":"&#10551; get_scores_array"},{"location":"src/HistStruct/#get95extscores","text":"full signature: def get_extscores( self, extname, histname=None ) comments: get the array of scores for a given histogram type in a given extra set. DEPRECATED, DO NOT USE ANYMORE input arguments: - extname: name of the extra set (see also add_exthistograms) - histname: name of the histogram type for which to retrieve the score. if None, return a dict matching histnames to arrays of scores notes: - this method takes the scores from the HistStruct.extscores attribute; make sure to have evaluated the classifiers before calling this method, else an exception will be thrown.","title":"&#10551; get_extscores"},{"location":"src/HistStruct/#get95extscores95array","text":"full signature: def get_extscores_array( self, extname ) comments: similar to get_extscores, but with different return type: np array of shape (nhistograms, nhistogramtypes) DEPRECATED, DO NOT USE ANYMORE","title":"&#10551; get_extscores_array"},{"location":"src/HistStruct/#get95scores95ls","text":"full signature: def get_scores_ls( self, modelname, runnb, lsnb, histnames=None ) comments: get the scores for a given run/lumisection number and for given histogram names input arguments: - modelname: name of the model for which to retrieve the score - runnb: run number - lsnb: lumisection number - histnames: names of the histogram types for which to retrieve the score. returns: - a dict matching each name in histnames to a score (or None if no valid score)","title":"&#10551; get_scores_ls"},{"location":"src/HistStruct/#get95globalscores","text":"full signature: def get_globalscores( self, modelname, setnames=None, masknames=None ) comments: get the array of global scores, optionally after masking input arguments: - modelname: name of the model for which to retrieve the global score - setnames: list of names of the histogram sets (use None for standard set) - masknames: list of names of masks (default: no masking, return full array) notes: - do not use setnames and masknames simultaneously, this is not defined - if multiple masks are given, the intersection is taken; if multiple sets are given, the union is taken - the classifiers in the appropriate model must have been evaluated before calling this method!","title":"&#10551; get_globalscores"},{"location":"src/HistStruct/#get95globalscores95jsonformat","text":"full signature: def get_globalscores_jsonformat( self, modelname=None ) comments: make a json format listing all lumisections in this histstruct the output list has entries for global scores and masks input arguments: - modelname: name of the model for wich to retrieve the global score if None, all available models will be used","title":"&#10551; get_globalscores_jsonformat"},{"location":"src/HistStruct/#get95globalscore95ls","text":"full signature: def get_globalscore_ls( self, modelname, runnb, lsnb ) comments: get the global score for a given run/lumisection number input arguments: - modelname: name of the model for which to retrieve the global score - runnb: run number - lsnb: lumisection number - histnames: names of the histogram types for which to retrieve the score. returns: - a dict matching each name in histnames to a score (or None if no valid score)","title":"&#10551; get_globalscore_ls"},{"location":"src/HistStruct/#get95globalscores95mask","text":"full signature: def get_globalscores_mask( self, modelname, masknames=None, setnames=None, score_up=None, score_down=None ) comments: get the mask for global score between specified boundaries input arguments: - modelname: name of the model for which to consider the global scores - masknames: list of additional masks (on top of score boundaries) to consider - setnames: list of set names for which to retrieve the global scores - score_up and score_down are upper and lower thresholds if both are not None, the mask for global scores between the boundaries are returned if score_up is None, the mask for global score > score_down are returned if score_down is None, the mask for global score < score_up are returned","title":"&#10551; get_globalscores_mask"},{"location":"src/HistStruct/#get95globalscores95indices","text":"full signature: def get_globalscores_indices( self, modelname, masknames=None, setnames=None, score_up=None, score_down=None ) comments: get the indices with a global score between specified boundaries input arguments: see get_globalscore_mask","title":"&#10551; get_globalscores_indices"},{"location":"src/HistStruct/#get95globalscores95runsls","text":"full signature: def get_globalscores_runsls( self, modelname, masknames=None, setnames=None, score_up=None, score_down=None ) comments: get the run and lumisection numbers with a global score between specified boundaries input arguments: see get_globalscore_mask","title":"&#10551; get_globalscores_runsls"},{"location":"src/HistStruct/#get95extglobalscores","text":"full signature: def get_extglobalscores( self, extname ) comments: get the array of global scores for one of the extra histogram sets DEPRECATED, DO NOT USE ANYMORE input arguments: - extname: name of the extra histogram set notes: - this method takes the scores from the HistStruct.extglobalscores attribute; make sure to have set this attribute with add_extglobalscores, else an exception will be thrown.","title":"&#10551; get_extglobalscores"},{"location":"src/HistStruct/#get95histograms","text":"full signature: def get_histograms( self, histname=None, masknames=None, setnames=None ) comments: get the array of histograms for a given type, optionally after masking input arguments: - histname: name of the histogram type to retrieve if None, return a dict matching histnames to arrays of histograms - masknames: list of names of masks (default: no masking, return full array) - setnames: list of names of the sets of extra histograms (see also add_exthistograms) if multiple setnames are provided, the union/concatenation is returned","title":"&#10551; get_histograms"},{"location":"src/HistStruct/#get95histogramsandscores","text":"full signature: def get_histogramsandscores( self, modelname=None, setnames=None, masknames=None, nrandoms=-1, nfirst=-1 ) comments: combination of get_histograms, get_scores and get_globalscores with additional options - modelname: name of the model for which to retrieve the score if None, no scores will be retrieved (only histograms) - setnames: list of names of histogram sets (use None for default set) - masknames: list of names of masks - nrandoms: if > 0, number of random instances to draw - nfirst: if > 0, number of first instances to keep return type: dict with keys 'histograms', 'scores' and 'globalscores' note that the values of scores and globalscores may be None if not initialized","title":"&#10551; get_histogramsandscores"},{"location":"src/HistStruct/#add95model","text":"full signature: def add_model( self, modelname, model ) comments: add a model to the HistStruct input arguments: - modelname: a name for the model - model: an instance of ModelInterface class with histnames corresponding to the ones for this HistStruct","title":"&#10551; add_model"},{"location":"src/HistStruct/#check95model","text":"full signature: def check_model( self, modelname ) comments: check if a given model name is present in the HistStruct input arguments: - modelname: name of the model to check","title":"&#10551; check_model"},{"location":"src/HistStruct/#remove95model","text":"full signature: def remove_model( self, modelname ) comments: remove a model input arguments: - modelname: name of the model to remove","title":"&#10551; remove_model"},{"location":"src/HistStruct/#train95classifier","text":"full signature: def train_classifier( self, modelname, histname, masknames=None, setnames=None, **kwargs ) comments: train a histogram classifier input arguments: - modelname: name of the model for which to train the classifiers - histname: a valid histogram name present in the HistStruct for which to train the classifier - masknames: list of masks the classifiers should be trained on - setnames: list of names of sets of extra histograms on which the classifiers should be trained - kwargs: additional keyword arguments for training","title":"&#10551; train_classifier"},{"location":"src/HistStruct/#train95classifiers","text":"full signature: def train_classifiers( self, modelname, masknames=None, setnames=None, **kwargs ) comments: train histogram classifiers for all histogram types input arguments: - modelname: name of the model for which to train the classifiers - masknames: list of masks the classifiers should be trained on - setnames: list of names of sets of extra histograms on which the classifiers should be trained - kwargs: additional keyword arguments for training","title":"&#10551; train_classifiers"},{"location":"src/HistStruct/#evaluate95classifier","text":"full signature: def evaluate_classifier( self, modelname, histname, masknames=None, setnames=None ) comments: evaluate a histogram classifier input arguments: - modelname: name of the model for wich to evaluate the classifiers - histname: a valid histogram name present in the HistStruct for which to evaluate the classifier - masknames: list of masks if the classifiers should be evaluated on a subset only (e.g. for speed) - setnames: list of names of sets of extra histograms for which the classifiers should be evaluated","title":"&#10551; evaluate_classifier"},{"location":"src/HistStruct/#evaluate95classifiers","text":"full signature: def evaluate_classifiers( self, modelname, masknames=None, setnames=None ) comments: evaluate histogram classifiers for all histogram types input arguments: - modelname: name of the model for wich to evaluate the classifiers - masknames: list of masks if the classifiers should be evaluated on a subset only (e.g. for speed) - setnames: list of names of a set of extra histograms for which the classifiers should be evaluated","title":"&#10551; evaluate_classifiers"},{"location":"src/HistStruct/#set95fitter","text":"full signature: def set_fitter( self, modelname, fitter ) comments: set the fitter for a given model","title":"&#10551; set_fitter"},{"location":"src/HistStruct/#train95fitter","text":"full signature: def train_fitter( self, modelname, masknames=None, setnames=None, verbose=False, **kwargs ) comments: train the fitter for a given model input arguments: - modelname: name of the model to train - masknames: list of mask names for training set - setnames: list of set names for training set - kwargs: additional keyword arguments for fitting note: use either masksnames or setnames, not both!","title":"&#10551; train_fitter"},{"location":"src/HistStruct/#train95partial95fitters","text":"full signature: def train_partial_fitters( self, modelname, dimslist, masknames=None, setnames=None, **kwargs ) comments: train partial fitters for a given model input arguments: - modelname: name of the model to train - dimslist: list of tuples with integer dimension numbers - masknames: list of mask names for training set - setnames: list of set names for training set - kwargs: additional keyword arguments for fitting note: use either masksnames or setnames, not both! note: see also plot_partial_fit for a convenient plotting method!","title":"&#10551; train_partial_fitters"},{"location":"src/HistStruct/#evaluate95fitter","text":"full signature: def evaluate_fitter( self, modelname, masknames=None, setnames=None, verbose=False ) comments: evaluate the fitter for a given model input arguments: - modelname: name of the model for which to evaluate the fitter - masknames: list of mask names if the fitter should be evaluated on a subset only (e.g. for speed) - setnames: list of set names of extra histograms for which the fitter should be evaluated","title":"&#10551; evaluate_fitter"},{"location":"src/HistStruct/#evaluate95fitter95on95point","text":"full signature: def evaluate_fitter_on_point( self, modelname, point ) comments: evaluate the fitter on a given points input arguments: - modelname: name of the model for which to evaluate the fitter - points: dict matching histnames to scores (one float per histogram type) (e.g. as returned by get_scores_ls) returns: - the global score for the provided point (a float)","title":"&#10551; evaluate_fitter_on_point"},{"location":"src/HistStruct/#evaluate95fitter95on95points","text":"full signature: def evaluate_fitter_on_points( self, modelname, points ) comments: evaluate the fitter on a given set of points input arguments: - modelname: name of the model for which to evaluate the fitter - points: dict matching histnames to scores (np array of shape (nhistograms)) returns: - the global scores for the provided points","title":"&#10551; evaluate_fitter_on_points"},{"location":"src/HistStruct/#plot95histograms","text":"full signature: def plot_histograms( self, histnames=None, masknames=None, histograms=None, ncols=4, colorlist=[], labellist=[], transparencylist=[], titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs ) comments: plot the histograms in a HistStruct, optionally after masking input arguments: - histnames: list of names of the histogram types to plot (default: all) - masknames: list of list of mask names note: each element in masknames represents a set of masks to apply; the histograms passing different sets of masks are plotted in different colors - histograms: list of dicts of histnames to 2D arrays of histograms, can be used to plot a given collection of histograms directly, and bypass the histnames and masknames arguments (note: for use in the gui, not recommended outside of it) - ncols: number of columns (only relevant for 1D histograms) - colorlist: list of matplotlib colors, must have same length as masknames - labellist: list of labels for the legend, must have same legnth as masknames - transparencylist: list of transparency values, must have same length as masknames - titledict: dict mapping histogram names to titles for the subplots (default: title = histogram name) - xaxtitledict: dict mapping histogram names to x-axis titles for the subplots (default: no x-axis title) - yaxtitledict: dict mapping histogram names to y-axis titles for the subplots (default: no y-axis title) - physicalxax: bool whether to use physical x-axis range or simply use bin number (default) - kwargs: keyword arguments passed down to plot_utils.plot_sets","title":"&#10551; plot_histograms"},{"location":"src/HistStruct/#plot95histograms951d","text":"full signature: def plot_histograms_1d( self, histnames=None, masknames=None, histograms=None, ncols=4, colorlist=[], labellist=[], transparencylist=[], titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs ) comments: plot the histograms in a histstruct, optionally after masking internal helper function, use only via plot_histograms","title":"&#10551; plot_histograms_1d"},{"location":"src/HistStruct/#plot95histograms952d","text":"full signature: def plot_histograms_2d( self, histnames=None, masknames=None, histograms=None, labellist=[], titledict=None, xaxtitledict=None, yaxtitledict=None, **kwargs ) comments: plot the histograms in a histstruct, optionally after masking internal helper function, use only via plot_histograms","title":"&#10551; plot_histograms_2d"},{"location":"src/HistStruct/#plot95histograms95run","text":"full signature: def plot_histograms_run( self, histnames=None, masknames=None, histograms=None, ncols=4, titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs ) comments: plot a set of histograms in a HistStruct with a smooth color gradient. typical use case: plot a single run. note: only for 1D histograms! input arguments: - histnames: list of names of the histogram types to plot (default: all) - masknames: list mask names (typically should contain a run number mask) - histograms: dict of histnames to 2D arrays of histograms, can be used to plot a given collection of histograms directly, and bypass the histnames and masknames arguments (note: for use in the gui, not recommended outside of it. - titledict: dict mapping histogram names to titles for the subplots (default: title = histogram name) - xaxtitledict: dict mapping histogram names to x-axis titles for the subplots (default: no x-axis title) - yaxtitledict: dict mapping histogram names to y-axis titles for the subplots (default: no y-axis title) - physicalxax: bool whether to use physical x-axis range or simply use bin number (default) - kwargs: keyword arguments passed down to plot_utils.plot_hists_multi","title":"&#10551; plot_histograms_run"},{"location":"src/HistStruct/#plot95histograms95run951d","text":"full signature: def plot_histograms_run_1d( self, histnames=None, masknames=None, histograms=None, ncols=4, titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs ) comments: plot the histograms in a histstruct, optionally after masking internal helper function, use only via plot_histograms_run","title":"&#10551; plot_histograms_run_1d"},{"location":"src/HistStruct/#plot95ls","text":"full signature: def plot_ls( self, runnb, lsnb, histnames=None, histlabel=None, ncols=4, recohist=None, recohistlabel='Reconstruction', refhists=None, refhistslabel='Reference histograms', refhiststransparency=None, titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs) comments: plot the histograms in a HistStruct for a given run/ls number versus their references and/or their reconstruction input arguments: - runnb: run number - lsnb: lumisection number - histnames: names of histogram types to plot (default: all) - histlabel: legend entry for the histogram (default: run and lumisection number) - recohist: dict matching histogram names to reconstructed histograms notes: - 'reconstructed histograms' refers to e.g. autoencoder or NMF reconstructions; some models (e.g. simply looking at histogram moments) might not have this kind of reconstruction - in principle one histogram per key is expected, but still the the shape must be 2D (i.e. (1,nbins)) - in case recohist is set to a valid model name present in the current HistStruct, the reconstruction is calculated on the fly for the input histograms - recohistlabel: legend entry for the reco histograms - refhists: dict matching histogram names to reference histograms notes: - multiple histograms (i.e. a 2D array) per key are expected; in case there is only one reference histogram, it must be reshaped into (1,nbins) - refhistslabel: legend entry for the reference histograms - titledict: dict mapping histogram names to titles for the subplots (default: title = histogram name) - xaxtitledict: dict mapping histogram names to x-axis titles for the subplots (default: no x-axis title) - yaxtitledict: dict mapping histogram names to y-axis titles for the subplots (default: no y-axis title) - physicalxax: bool whether to use physical x-axis range or simply use bin number (default) - kwargs: keyword arguments passed down to plot_utils.plot_sets","title":"&#10551; plot_ls"},{"location":"src/HistStruct/#plot95run","text":"full signature: def plot_run( self, runnb, masknames=None, ncols=4, recohist=None, recohistlabel='reco', refhists=None, refhistslabel='reference', doprint=False) comments: call plot_ls for all lumisections in a given run","title":"&#10551; plot_run"},{"location":"src/HistStruct/#plot95ls951d","text":"full signature: def plot_ls_1d( self, runnb, lsnb, histnames=None, histlabel=None, ncols=4, recohist=None, recohistlabel='Reconstruction', refhists=None, refhistslabel='Reference histograms', refhiststransparency=None, titledict=None, xaxtitledict=None, physicalxax=False, yaxtitledict=None, **kwargs) comments: plot the histograms in a HistStruct for a given run/ls number versus their references and/or their reconstruction internal helper function, use only via plot_ls","title":"&#10551; plot_ls_1d"},{"location":"src/HistStruct/#plot95ls952d","text":"full signature: def plot_ls_2d( self, runnb, lsnb, histnames=None, histlabel=None, recohist=None, recohistlabel='Reconstruction', titledict=None, xaxtitledict=None, yaxtitledict=None, **kwargs) comments: plot the histograms in a HistStruct for a given run/ls number versus their reconstruction internal helper function, use only via plot_ls","title":"&#10551; plot_ls_2d"},{"location":"src/HistStruct/#plot95ls95score","text":"full signature: def plot_ls_score( self, modelname, runnb, lsnb, ncols=4, masknames=None, setnames=None, **kwargs ) comments: plot the score of a given lumisection for each histogram type compared to reference scores input arguments: - modelname: name of the model for which to retrieve the score - runnb: run number - lsnb: lumisection number - masknames: list of mask names for the reference scores - setnames: list of set names for the reference scores - kwargs: additional keyword arguments passed down to pu.plot_score_dist","title":"&#10551; plot_ls_score"},{"location":"src/HistStruct/#plot95partial95fit","text":"full signature: def plot_partial_fit( self, modelname, dims, clusters, **kwargs) comments: plot a partial fit calculated with train_partial_fitters input arguments: - modelname: name of the model for which to plot the partial fits - dims: a tuple of length 1 or 2 with integer dimension indices note: the partial fit for this dimension must have been calculated with train_partial_fitters first - clusters: a list of the different point clusters to plot each element in the list should be a dict of the form {'masknames': [list of mask names]} or {'setnames': [list of set names]} - kwargs: plot options passed down to pu.plot_fit_1d_clusters or pu.plot_fit_2d_clusters; some of them have to have the same length as clusters (e.g. colors and labels)","title":"&#10551; plot_partial_fit"},{"location":"src/HistStruct/#plot95score95dist","text":"full signature: def plot_score_dist( self, modelname, histname=None, masknames_sig=None, setnames_sig=None, masknames_bkg=None, setnames_bkg=None, **kwargs ) comments: plot a 1D score distribution input arguments: - modelname: name of the model for which to retrieve the scores - histname: type of histogram for which to retrieve the scores if None, the global scores will be retrieved - masknames_sig, setnames_sig: lists of mask or set names for signal distribution - masknames_bkg, setnames_bkg: lists of mask or set names for background distribution note: in case of multiple masks, the intersection is taken (as usual); in case of multiple sets, the union is taken! - kwargs: additional keyword arguments passed down to pu.plot_score_dist","title":"&#10551; plot_score_dist"},{"location":"src/Model/","text":"Model Model: grouping classifiers for different histogram types This class represents a general model for assigning a score to a lumisection. It consists of two distinct parts: a collection of classifiers acting on individual histogramgs (one for each type). a fitter to assign a probability density to the output scores obtained in the previous step. The types of histograms, classifiers, and fitter can be freely chosen. This class does not contain the histograms or other data; it only contains the classifiers and fitter. Use the derived class ModelInterface to make the bridge between a HistStruct (holding the data) and a Model (holding the classifiers and fitter). [class] Model comments: (no valid documentation found) \u2937 __init__ full signature: def __init__( self, histnames ) comments: initializer input arguments: - histnames: list of the histogram names for this Model. this is the only argument needed for initialization, use the functions set_classifiers and set_fitter to set the classifiers and fitter respectively. \u2937 set_classifiers full signature: def set_classifiers( self, classifiers ) comments: set the classifiers for this Model input arguments: - classifiers: dict of histnames to classifiers. the histnames must match the ones used to initialize this Model, the classifiers must be a subtype of HistogramClassifier. \u2937 set_fitter full signature: def set_fitter( self, fitter ) comments: set the fitter for this Model input arguments: - fitter: an (untrained) object of type CloudFitter \u2937 check_classifier full signature: def check_classifier( self, histname ) comments: check if a classifier was initialized input arguments: - histname: type of histogram for which to check the classifier \u2937 check_fitter full signature: def check_fitter( self ) comments: check if a fitter was initialized \u2937 train_classifier full signature: def train_classifier( self, histname, histograms, **kwargs ) comments: train a classifier input arguments: - histname: histogram name for which to train the classifier - histograms: the histograms for training, np array of shape (nhistograms,nbins) - kwargs: additional keyword arguments for training \u2937 train_classifiers full signature: def train_classifiers( self, histograms, **kwargs ) comments: train classifiers for all histograms in this Model input arguments: - histograms: the histograms for training, dict of histnames to np arrays of shape (nhistograms,nbins) - kwargs: additional keyword arguments for training \u2937 evaluate_classifier full signature: def evaluate_classifier( self, histname, histograms, mask=None ) comments: evaluate a classifier and return the score input arguments: - histname: histogram name for which to evaluate the classifier - histograms: the histograms for evaluation, np array of shape (nhistograms,nbins) - mask: a np boolean array masking the histograms to be evaluated returns: - a np array of shape (nhistograms) with the scores note: masked-out indices are set to np.nan! \u2937 evaluate_classifiers full signature: def evaluate_classifiers( self, histograms, mask=None ) comments: evaluate the classifiers and return the scores input arguments: - histograms: dict of histnames to histogram arrays (shape (nhistograms,nbins)) - mask: a np boolean array masking the histograms to be evaluated returns: - dict of histnames to scores (shape (nhistograms)) note: masked-out indices are set to np.nan! \u2937 get_point_array full signature: def get_point_array( self, points ) comments: for internal use in train_fitter and evaluate_fitter input arguments: - points: dict matching histnames to scores (np array of shape (nhistograms)) \u2937 train_fitter full signature: def train_fitter( self, points, verbose=False, **kwargs ) comments: train the fitter input arguments: - points: dict matching histnames to scores (np array of shape (nhistograms)) - kwargs: additional keyword arguments for fitting \u2937 evaluate_fitter full signature: def evaluate_fitter( self, points, mask=None, verbose=False ) comments: evaluate the fitter and return the scores input arguments: - points: dict matching histnames to scores (np array of shape (nhistograms)) - mask: a np boolean array masking the histograms to be evaluated returns: - a np array of shape (nhistograms) with the scores note: masked-out indices are set to np.nan!","title":"Model"},{"location":"src/Model/#model","text":"Model: grouping classifiers for different histogram types This class represents a general model for assigning a score to a lumisection. It consists of two distinct parts: a collection of classifiers acting on individual histogramgs (one for each type). a fitter to assign a probability density to the output scores obtained in the previous step. The types of histograms, classifiers, and fitter can be freely chosen. This class does not contain the histograms or other data; it only contains the classifiers and fitter. Use the derived class ModelInterface to make the bridge between a HistStruct (holding the data) and a Model (holding the classifiers and fitter).","title":"Model"},{"location":"src/Model/#class-model","text":"comments: (no valid documentation found)","title":"[class] Model"},{"location":"src/Model/#9595init9595","text":"full signature: def __init__( self, histnames ) comments: initializer input arguments: - histnames: list of the histogram names for this Model. this is the only argument needed for initialization, use the functions set_classifiers and set_fitter to set the classifiers and fitter respectively.","title":"&#10551; __init__"},{"location":"src/Model/#set95classifiers","text":"full signature: def set_classifiers( self, classifiers ) comments: set the classifiers for this Model input arguments: - classifiers: dict of histnames to classifiers. the histnames must match the ones used to initialize this Model, the classifiers must be a subtype of HistogramClassifier.","title":"&#10551; set_classifiers"},{"location":"src/Model/#set95fitter","text":"full signature: def set_fitter( self, fitter ) comments: set the fitter for this Model input arguments: - fitter: an (untrained) object of type CloudFitter","title":"&#10551; set_fitter"},{"location":"src/Model/#check95classifier","text":"full signature: def check_classifier( self, histname ) comments: check if a classifier was initialized input arguments: - histname: type of histogram for which to check the classifier","title":"&#10551; check_classifier"},{"location":"src/Model/#check95fitter","text":"full signature: def check_fitter( self ) comments: check if a fitter was initialized","title":"&#10551; check_fitter"},{"location":"src/Model/#train95classifier","text":"full signature: def train_classifier( self, histname, histograms, **kwargs ) comments: train a classifier input arguments: - histname: histogram name for which to train the classifier - histograms: the histograms for training, np array of shape (nhistograms,nbins) - kwargs: additional keyword arguments for training","title":"&#10551; train_classifier"},{"location":"src/Model/#train95classifiers","text":"full signature: def train_classifiers( self, histograms, **kwargs ) comments: train classifiers for all histograms in this Model input arguments: - histograms: the histograms for training, dict of histnames to np arrays of shape (nhistograms,nbins) - kwargs: additional keyword arguments for training","title":"&#10551; train_classifiers"},{"location":"src/Model/#evaluate95classifier","text":"full signature: def evaluate_classifier( self, histname, histograms, mask=None ) comments: evaluate a classifier and return the score input arguments: - histname: histogram name for which to evaluate the classifier - histograms: the histograms for evaluation, np array of shape (nhistograms,nbins) - mask: a np boolean array masking the histograms to be evaluated returns: - a np array of shape (nhistograms) with the scores note: masked-out indices are set to np.nan!","title":"&#10551; evaluate_classifier"},{"location":"src/Model/#evaluate95classifiers","text":"full signature: def evaluate_classifiers( self, histograms, mask=None ) comments: evaluate the classifiers and return the scores input arguments: - histograms: dict of histnames to histogram arrays (shape (nhistograms,nbins)) - mask: a np boolean array masking the histograms to be evaluated returns: - dict of histnames to scores (shape (nhistograms)) note: masked-out indices are set to np.nan!","title":"&#10551; evaluate_classifiers"},{"location":"src/Model/#get95point95array","text":"full signature: def get_point_array( self, points ) comments: for internal use in train_fitter and evaluate_fitter input arguments: - points: dict matching histnames to scores (np array of shape (nhistograms))","title":"&#10551; get_point_array"},{"location":"src/Model/#train95fitter","text":"full signature: def train_fitter( self, points, verbose=False, **kwargs ) comments: train the fitter input arguments: - points: dict matching histnames to scores (np array of shape (nhistograms)) - kwargs: additional keyword arguments for fitting","title":"&#10551; train_fitter"},{"location":"src/Model/#evaluate95fitter","text":"full signature: def evaluate_fitter( self, points, mask=None, verbose=False ) comments: evaluate the fitter and return the scores input arguments: - points: dict matching histnames to scores (np array of shape (nhistograms)) - mask: a np boolean array masking the histograms to be evaluated returns: - a np array of shape (nhistograms) with the scores note: masked-out indices are set to np.nan!","title":"&#10551; evaluate_fitter"},{"location":"src/ModelInterface/","text":"ModelInterface ModelInterface: extension of Model class interfaced by HistStruct This class is the interface between a Model (holding classifiers and fitters) and a HistStruct (holding histogram data). It stores the classifier and model scores for the histograms in a HistStruct. [class] ModelInterface comments: (no valid documentation found) \u2937 __init__ full signature: def __init__( self, histnames ) comments: initializer input arguments: - histnames: list of the histogram names for this Model(Interface). \u2937 __str__ full signature: def __str__( self ) comments: get a printable representation of a ModelInterface \u2937 add_setname full signature: def add_setname( self, setname ) comments: initialize empty scores for extended set input arguments: - setname: name of extended set \u2937 check_setname full signature: def check_setname( self, setname ) comments: check if a setname is present input arguments: - setname: name of the set to check \u2937 check_setnames full signature: def check_setnames( self, setnames ) comments: check if all names in a list of set names are present \u2937 check_scores full signature: def check_scores( self, histnames=None, setnames=None ) comments: check if scores are present for a given set name input arguments: - histnames: list of histogram names for which to check the scores (default: all) - setname: list of set names for which to check the scores (default: standard set) \u2937 check_globalscores full signature: def check_globalscores( self, setnames=None ) comments: check if global scores are present for a given set name input arguments: - setname: list of set names for which to check the scores (default: standard set) \u2937 evaluate_store_classifier full signature: def evaluate_store_classifier( self, histname, histograms, mask=None, setname=None ) comments: same as Model.evaluate_classifier but store the result internally input arguments: - histname: histogram name for which to evaluate the classifier - histograms: the histograms for evaluation, np array of shape (nhistograms,nbins) - mask: a np boolean array masking the histograms to be evaluated - setname: name of extended set (default: standard set) \u2937 evaluate_store_classifiers full signature: def evaluate_store_classifiers( self, histograms, mask=None, setname=None ) comments: same as Model.evaluate_classifiers but store the result internally input arguments: - histograms: dict of histnames to histogram arrays (shape (nhistograms,nbins)) - mask: a np boolean array masking the histograms to be evaluated - setname: name of extended set (default: standard set) \u2937 evaluate_store_fitter full signature: def evaluate_store_fitter( self, points, mask=None, setname=None, verbose=False ) comments: same as Model.evaluate_fitter but store the result internally input arguments: - points: dict matching histnames to scores (np array of shape (nhistograms)) - mask: a np boolean array masking the histograms to be evaluated - setname: name of extended set (default: standard set) \u2937 get_scores full signature: def get_scores( self, setnames=None, histname=None ) comments: get the scores stored internally input arguments: - setnames: list of names of extended sets (default: standard set) - histname: name of histogram type for which to get the scores if specified, an array of scores is returned. if not, a dict matching histnames to arrays of scores is returned. \u2937 get_globalscores full signature: def get_globalscores( self, setnames=None ) comments: get the global scores stored internally input arguments: - setnames: list of name of extended sets (default: standard set) \u2937 get_globalscores_mask full signature: def get_globalscores_mask( self, setnames=None, score_up=None, score_down=None ) comments: get a mask of global scores within boundaries input arguments: - setnames: list of name of extended sets (default: standard set) - score_up and score_down are upper and lower thresholds if both are not None, the mask for global scores between the boundaries are returned if score_up is None, the mask for global score > score_down are returned if score_down is None, the mask for global score < score_up are returned \u2937 get_globalscores_indices full signature: def get_globalscores_indices( self, setnames=None, score_up=None, score_down=None ) comments: get the indices of global scores within boundaries input arguments: - setnames: list of name of extended sets (default: standard set) - score_up and score_down are upper and lower thresholds if both are not None, the indices with global scores between the boundaries are returned if score_up is None, the indices with global score > score_down are returned if score_down is None, the indices with global score < score_up are returned \u2937 train_partial_fitters full signature: def train_partial_fitters( self, dimslist, points, **kwargs ) comments: train partial fitters on a given set of dimensions input arguments: - dimslist: list of tuples with integer dimension numbers - points: dict matching histnames to scores (np array of shape (nhistograms)) - kwargs: additional keyword arguments for fitting \u2937 save full signature: def save( self, path, save_classifiers=True, save_fitter=True ) comments: save a ModelInterface object to a pkl file input arguments: - path where to store the file - save_classifiers: a boolean whether to include the classifiers (alternative: only scores) - save_fitter: a boolean whether to include the fitter (alternative: only scores) \u2937 load full signature: def load( self, path, load_classifiers=True, load_fitter=True, verbose=False ) comments: load a ModelInterface object input arguments: - path to a zip file containing a ModelInterface object - load_classifiers: a boolean whether to load the classifiers if present - load_fitter: a boolean whether to load the fitter if present - verbose: boolean whether to print some information","title":"ModelInterface"},{"location":"src/ModelInterface/#modelinterface","text":"ModelInterface: extension of Model class interfaced by HistStruct This class is the interface between a Model (holding classifiers and fitters) and a HistStruct (holding histogram data). It stores the classifier and model scores for the histograms in a HistStruct.","title":"ModelInterface"},{"location":"src/ModelInterface/#class-modelinterface","text":"comments: (no valid documentation found)","title":"[class] ModelInterface"},{"location":"src/ModelInterface/#9595init9595","text":"full signature: def __init__( self, histnames ) comments: initializer input arguments: - histnames: list of the histogram names for this Model(Interface).","title":"&#10551; __init__"},{"location":"src/ModelInterface/#9595str9595","text":"full signature: def __str__( self ) comments: get a printable representation of a ModelInterface","title":"&#10551; __str__"},{"location":"src/ModelInterface/#add95setname","text":"full signature: def add_setname( self, setname ) comments: initialize empty scores for extended set input arguments: - setname: name of extended set","title":"&#10551; add_setname"},{"location":"src/ModelInterface/#check95setname","text":"full signature: def check_setname( self, setname ) comments: check if a setname is present input arguments: - setname: name of the set to check","title":"&#10551; check_setname"},{"location":"src/ModelInterface/#check95setnames","text":"full signature: def check_setnames( self, setnames ) comments: check if all names in a list of set names are present","title":"&#10551; check_setnames"},{"location":"src/ModelInterface/#check95scores","text":"full signature: def check_scores( self, histnames=None, setnames=None ) comments: check if scores are present for a given set name input arguments: - histnames: list of histogram names for which to check the scores (default: all) - setname: list of set names for which to check the scores (default: standard set)","title":"&#10551; check_scores"},{"location":"src/ModelInterface/#check95globalscores","text":"full signature: def check_globalscores( self, setnames=None ) comments: check if global scores are present for a given set name input arguments: - setname: list of set names for which to check the scores (default: standard set)","title":"&#10551; check_globalscores"},{"location":"src/ModelInterface/#evaluate95store95classifier","text":"full signature: def evaluate_store_classifier( self, histname, histograms, mask=None, setname=None ) comments: same as Model.evaluate_classifier but store the result internally input arguments: - histname: histogram name for which to evaluate the classifier - histograms: the histograms for evaluation, np array of shape (nhistograms,nbins) - mask: a np boolean array masking the histograms to be evaluated - setname: name of extended set (default: standard set)","title":"&#10551; evaluate_store_classifier"},{"location":"src/ModelInterface/#evaluate95store95classifiers","text":"full signature: def evaluate_store_classifiers( self, histograms, mask=None, setname=None ) comments: same as Model.evaluate_classifiers but store the result internally input arguments: - histograms: dict of histnames to histogram arrays (shape (nhistograms,nbins)) - mask: a np boolean array masking the histograms to be evaluated - setname: name of extended set (default: standard set)","title":"&#10551; evaluate_store_classifiers"},{"location":"src/ModelInterface/#evaluate95store95fitter","text":"full signature: def evaluate_store_fitter( self, points, mask=None, setname=None, verbose=False ) comments: same as Model.evaluate_fitter but store the result internally input arguments: - points: dict matching histnames to scores (np array of shape (nhistograms)) - mask: a np boolean array masking the histograms to be evaluated - setname: name of extended set (default: standard set)","title":"&#10551; evaluate_store_fitter"},{"location":"src/ModelInterface/#get95scores","text":"full signature: def get_scores( self, setnames=None, histname=None ) comments: get the scores stored internally input arguments: - setnames: list of names of extended sets (default: standard set) - histname: name of histogram type for which to get the scores if specified, an array of scores is returned. if not, a dict matching histnames to arrays of scores is returned.","title":"&#10551; get_scores"},{"location":"src/ModelInterface/#get95globalscores","text":"full signature: def get_globalscores( self, setnames=None ) comments: get the global scores stored internally input arguments: - setnames: list of name of extended sets (default: standard set)","title":"&#10551; get_globalscores"},{"location":"src/ModelInterface/#get95globalscores95mask","text":"full signature: def get_globalscores_mask( self, setnames=None, score_up=None, score_down=None ) comments: get a mask of global scores within boundaries input arguments: - setnames: list of name of extended sets (default: standard set) - score_up and score_down are upper and lower thresholds if both are not None, the mask for global scores between the boundaries are returned if score_up is None, the mask for global score > score_down are returned if score_down is None, the mask for global score < score_up are returned","title":"&#10551; get_globalscores_mask"},{"location":"src/ModelInterface/#get95globalscores95indices","text":"full signature: def get_globalscores_indices( self, setnames=None, score_up=None, score_down=None ) comments: get the indices of global scores within boundaries input arguments: - setnames: list of name of extended sets (default: standard set) - score_up and score_down are upper and lower thresholds if both are not None, the indices with global scores between the boundaries are returned if score_up is None, the indices with global score > score_down are returned if score_down is None, the indices with global score < score_up are returned","title":"&#10551; get_globalscores_indices"},{"location":"src/ModelInterface/#train95partial95fitters","text":"full signature: def train_partial_fitters( self, dimslist, points, **kwargs ) comments: train partial fitters on a given set of dimensions input arguments: - dimslist: list of tuples with integer dimension numbers - points: dict matching histnames to scores (np array of shape (nhistograms)) - kwargs: additional keyword arguments for fitting","title":"&#10551; train_partial_fitters"},{"location":"src/ModelInterface/#save","text":"full signature: def save( self, path, save_classifiers=True, save_fitter=True ) comments: save a ModelInterface object to a pkl file input arguments: - path where to store the file - save_classifiers: a boolean whether to include the classifiers (alternative: only scores) - save_fitter: a boolean whether to include the fitter (alternative: only scores)","title":"&#10551; save"},{"location":"src/ModelInterface/#load","text":"full signature: def load( self, path, load_classifiers=True, load_fitter=True, verbose=False ) comments: load a ModelInterface object input arguments: - path to a zip file containing a ModelInterface object - load_classifiers: a boolean whether to load the classifiers if present - load_fitter: a boolean whether to load the fitter if present - verbose: boolean whether to print some information","title":"&#10551; load"},{"location":"src/PlotStyleParser/","text":"PlotStyleParser [class] PlotStyleParser comments: (no valid documentation found) \u2937 __init__ full signature: def __init__(self, jsonfile=None) comments: (no valid documentation found) \u2937 load full signature: def load(self, jsonfile) comments: (no valid documentation found) \u2937 get_general_plot_options full signature: def get_general_plot_options(self) comments: (no valid documentation found) \u2937 get_general_plot_option full signature: def get_general_plot_option(self, attribute, histname=None) comments: (no valid documentation found) \u2937 get_title full signature: def get_title(self, histname=None) comments: (no valid documentation found) \u2937 get_titlesize full signature: def get_titlesize(self) comments: (no valid documentation found) \u2937 get_xaxtitle full signature: def get_xaxtitle(self, histname=None) comments: (no valid documentation found) \u2937 get_xaxtitlesize full signature: def get_xaxtitlesize(self) comments: (no valid documentation found) \u2937 get_physicalxax full signature: def get_physicalxax(self) comments: (no valid documentation found) \u2937 get_yaxtitle full signature: def get_yaxtitle(self, histname=None) comments: (no valid documentation found) \u2937 get_yaxtitlesize full signature: def get_yaxtitlesize(self) comments: (no valid documentation found) \u2937 get_ymaxfactor full signature: def get_ymaxfactor(self) comments: (no valid documentation found) \u2937 get_extratext full signature: def get_extratext(self, histname=None) comments: (no valid documentation found) \u2937 get_extratextsize full signature: def get_extratextsize(self) comments: (no valid documentation found) \u2937 get_legendsize full signature: def get_legendsize(self) comments: (no valid documentation found) \u2937 get_extracmstext full signature: def get_extracmstext(self) comments: (no valid documentation found) \u2937 get_cmstextsize full signature: def get_cmstextsize(self) comments: (no valid documentation found) \u2937 get_condtext full signature: def get_condtext(self) comments: (no valid documentation found) \u2937 get_condtextsize full signature: def get_condtextsize(self) comments: (no valid documentation found)","title":"PlotStyleParser"},{"location":"src/PlotStyleParser/#plotstyleparser","text":"","title":"PlotStyleParser"},{"location":"src/PlotStyleParser/#class-plotstyleparser","text":"comments: (no valid documentation found)","title":"[class] PlotStyleParser"},{"location":"src/PlotStyleParser/#9595init9595","text":"full signature: def __init__(self, jsonfile=None) comments: (no valid documentation found)","title":"&#10551; __init__"},{"location":"src/PlotStyleParser/#load","text":"full signature: def load(self, jsonfile) comments: (no valid documentation found)","title":"&#10551; load"},{"location":"src/PlotStyleParser/#get95general95plot95options","text":"full signature: def get_general_plot_options(self) comments: (no valid documentation found)","title":"&#10551; get_general_plot_options"},{"location":"src/PlotStyleParser/#get95general95plot95option","text":"full signature: def get_general_plot_option(self, attribute, histname=None) comments: (no valid documentation found)","title":"&#10551; get_general_plot_option"},{"location":"src/PlotStyleParser/#get95title","text":"full signature: def get_title(self, histname=None) comments: (no valid documentation found)","title":"&#10551; get_title"},{"location":"src/PlotStyleParser/#get95titlesize","text":"full signature: def get_titlesize(self) comments: (no valid documentation found)","title":"&#10551; get_titlesize"},{"location":"src/PlotStyleParser/#get95xaxtitle","text":"full signature: def get_xaxtitle(self, histname=None) comments: (no valid documentation found)","title":"&#10551; get_xaxtitle"},{"location":"src/PlotStyleParser/#get95xaxtitlesize","text":"full signature: def get_xaxtitlesize(self) comments: (no valid documentation found)","title":"&#10551; get_xaxtitlesize"},{"location":"src/PlotStyleParser/#get95physicalxax","text":"full signature: def get_physicalxax(self) comments: (no valid documentation found)","title":"&#10551; get_physicalxax"},{"location":"src/PlotStyleParser/#get95yaxtitle","text":"full signature: def get_yaxtitle(self, histname=None) comments: (no valid documentation found)","title":"&#10551; get_yaxtitle"},{"location":"src/PlotStyleParser/#get95yaxtitlesize","text":"full signature: def get_yaxtitlesize(self) comments: (no valid documentation found)","title":"&#10551; get_yaxtitlesize"},{"location":"src/PlotStyleParser/#get95ymaxfactor","text":"full signature: def get_ymaxfactor(self) comments: (no valid documentation found)","title":"&#10551; get_ymaxfactor"},{"location":"src/PlotStyleParser/#get95extratext","text":"full signature: def get_extratext(self, histname=None) comments: (no valid documentation found)","title":"&#10551; get_extratext"},{"location":"src/PlotStyleParser/#get95extratextsize","text":"full signature: def get_extratextsize(self) comments: (no valid documentation found)","title":"&#10551; get_extratextsize"},{"location":"src/PlotStyleParser/#get95legendsize","text":"full signature: def get_legendsize(self) comments: (no valid documentation found)","title":"&#10551; get_legendsize"},{"location":"src/PlotStyleParser/#get95extracmstext","text":"full signature: def get_extracmstext(self) comments: (no valid documentation found)","title":"&#10551; get_extracmstext"},{"location":"src/PlotStyleParser/#get95cmstextsize","text":"full signature: def get_cmstextsize(self) comments: (no valid documentation found)","title":"&#10551; get_cmstextsize"},{"location":"src/PlotStyleParser/#get95condtext","text":"full signature: def get_condtext(self) comments: (no valid documentation found)","title":"&#10551; get_condtext"},{"location":"src/PlotStyleParser/#get95condtextsize","text":"full signature: def get_condtextsize(self) comments: (no valid documentation found)","title":"&#10551; get_condtextsize"},{"location":"src/classifiers/AutoEncoder/","text":"AutoEncoder Histogram classfier based on the MSE of an autoencoder reconstruction The AutoEncoder derives from the generic HistogramClassifier. For this specific classifier, the output score of a histogram is the mean-square-error (MSE) between the original histogram and its autoencoder reconstruction. In essence, it is just a wrapper for a tensorflow model. [class] AutoEncoder comments: histogram classfier based on the MSE of an autoencoder reconstruction the AutoEncoder derives from the generic HistogramClassifier. for this specific classifier, the output score of a histogram is the mean-square-error (MSE) between the original histogram and its autoencoder reconstruction. in essence, it is just a wrapper for a tensorflow model. \u2937 __init__ full signature: def __init__( self, model=None, modelpath=None ) comments: intializer from a tensorflow model input arguments: - model: a valid tensorflow model; it does not have to be trained already, the AutoEncoder.train function will take care of this. - modelpath: path to a stored tensorflow model, it does not have to be trained already, the AutoEncoder.train function will take care of this. note: model and modelpath are alternative options, they should not both be used simultaneously. \u2937 train full signature: def train( self, histograms, doplot=True, epochs=10, batch_size=500, shuffle=False, verbose=1, validation_split=0.1, **kwargs ) comments: train the model on a given set of input histograms input arguments: - histograms: set of training histograms, a numpy array of shape (nhistograms,nbins) - doplot: boolean whether to make a plot of the loss value - others: see the keras fit function - kwargs: additional arguments passed down to keras fit function \u2937 evaluate full signature: def evaluate( self, histograms ) comments: classification of a collection of histograms based on their autoencoder reconstruction \u2937 reconstruct full signature: def reconstruct( self, histograms ) comments: return the autoencoder reconstruction of a set of histograms \u2937 save full signature: def save( self, path ) comments: save the underlying tensorflow model to a tensorflow SavedModel or H5 format. note: depending on the extension specified in path, the SavedModel or H5 format is chosen, see https://www.tensorflow.org/guide/keras/save_and_serialize \u2937 load full signature: def load( self, path, **kwargs ) comments: get an AutoEncoder instance from a saved tensorflow SavedModel or H5 file","title":"AutoEncoder"},{"location":"src/classifiers/AutoEncoder/#autoencoder","text":"Histogram classfier based on the MSE of an autoencoder reconstruction The AutoEncoder derives from the generic HistogramClassifier. For this specific classifier, the output score of a histogram is the mean-square-error (MSE) between the original histogram and its autoencoder reconstruction. In essence, it is just a wrapper for a tensorflow model.","title":"AutoEncoder"},{"location":"src/classifiers/AutoEncoder/#class-autoencoder","text":"comments: histogram classfier based on the MSE of an autoencoder reconstruction the AutoEncoder derives from the generic HistogramClassifier. for this specific classifier, the output score of a histogram is the mean-square-error (MSE) between the original histogram and its autoencoder reconstruction. in essence, it is just a wrapper for a tensorflow model.","title":"[class] AutoEncoder"},{"location":"src/classifiers/AutoEncoder/#9595init9595","text":"full signature: def __init__( self, model=None, modelpath=None ) comments: intializer from a tensorflow model input arguments: - model: a valid tensorflow model; it does not have to be trained already, the AutoEncoder.train function will take care of this. - modelpath: path to a stored tensorflow model, it does not have to be trained already, the AutoEncoder.train function will take care of this. note: model and modelpath are alternative options, they should not both be used simultaneously.","title":"&#10551; __init__"},{"location":"src/classifiers/AutoEncoder/#train","text":"full signature: def train( self, histograms, doplot=True, epochs=10, batch_size=500, shuffle=False, verbose=1, validation_split=0.1, **kwargs ) comments: train the model on a given set of input histograms input arguments: - histograms: set of training histograms, a numpy array of shape (nhistograms,nbins) - doplot: boolean whether to make a plot of the loss value - others: see the keras fit function - kwargs: additional arguments passed down to keras fit function","title":"&#10551; train"},{"location":"src/classifiers/AutoEncoder/#evaluate","text":"full signature: def evaluate( self, histograms ) comments: classification of a collection of histograms based on their autoencoder reconstruction","title":"&#10551; evaluate"},{"location":"src/classifiers/AutoEncoder/#reconstruct","text":"full signature: def reconstruct( self, histograms ) comments: return the autoencoder reconstruction of a set of histograms","title":"&#10551; reconstruct"},{"location":"src/classifiers/AutoEncoder/#save","text":"full signature: def save( self, path ) comments: save the underlying tensorflow model to a tensorflow SavedModel or H5 format. note: depending on the extension specified in path, the SavedModel or H5 format is chosen, see https://www.tensorflow.org/guide/keras/save_and_serialize","title":"&#10551; save"},{"location":"src/classifiers/AutoEncoder/#load","text":"full signature: def load( self, path, **kwargs ) comments: get an AutoEncoder instance from a saved tensorflow SavedModel or H5 file","title":"&#10551; load"},{"location":"src/classifiers/HistogramClassifier/","text":"HistogramClassifier Abstract base class for histogram classifying objects Note that all concrete histogram classifiers must inherit from HistogramClassifier! A HistogramClassifier can be any object that classifies a histogram; in more detail: - the input is a collection of histograms (of the same type), represented by a numpy array of shape (nhists,nbins) for 1D histograms or (nhists,nybins,nxbins) for 2D histograms. - the output is an array of numbers of shape (nhists). - the processing between input and output can in principle be anything, but usually some sort of discriminating power is assumed. How to make a concrete HistogramClassifier class: - define a class that inherits from HistogramClassifier - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary See also the existing examples! [class] HistogramClassifier comments: abstract base class for histogram classifying objects note that all concrete histogram classifiers must inherit from HistogramClassifier! a HistogramClassifier can be any object that classifies a histogram; in more detail: - the input is a collection of histograms (of the same type), represented by a numpy array of shape (nhists,nbins) for 1D histograms or (nhists,nybins,nxbins) for 2D histograms. - the output is an array of numbers of shape (nhists). - the processing between input and output can in principle be anything, but usually some sort of discriminating power is assumed. how to make a concrete HistogramClassifier class: - define a class that inherits from HistogramClassifier - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary see also the existing examples! \u2937 __init__ full signature: def __init__( self ) comments: empty intializer this is an @abstractmethod and must be overridden in any concrete deriving class! \u2937 train full signature: def train( self, histograms ) comments: train the classifier on a set of input histograms this is an @abstractmethod and must be overridden in any concrete deriving class! input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins). output: expected to be none. \u2937 evaluate full signature: def evaluate( self, histograms ) comments: main function used to evaluate a set of histograms this is an @abstractmethod and must be overridden in any concrete deriving class! input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins). output: expected to be a 1D numpy array of shape (nhists), one number per histogram. \u2937 save full signature: def save( self, path ) comments: save a classifier to disk specific implementation in concrete classes, here only path creation \u2937 load full signature: def load( self, path ) comments: load a classifier object from disk specific implementation in concrete classes, here only path checking","title":"HistogramClassifier"},{"location":"src/classifiers/HistogramClassifier/#histogramclassifier","text":"Abstract base class for histogram classifying objects Note that all concrete histogram classifiers must inherit from HistogramClassifier! A HistogramClassifier can be any object that classifies a histogram; in more detail: - the input is a collection of histograms (of the same type), represented by a numpy array of shape (nhists,nbins) for 1D histograms or (nhists,nybins,nxbins) for 2D histograms. - the output is an array of numbers of shape (nhists). - the processing between input and output can in principle be anything, but usually some sort of discriminating power is assumed. How to make a concrete HistogramClassifier class: - define a class that inherits from HistogramClassifier - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary See also the existing examples!","title":"HistogramClassifier"},{"location":"src/classifiers/HistogramClassifier/#class-histogramclassifier","text":"comments: abstract base class for histogram classifying objects note that all concrete histogram classifiers must inherit from HistogramClassifier! a HistogramClassifier can be any object that classifies a histogram; in more detail: - the input is a collection of histograms (of the same type), represented by a numpy array of shape (nhists,nbins) for 1D histograms or (nhists,nybins,nxbins) for 2D histograms. - the output is an array of numbers of shape (nhists). - the processing between input and output can in principle be anything, but usually some sort of discriminating power is assumed. how to make a concrete HistogramClassifier class: - define a class that inherits from HistogramClassifier - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary see also the existing examples!","title":"[class] HistogramClassifier"},{"location":"src/classifiers/HistogramClassifier/#9595init9595","text":"full signature: def __init__( self ) comments: empty intializer this is an @abstractmethod and must be overridden in any concrete deriving class!","title":"&#10551; __init__"},{"location":"src/classifiers/HistogramClassifier/#train","text":"full signature: def train( self, histograms ) comments: train the classifier on a set of input histograms this is an @abstractmethod and must be overridden in any concrete deriving class! input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins). output: expected to be none.","title":"&#10551; train"},{"location":"src/classifiers/HistogramClassifier/#evaluate","text":"full signature: def evaluate( self, histograms ) comments: main function used to evaluate a set of histograms this is an @abstractmethod and must be overridden in any concrete deriving class! input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins). output: expected to be a 1D numpy array of shape (nhists), one number per histogram.","title":"&#10551; evaluate"},{"location":"src/classifiers/HistogramClassifier/#save","text":"full signature: def save( self, path ) comments: save a classifier to disk specific implementation in concrete classes, here only path creation","title":"&#10551; save"},{"location":"src/classifiers/HistogramClassifier/#load","text":"full signature: def load( self, path ) comments: load a classifier object from disk specific implementation in concrete classes, here only path checking","title":"&#10551; load"},{"location":"src/classifiers/LandauClassifier/","text":"LandauClassifier landaufun full signature: def landaufun(x, landaumax, landauwidth, norm) comments: see https://en.wikipedia.org/wiki/Landau_distribution langaufun full signature: def langaufun(x, landaumax, landauwidth, norm, gausswidth) comments: (no valid documentation found) [class] LandauClassifier comments: (no valid documentation found) \u2937 __init__ full signature: def __init__( self, dogauss=False ) comments: (no valid documentation found) \u2937 train full signature: def train( self ) comments: (no valid documentation found) \u2937 fit full signature: def fit( self, histogram ) comments: find initial guess for the parameters \u2937 evaluate full signature: def evaluate( self, histograms ) comments: (no valid documentation found) \u2937 reconstruct full signature: def reconstruct( self, histograms ) comments: (no valid documentation found) \u2937 save full signature: def save( self, path ) comments: save the classifier \u2937 load full signature: def load( self, path, **kwargs ) comments: get a LandauClassifier instance from a pkl file","title":"LandauClassifier"},{"location":"src/classifiers/LandauClassifier/#landauclassifier","text":"","title":"LandauClassifier"},{"location":"src/classifiers/LandauClassifier/#landaufun","text":"full signature: def landaufun(x, landaumax, landauwidth, norm) comments: see https://en.wikipedia.org/wiki/Landau_distribution","title":"landaufun"},{"location":"src/classifiers/LandauClassifier/#langaufun","text":"full signature: def langaufun(x, landaumax, landauwidth, norm, gausswidth) comments: (no valid documentation found)","title":"langaufun"},{"location":"src/classifiers/LandauClassifier/#class-landauclassifier","text":"comments: (no valid documentation found)","title":"[class] LandauClassifier"},{"location":"src/classifiers/LandauClassifier/#9595init9595","text":"full signature: def __init__( self, dogauss=False ) comments: (no valid documentation found)","title":"&#10551; __init__"},{"location":"src/classifiers/LandauClassifier/#train","text":"full signature: def train( self ) comments: (no valid documentation found)","title":"&#10551; train"},{"location":"src/classifiers/LandauClassifier/#fit","text":"full signature: def fit( self, histogram ) comments: find initial guess for the parameters","title":"&#10551; fit"},{"location":"src/classifiers/LandauClassifier/#evaluate","text":"full signature: def evaluate( self, histograms ) comments: (no valid documentation found)","title":"&#10551; evaluate"},{"location":"src/classifiers/LandauClassifier/#reconstruct","text":"full signature: def reconstruct( self, histograms ) comments: (no valid documentation found)","title":"&#10551; reconstruct"},{"location":"src/classifiers/LandauClassifier/#save","text":"full signature: def save( self, path ) comments: save the classifier","title":"&#10551; save"},{"location":"src/classifiers/LandauClassifier/#load","text":"full signature: def load( self, path, **kwargs ) comments: get a LandauClassifier instance from a pkl file","title":"&#10551; load"},{"location":"src/classifiers/MaxPullClassifier/","text":"MaxPullClassifier Histogram classification based on maximum pull between test histogram and reference histogram. Specifically intended for 2D histograms, but should in principle work for 1D as well. Ssee static function 'pull' for definition of bin-per-bin pull and other notes. pull full signature: def pull( testhist, refhist ) comments: calculate bin-per-bin pull between two histograms bin-per-bin pull is defined here preliminarily as (testhist(bin)-refhist(bin))/sqrt(refhist(bin)) notes: - bins in the denominator where refhist is < 1 are set to one! This is for histograms with absolute counts, and they should not be normalized! - instead another normalization is applied: the test histogram is multiplied by sum(refhist)/sum(testhist) before computing the pulls input arguments: - testhist, refhist: numpy arrays of the same shape output: numpy array of same shape as testhist and refhist maxabspull full signature: def maxabspull( testhist, refhist, n=1 ) comments: calculate maximum of bin-per-bin pulls (in absolute value) between two histograms see definition of bin-per-bin pull in function pull (above) input arguments: - testhist, refhist: numpy arrays of the same shape - n: nubmer of largest pull values to average over (default: 1, just take single maximum) output: a float [class] MaxPullClassifier comments: histogram classification based on maximum pull between test histogram and reference histogram. specifically intended for 2D histograms, but should in principle work for 1D as well. see static function pull (above) for definition of bin-per-bin pull and other notes. \u2937 __init__ full signature: def __init__( self, nmaxpulls=1 ) comments: initializer input arguments: - nmaxpulls: number of largest pull values to average over (default: 1, just take single maximum) \u2937 set_nmaxpulls full signature: def set_nmaxpulls( self, nmaxpulls ) comments: set the nmaxpulls parameter (see also initializer) \u2937 train full signature: def train( self, refhist ) comments: 'train' the classifier, i.e. set the reference histogram. input arguments: - refhist: a numpy array of shape (1,nbins) or (1,nybins,nxbins) \u2937 evaluate full signature: def evaluate( self, histograms ) comments: classify the histograms based on their max bin-per-bin pull (in absolute value) with respect to a reference histogram \u2937 getpull full signature: def getpull( self, histogram ) comments: get the pull histogram for a given test histogram input arguments: histogram: a single histogram, i.e. numpy array of shape (nbins) for 1D or (nybins,nxbins) for 2D. output: numpy array of same shape as histogram containing bin-per-bin pull w.r.t. reference histogram","title":"MaxPullClassifier"},{"location":"src/classifiers/MaxPullClassifier/#maxpullclassifier","text":"Histogram classification based on maximum pull between test histogram and reference histogram. Specifically intended for 2D histograms, but should in principle work for 1D as well. Ssee static function 'pull' for definition of bin-per-bin pull and other notes.","title":"MaxPullClassifier"},{"location":"src/classifiers/MaxPullClassifier/#pull","text":"full signature: def pull( testhist, refhist ) comments: calculate bin-per-bin pull between two histograms bin-per-bin pull is defined here preliminarily as (testhist(bin)-refhist(bin))/sqrt(refhist(bin)) notes: - bins in the denominator where refhist is < 1 are set to one! This is for histograms with absolute counts, and they should not be normalized! - instead another normalization is applied: the test histogram is multiplied by sum(refhist)/sum(testhist) before computing the pulls input arguments: - testhist, refhist: numpy arrays of the same shape output: numpy array of same shape as testhist and refhist","title":"pull"},{"location":"src/classifiers/MaxPullClassifier/#maxabspull","text":"full signature: def maxabspull( testhist, refhist, n=1 ) comments: calculate maximum of bin-per-bin pulls (in absolute value) between two histograms see definition of bin-per-bin pull in function pull (above) input arguments: - testhist, refhist: numpy arrays of the same shape - n: nubmer of largest pull values to average over (default: 1, just take single maximum) output: a float","title":"maxabspull"},{"location":"src/classifiers/MaxPullClassifier/#class-maxpullclassifier","text":"comments: histogram classification based on maximum pull between test histogram and reference histogram. specifically intended for 2D histograms, but should in principle work for 1D as well. see static function pull (above) for definition of bin-per-bin pull and other notes.","title":"[class] MaxPullClassifier"},{"location":"src/classifiers/MaxPullClassifier/#9595init9595","text":"full signature: def __init__( self, nmaxpulls=1 ) comments: initializer input arguments: - nmaxpulls: number of largest pull values to average over (default: 1, just take single maximum)","title":"&#10551; __init__"},{"location":"src/classifiers/MaxPullClassifier/#set95nmaxpulls","text":"full signature: def set_nmaxpulls( self, nmaxpulls ) comments: set the nmaxpulls parameter (see also initializer)","title":"&#10551; set_nmaxpulls"},{"location":"src/classifiers/MaxPullClassifier/#train","text":"full signature: def train( self, refhist ) comments: 'train' the classifier, i.e. set the reference histogram. input arguments: - refhist: a numpy array of shape (1,nbins) or (1,nybins,nxbins)","title":"&#10551; train"},{"location":"src/classifiers/MaxPullClassifier/#evaluate","text":"full signature: def evaluate( self, histograms ) comments: classify the histograms based on their max bin-per-bin pull (in absolute value) with respect to a reference histogram","title":"&#10551; evaluate"},{"location":"src/classifiers/MaxPullClassifier/#getpull","text":"full signature: def getpull( self, histogram ) comments: get the pull histogram for a given test histogram input arguments: histogram: a single histogram, i.e. numpy array of shape (nbins) for 1D or (nybins,nxbins) for 2D. output: numpy array of same shape as histogram containing bin-per-bin pull w.r.t. reference histogram","title":"&#10551; getpull"},{"location":"src/classifiers/MomentClassifier/","text":"MomentClassifier [class] MomentClassifier comments: (no valid documentation found) \u2937 __init__ full signature: def __init__( self, orders=[1] ) comments: (no valid documentation found) \u2937 train full signature: def train( self, histograms ) comments: (no valid documentation found) \u2937 evaluate full signature: def evaluate( self, histograms ) comments: (no valid documentation found) \u2937 printout full signature: def printout( self, histogram ) comments: (no valid documentation found) \u2937 save full signature: def save( self, path ) comments: save the classifier \u2937 load full signature: def load( self, path, **kwargs ) comments: get a MomentClassifier instance from a pkl file","title":"MomentClassifier"},{"location":"src/classifiers/MomentClassifier/#momentclassifier","text":"","title":"MomentClassifier"},{"location":"src/classifiers/MomentClassifier/#class-momentclassifier","text":"comments: (no valid documentation found)","title":"[class] MomentClassifier"},{"location":"src/classifiers/MomentClassifier/#9595init9595","text":"full signature: def __init__( self, orders=[1] ) comments: (no valid documentation found)","title":"&#10551; __init__"},{"location":"src/classifiers/MomentClassifier/#train","text":"full signature: def train( self, histograms ) comments: (no valid documentation found)","title":"&#10551; train"},{"location":"src/classifiers/MomentClassifier/#evaluate","text":"full signature: def evaluate( self, histograms ) comments: (no valid documentation found)","title":"&#10551; evaluate"},{"location":"src/classifiers/MomentClassifier/#printout","text":"full signature: def printout( self, histogram ) comments: (no valid documentation found)","title":"&#10551; printout"},{"location":"src/classifiers/MomentClassifier/#save","text":"full signature: def save( self, path ) comments: save the classifier","title":"&#10551; save"},{"location":"src/classifiers/MomentClassifier/#load","text":"full signature: def load( self, path, **kwargs ) comments: get a MomentClassifier instance from a pkl file","title":"&#10551; load"},{"location":"src/classifiers/NMFClassifier/","text":"NMFClassifier Histogram classification based on nonnegative matrix factorization Specifically intended for 2D histograms, but should in principle work for 1D as well. It is basically a wrapper for a sklearn.decomposition.NMF instance. [class] NMFClassifier comments: histogram classification based on nonnegative matrix factorization specifically intended for 2D histograms, but should in principle work for 1D as well. it is basically a wrapper for a sklearn.decomposition.NMF instance. \u2937 __init__ full signature: def __init__( self, ncomponents=5, loss_type='mse', nmax=10 ) comments: initializer input arguments: - ncomponents: number of NMF components (aka clusters aka basis vectors) to use in the decomposition - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error) - nmax: number of largest elements to keep in error calculation TODO: add keyword arguments to pass down to sklearn.decomposition.NMF \u2937 train full signature: def train( self, histograms, doplot=True, ncols=None, title=None ) comments: train the NMF model on a given set of input histograms input arguments: - histograms: a numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins) that will be used to fit a NMF model \u2937 set_nmax full signature: def set_nmax( self, nmax ) comments: set number of largest elements to keep in mean square error calculation useful to quickly re-evaluate the model with different nmax without retraining input arguments: - nmax: number of largest elements to keep in mean square error calculation \u2937 set_loss_type full signature: def set_loss_type( self, loss_type ) comments: set loss type useful to quickly re-evaluate the model with different loss without retraining input arguments: - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error) \u2937 evaluate full signature: def evaluate( self, histograms ) comments: classify the given histograms based on the MSE with respect to their reconstructed version input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins) \u2937 get_components full signature: def get_components( self ) comments: return the NMF components (aka cluster centers aka basis vectors) output: a numpy array of shape (ncomponents,nbins) or (ncomponents,nybins,nxbins) \u2937 reconstruct full signature: def reconstruct( self, histograms ) comments: return the NMF reconstruction for a given set of histograms input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins) \u2937 save full signature: def save( self, path ) comments: save the underlying NMF model \u2937 load full signature: def load( self, path, **kwargs ) comments: get an NMFClassifier instance from a pkl file","title":"NMFClassifier"},{"location":"src/classifiers/NMFClassifier/#nmfclassifier","text":"Histogram classification based on nonnegative matrix factorization Specifically intended for 2D histograms, but should in principle work for 1D as well. It is basically a wrapper for a sklearn.decomposition.NMF instance.","title":"NMFClassifier"},{"location":"src/classifiers/NMFClassifier/#class-nmfclassifier","text":"comments: histogram classification based on nonnegative matrix factorization specifically intended for 2D histograms, but should in principle work for 1D as well. it is basically a wrapper for a sklearn.decomposition.NMF instance.","title":"[class] NMFClassifier"},{"location":"src/classifiers/NMFClassifier/#9595init9595","text":"full signature: def __init__( self, ncomponents=5, loss_type='mse', nmax=10 ) comments: initializer input arguments: - ncomponents: number of NMF components (aka clusters aka basis vectors) to use in the decomposition - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error) - nmax: number of largest elements to keep in error calculation TODO: add keyword arguments to pass down to sklearn.decomposition.NMF","title":"&#10551; __init__"},{"location":"src/classifiers/NMFClassifier/#train","text":"full signature: def train( self, histograms, doplot=True, ncols=None, title=None ) comments: train the NMF model on a given set of input histograms input arguments: - histograms: a numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins) that will be used to fit a NMF model","title":"&#10551; train"},{"location":"src/classifiers/NMFClassifier/#set95nmax","text":"full signature: def set_nmax( self, nmax ) comments: set number of largest elements to keep in mean square error calculation useful to quickly re-evaluate the model with different nmax without retraining input arguments: - nmax: number of largest elements to keep in mean square error calculation","title":"&#10551; set_nmax"},{"location":"src/classifiers/NMFClassifier/#set95loss95type","text":"full signature: def set_loss_type( self, loss_type ) comments: set loss type useful to quickly re-evaluate the model with different loss without retraining input arguments: - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error)","title":"&#10551; set_loss_type"},{"location":"src/classifiers/NMFClassifier/#evaluate","text":"full signature: def evaluate( self, histograms ) comments: classify the given histograms based on the MSE with respect to their reconstructed version input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins)","title":"&#10551; evaluate"},{"location":"src/classifiers/NMFClassifier/#get95components","text":"full signature: def get_components( self ) comments: return the NMF components (aka cluster centers aka basis vectors) output: a numpy array of shape (ncomponents,nbins) or (ncomponents,nybins,nxbins)","title":"&#10551; get_components"},{"location":"src/classifiers/NMFClassifier/#reconstruct","text":"full signature: def reconstruct( self, histograms ) comments: return the NMF reconstruction for a given set of histograms input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins)","title":"&#10551; reconstruct"},{"location":"src/classifiers/NMFClassifier/#save","text":"full signature: def save( self, path ) comments: save the underlying NMF model","title":"&#10551; save"},{"location":"src/classifiers/NMFClassifier/#load","text":"full signature: def load( self, path, **kwargs ) comments: get an NMFClassifier instance from a pkl file","title":"&#10551; load"},{"location":"src/classifiers/PCAClassifier/","text":"PCAClassifier Histogram classification based on principal component analysis It is basically a wrapper for a sklearn.decomposition.PCA instance. [class] PCAClassifier comments: histogram classification based on principal component analysis it is basically a wrapper for a sklearn.decomposition.PCA instance. \u2937 __init__ full signature: def __init__( self, ncomponents=None, svd_solver='auto', loss_type='mse', nmax=10 ) comments: initializer input arguments: - ncomponents: number of PCA components (aka clusters aka basis vectors) to use in the decomposition - svd_solver: solver method to extract the PCA components note: both ncomponents and svd_solver are arguments passed down to sklearn.decomposition.PCA, see https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error) - nmax: number of largest elements to keep in error calculation TODO: add keyword arguments to pass down to sklearn.decomposition.PCA \u2937 train full signature: def train( self, histograms ) comments: train the PCA model on a given set of input histograms input arguments: - histograms: a numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins) that will be used to fit a PCA model \u2937 set_nmax full signature: def set_nmax( self, nmax ) comments: set number of largest elements to keep in mean square error calculation useful to quickly re-evaluate the model with different nmax without retraining input arguments: - nmax: number of largest elements to keep in mean square error calculation \u2937 set_loss_type full signature: def set_loss_type( self, loss_type ) comments: set loss type useful to quickly re-evaluate the model with different loss without retraining input arguments: - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error) \u2937 evaluate full signature: def evaluate( self, histograms ) comments: classify the given histograms based on the MSE with respect to their reconstructed version input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins) \u2937 get_components full signature: def get_components( self ) comments: return the PCA components (aka cluster centers aka basis vectors) output: a numpy array of shape (ncomponents,nbins) or (ncomponents,nybins,nxbins) \u2937 reconstruct full signature: def reconstruct( self, histograms ) comments: return the PCA reconstruction for a given set of histograms input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins)","title":"PCAClassifier"},{"location":"src/classifiers/PCAClassifier/#pcaclassifier","text":"Histogram classification based on principal component analysis It is basically a wrapper for a sklearn.decomposition.PCA instance.","title":"PCAClassifier"},{"location":"src/classifiers/PCAClassifier/#class-pcaclassifier","text":"comments: histogram classification based on principal component analysis it is basically a wrapper for a sklearn.decomposition.PCA instance.","title":"[class] PCAClassifier"},{"location":"src/classifiers/PCAClassifier/#9595init9595","text":"full signature: def __init__( self, ncomponents=None, svd_solver='auto', loss_type='mse', nmax=10 ) comments: initializer input arguments: - ncomponents: number of PCA components (aka clusters aka basis vectors) to use in the decomposition - svd_solver: solver method to extract the PCA components note: both ncomponents and svd_solver are arguments passed down to sklearn.decomposition.PCA, see https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error) - nmax: number of largest elements to keep in error calculation TODO: add keyword arguments to pass down to sklearn.decomposition.PCA","title":"&#10551; __init__"},{"location":"src/classifiers/PCAClassifier/#train","text":"full signature: def train( self, histograms ) comments: train the PCA model on a given set of input histograms input arguments: - histograms: a numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins) that will be used to fit a PCA model","title":"&#10551; train"},{"location":"src/classifiers/PCAClassifier/#set95nmax","text":"full signature: def set_nmax( self, nmax ) comments: set number of largest elements to keep in mean square error calculation useful to quickly re-evaluate the model with different nmax without retraining input arguments: - nmax: number of largest elements to keep in mean square error calculation","title":"&#10551; set_nmax"},{"location":"src/classifiers/PCAClassifier/#set95loss95type","text":"full signature: def set_loss_type( self, loss_type ) comments: set loss type useful to quickly re-evaluate the model with different loss without retraining input arguments: - loss_type: choose from 'mse' (mean-squared-error) or 'chi2' (chi squared error)","title":"&#10551; set_loss_type"},{"location":"src/classifiers/PCAClassifier/#evaluate","text":"full signature: def evaluate( self, histograms ) comments: classify the given histograms based on the MSE with respect to their reconstructed version input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins)","title":"&#10551; evaluate"},{"location":"src/classifiers/PCAClassifier/#get95components","text":"full signature: def get_components( self ) comments: return the PCA components (aka cluster centers aka basis vectors) output: a numpy array of shape (ncomponents,nbins) or (ncomponents,nybins,nxbins)","title":"&#10551; get_components"},{"location":"src/classifiers/PCAClassifier/#reconstruct","text":"full signature: def reconstruct( self, histograms ) comments: return the PCA reconstruction for a given set of histograms input arguments: - histograms: numpy array of shape (nhists,nbins) or (nhists,nybins,nxbins)","title":"&#10551; reconstruct"},{"location":"src/classifiers/TemplateBasedClassifier/","text":"TemplateBasedClassifier Histogram classifier based on a direct comparison with templates (i.e. reference histograms) mseTopN_templates full signature: def mseTopN_templates( histograms, templates, n=-1 ) comments: calculate the mse between each histogram in histograms and each histogram in templates input arguments: - histograms: 2D numpy array of shape (nhistograms, nbins) - templates: 2D numpy array of shape (ntemplates,nbins) - n: integer representing the number of (sorted) bin squared errors to take into account (default: all) output: 2D numpy array of shape (nhistograms,ntemplates) holding the mseTopN between each mseTopN_min full signature: def mseTopN_min( histograms, templates, n=-1 ) comments: calculate the mse betwee a histogram and each template and return the minimum input arguments: - histograms: 2D numpy array of shape (nhistograms, nbins) - templates: 2D numpy array of shape (ntemplates,nbins) - n: integer representing the number of (sorted) bin squared errors to take into account (default: all) output: 1D numpy array of shape (nhistograms) holding the minimum mseTopN for each histogram mseTop10_min full signature: def mseTop10_min( histograms, templates ) comments: special case of above with n=10 mseTopN_avg full signature: def mseTopN_avg( histograms, templates, n=-1 ) comments: calculate the mse betwee a histogram and each template and return the average input arguments: - histograms: 2D numpy array of shape (nhistograms, nbins) - templates: 2D numpy array of shape (ntemplates,nbins) - n: integer representing the number of (sorted) bin squared errors to take into account (default: all) output: 1D numpy array of shape (nhistograms) holding the average mseTopN for each histogram mseTop10_avg full signature: def mseTop10_avg( histograms, templates ) comments: special case of above with n=10 [class] TemplateBasedClassifier comments: histogram classifier based on a direct comparison with templates (i.e. reference histograms) \u2937 __init__ full signature: def __init__( self, comparemethod='minmse' ) comments: initializer input arguments: - comparemethod: string representing the method by which to compare a histogram with a set of templates currently supported methods are: - minmse: minimum mean square error between histogram and all templates - avgmse: average mean square error between histogram and all templates \u2937 train full signature: def train( self, templates ) comments: 'train' the classifier, i.e. set the templates (reference histograms) input arguments: - templates: a 2D numpy array of shape (nhistograms,nbins) \u2937 evaluate full signature: def evaluate( self, histograms ) comments: classification of a collection of histograms based on their deviation from templates \u2937 save full signature: def save( self, path ) comments: save the classifier \u2937 load full signature: def load( self, path, **kwargs ) comments: get a TemplateBasedClassifier instance from a pkl file","title":"TemplateBasedClassifier"},{"location":"src/classifiers/TemplateBasedClassifier/#templatebasedclassifier","text":"Histogram classifier based on a direct comparison with templates (i.e. reference histograms)","title":"TemplateBasedClassifier"},{"location":"src/classifiers/TemplateBasedClassifier/#msetopn95templates","text":"full signature: def mseTopN_templates( histograms, templates, n=-1 ) comments: calculate the mse between each histogram in histograms and each histogram in templates input arguments: - histograms: 2D numpy array of shape (nhistograms, nbins) - templates: 2D numpy array of shape (ntemplates,nbins) - n: integer representing the number of (sorted) bin squared errors to take into account (default: all) output: 2D numpy array of shape (nhistograms,ntemplates) holding the mseTopN between each","title":"mseTopN_templates"},{"location":"src/classifiers/TemplateBasedClassifier/#msetopn95min","text":"full signature: def mseTopN_min( histograms, templates, n=-1 ) comments: calculate the mse betwee a histogram and each template and return the minimum input arguments: - histograms: 2D numpy array of shape (nhistograms, nbins) - templates: 2D numpy array of shape (ntemplates,nbins) - n: integer representing the number of (sorted) bin squared errors to take into account (default: all) output: 1D numpy array of shape (nhistograms) holding the minimum mseTopN for each histogram","title":"mseTopN_min"},{"location":"src/classifiers/TemplateBasedClassifier/#msetop1095min","text":"full signature: def mseTop10_min( histograms, templates ) comments: special case of above with n=10","title":"mseTop10_min"},{"location":"src/classifiers/TemplateBasedClassifier/#msetopn95avg","text":"full signature: def mseTopN_avg( histograms, templates, n=-1 ) comments: calculate the mse betwee a histogram and each template and return the average input arguments: - histograms: 2D numpy array of shape (nhistograms, nbins) - templates: 2D numpy array of shape (ntemplates,nbins) - n: integer representing the number of (sorted) bin squared errors to take into account (default: all) output: 1D numpy array of shape (nhistograms) holding the average mseTopN for each histogram","title":"mseTopN_avg"},{"location":"src/classifiers/TemplateBasedClassifier/#msetop1095avg","text":"full signature: def mseTop10_avg( histograms, templates ) comments: special case of above with n=10","title":"mseTop10_avg"},{"location":"src/classifiers/TemplateBasedClassifier/#class-templatebasedclassifier","text":"comments: histogram classifier based on a direct comparison with templates (i.e. reference histograms)","title":"[class] TemplateBasedClassifier"},{"location":"src/classifiers/TemplateBasedClassifier/#9595init9595","text":"full signature: def __init__( self, comparemethod='minmse' ) comments: initializer input arguments: - comparemethod: string representing the method by which to compare a histogram with a set of templates currently supported methods are: - minmse: minimum mean square error between histogram and all templates - avgmse: average mean square error between histogram and all templates","title":"&#10551; __init__"},{"location":"src/classifiers/TemplateBasedClassifier/#train","text":"full signature: def train( self, templates ) comments: 'train' the classifier, i.e. set the templates (reference histograms) input arguments: - templates: a 2D numpy array of shape (nhistograms,nbins)","title":"&#10551; train"},{"location":"src/classifiers/TemplateBasedClassifier/#evaluate","text":"full signature: def evaluate( self, histograms ) comments: classification of a collection of histograms based on their deviation from templates","title":"&#10551; evaluate"},{"location":"src/classifiers/TemplateBasedClassifier/#save","text":"full signature: def save( self, path ) comments: save the classifier","title":"&#10551; save"},{"location":"src/classifiers/TemplateBasedClassifier/#load","text":"full signature: def load( self, path, **kwargs ) comments: get a TemplateBasedClassifier instance from a pkl file","title":"&#10551; load"},{"location":"src/cloudfitters/CloudFitter/","text":"CloudFitter Abstract base class for all point cloud fitting algorithms Note that all concrete point cloud fitters must inherit from CloudFitter! How to make a concrete CloudFitter class: - define a class that inherits from CloudFitter - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary See also the existing examples! [class] CloudFitter comments: abstract base class for all point cloud fitting algorithms note that all concrete point cloud fitters must inherit from CloudFitter! how to make a concrete CloudFitter class: - define a class that inherits from CloudFitter - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary see also the existing examples! \u2937 __init__ full signature: def __init__( self ) comments: empty intializer this is an @abstractmethod and must be overridden in any concrete deriving class! \u2937 fit full signature: def fit( self, points ) comments: input arguments: - points: 2D numpy array of shape (npoints,ndims) \u2937 pdf full signature: def pdf( self, points ) comments: evaluate the pdf (probability density function) at given points this is an @abstractmethod and must be overridden in any concrete deriving class! input arguments: - points: a 2D numpy array of shape (npoints,ndims) output: a 1D array of shape (npoints)","title":"CloudFitter"},{"location":"src/cloudfitters/CloudFitter/#cloudfitter","text":"Abstract base class for all point cloud fitting algorithms Note that all concrete point cloud fitters must inherit from CloudFitter! How to make a concrete CloudFitter class: - define a class that inherits from CloudFitter - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary See also the existing examples!","title":"CloudFitter"},{"location":"src/cloudfitters/CloudFitter/#class-cloudfitter","text":"comments: abstract base class for all point cloud fitting algorithms note that all concrete point cloud fitters must inherit from CloudFitter! how to make a concrete CloudFitter class: - define a class that inherits from CloudFitter - make sure all functions with @abstractmethod are implemented in your class - it is recommended to start each overriding function with a call to super(), but this is not strictly necessary see also the existing examples!","title":"[class] CloudFitter"},{"location":"src/cloudfitters/CloudFitter/#9595init9595","text":"full signature: def __init__( self ) comments: empty intializer this is an @abstractmethod and must be overridden in any concrete deriving class!","title":"&#10551; __init__"},{"location":"src/cloudfitters/CloudFitter/#fit","text":"full signature: def fit( self, points ) comments: input arguments: - points: 2D numpy array of shape (npoints,ndims)","title":"&#10551; fit"},{"location":"src/cloudfitters/CloudFitter/#pdf","text":"full signature: def pdf( self, points ) comments: evaluate the pdf (probability density function) at given points this is an @abstractmethod and must be overridden in any concrete deriving class! input arguments: - points: a 2D numpy array of shape (npoints,ndims) output: a 1D array of shape (npoints)","title":"&#10551; pdf"},{"location":"src/cloudfitters/ExponentialFitter/","text":"ExponentialFitter Class for fitting an exponential distribution to a point cloud An exponential distribution in N dimensions is fully determined by an N-dimensional vector, representing the N-dimensional decay parameter (or lambda parameter) of the distribution. [class] ExponentialFitter comments: class for fitting an exponential distribution to a point cloud parameters - l: multidimensional lambda parameter of exponential \u2937 __init__ full signature: def __init__(self) comments: empty constructor input arguments: - points: a np array of shape (npoints,ndims) \u2937 fit full signature: def fit(self, points) comments: fit to a set of points input arguments: - points: a np array of shape (npoints,ndims) \u2937 pdf full signature: def pdf(self, points) comments: get pdf at points","title":"ExponentialFitter"},{"location":"src/cloudfitters/ExponentialFitter/#exponentialfitter","text":"Class for fitting an exponential distribution to a point cloud An exponential distribution in N dimensions is fully determined by an N-dimensional vector, representing the N-dimensional decay parameter (or lambda parameter) of the distribution.","title":"ExponentialFitter"},{"location":"src/cloudfitters/ExponentialFitter/#class-exponentialfitter","text":"comments: class for fitting an exponential distribution to a point cloud parameters - l: multidimensional lambda parameter of exponential","title":"[class] ExponentialFitter"},{"location":"src/cloudfitters/ExponentialFitter/#9595init9595","text":"full signature: def __init__(self) comments: empty constructor input arguments: - points: a np array of shape (npoints,ndims)","title":"&#10551; __init__"},{"location":"src/cloudfitters/ExponentialFitter/#fit","text":"full signature: def fit(self, points) comments: fit to a set of points input arguments: - points: a np array of shape (npoints,ndims)","title":"&#10551; fit"},{"location":"src/cloudfitters/ExponentialFitter/#pdf","text":"full signature: def pdf(self, points) comments: get pdf at points","title":"&#10551; pdf"},{"location":"src/cloudfitters/GaussianKdeFitter/","text":"GaussianKdeFitter Class for fitting a gaussian kernel density to a point cloud Basically a wrapper for scipy.stats.gaussian_kde. A gaussian kernel density can be thought of as a sum of little (potentially multidimensional) gaussians, each one centered at one of the points in the cloud. Hence, the resulting distribution is a sort of smoothed version of the discrete point cloud. [class] GaussianKdeFitter comments: class for fitting a gaussian kernel density to a point cloud basically a wrapper for scipy.stats.gaussian_kde. parameters - kernel: scipy.stats.gaussian_kde object - cov: covariance matrix (use np.cov for now, maybe later replace by internal kernel.covariance) \u2937 __init__ full signature: def __init__(self) comments: empty constructor \u2937 fit full signature: def fit(self, points, bw_method='scott', bw_scott_factor=None) comments: fit to a set of points input arguments: - points: a np array of shape (npoints,ndims) - bw_method: method to calculate the bandwidth of the gaussians, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html - bw_scott_factor: additional multiplication factor applied to bandwidth in case it is set to 'scott' \u2937 pdf full signature: def pdf(self,points) comments: get pdf at points","title":"GaussianKdeFitter"},{"location":"src/cloudfitters/GaussianKdeFitter/#gaussiankdefitter","text":"Class for fitting a gaussian kernel density to a point cloud Basically a wrapper for scipy.stats.gaussian_kde. A gaussian kernel density can be thought of as a sum of little (potentially multidimensional) gaussians, each one centered at one of the points in the cloud. Hence, the resulting distribution is a sort of smoothed version of the discrete point cloud.","title":"GaussianKdeFitter"},{"location":"src/cloudfitters/GaussianKdeFitter/#class-gaussiankdefitter","text":"comments: class for fitting a gaussian kernel density to a point cloud basically a wrapper for scipy.stats.gaussian_kde. parameters - kernel: scipy.stats.gaussian_kde object - cov: covariance matrix (use np.cov for now, maybe later replace by internal kernel.covariance)","title":"[class] GaussianKdeFitter"},{"location":"src/cloudfitters/GaussianKdeFitter/#9595init9595","text":"full signature: def __init__(self) comments: empty constructor","title":"&#10551; __init__"},{"location":"src/cloudfitters/GaussianKdeFitter/#fit","text":"full signature: def fit(self, points, bw_method='scott', bw_scott_factor=None) comments: fit to a set of points input arguments: - points: a np array of shape (npoints,ndims) - bw_method: method to calculate the bandwidth of the gaussians, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html - bw_scott_factor: additional multiplication factor applied to bandwidth in case it is set to 'scott'","title":"&#10551; fit"},{"location":"src/cloudfitters/GaussianKdeFitter/#pdf","text":"full signature: def pdf(self,points) comments: get pdf at points","title":"&#10551; pdf"},{"location":"src/cloudfitters/HyperRectangleFitter/","text":"HyperRectangleFitter Simple fitter making a hard cut in each dimension calculate_cut_values full signature: def calculate_cut_values( values, quantile, side='both' ) comments: calculate the appropriate cut values to discard a given quantile of values input arguments: - values: a 1D numpy array - quantile: quantile of values to discard, a float between 0 and 1 (or between 0 and 0.5 for side='both') - side: either 'both', 'down' or 'up' for 'up', the cut will discard the quantile highest values, for 'down', cut will discard the quantile lowest values, for 'both', the cut(s) will discard the quantile values both at the high and low end. returns: - a tuple of shape (lower cut, upper cut), with None entries if not applicable [class] HyperRectangleFitter comments: Simple fitter making a hard cut in each dimension \u2937 __init__ full signature: def __init__(self) comments: empty constructor \u2937 fit full signature: def fit(self, points, quantiles=0, side='both', verbose=False) comments: fit to a set of points input arguments: - points: a np array of shape (npoints,ndims) - quantiles: quantiles of values to discard. can either be a float between 0 and 1 (applied in all dimensions), or a list of such floats with same length as number of dimensions in points. (note: for side='both', quantiles above 0.5 will discard everything) - side: either 'both', 'down' or 'up' for 'up', the cut will discard the quantile highest values, for 'down', cut will discard the quantile lowest values, for 'both', the cut(s) will discard the quantile values both at the high and low end. \u2937 apply_cuts full signature: def apply_cuts(self, point) comments: apply the cuts to a point and return whether it passes them input arguments: - point: a 1D numpy array of shape (ndims,) returns: - boolean \u2937 pdf full signature: def pdf(self, points) comments: get pdf at points note that the pdf is either 0 (does not pass cuts) or 1 (passes cuts)","title":"HyperRectangleFitter"},{"location":"src/cloudfitters/HyperRectangleFitter/#hyperrectanglefitter","text":"Simple fitter making a hard cut in each dimension","title":"HyperRectangleFitter"},{"location":"src/cloudfitters/HyperRectangleFitter/#calculate95cut95values","text":"full signature: def calculate_cut_values( values, quantile, side='both' ) comments: calculate the appropriate cut values to discard a given quantile of values input arguments: - values: a 1D numpy array - quantile: quantile of values to discard, a float between 0 and 1 (or between 0 and 0.5 for side='both') - side: either 'both', 'down' or 'up' for 'up', the cut will discard the quantile highest values, for 'down', cut will discard the quantile lowest values, for 'both', the cut(s) will discard the quantile values both at the high and low end. returns: - a tuple of shape (lower cut, upper cut), with None entries if not applicable","title":"calculate_cut_values"},{"location":"src/cloudfitters/HyperRectangleFitter/#class-hyperrectanglefitter","text":"comments: Simple fitter making a hard cut in each dimension","title":"[class] HyperRectangleFitter"},{"location":"src/cloudfitters/HyperRectangleFitter/#9595init9595","text":"full signature: def __init__(self) comments: empty constructor","title":"&#10551; __init__"},{"location":"src/cloudfitters/HyperRectangleFitter/#fit","text":"full signature: def fit(self, points, quantiles=0, side='both', verbose=False) comments: fit to a set of points input arguments: - points: a np array of shape (npoints,ndims) - quantiles: quantiles of values to discard. can either be a float between 0 and 1 (applied in all dimensions), or a list of such floats with same length as number of dimensions in points. (note: for side='both', quantiles above 0.5 will discard everything) - side: either 'both', 'down' or 'up' for 'up', the cut will discard the quantile highest values, for 'down', cut will discard the quantile lowest values, for 'both', the cut(s) will discard the quantile values both at the high and low end.","title":"&#10551; fit"},{"location":"src/cloudfitters/HyperRectangleFitter/#apply95cuts","text":"full signature: def apply_cuts(self, point) comments: apply the cuts to a point and return whether it passes them input arguments: - point: a 1D numpy array of shape (ndims,) returns: - boolean","title":"&#10551; apply_cuts"},{"location":"src/cloudfitters/HyperRectangleFitter/#pdf","text":"full signature: def pdf(self, points) comments: get pdf at points note that the pdf is either 0 (does not pass cuts) or 1 (passes cuts)","title":"&#10551; pdf"},{"location":"src/cloudfitters/IdentityFitter/","text":"IdentityFitter Class for using classifier scores directly as global scores [class] IdentityFitter comments: class for propagating classifier output scores (e.g. MSE) to global lumisection score \u2937 __init__ full signature: def __init__(self) comments: empty constructor \u2937 fit full signature: def fit(self, points) comments: fit to a set of points input arguments: - points: a numpy array of shape (npoints,ndims) note that ndims is supposed to be 1, else this type of classifier is not well defined. \u2937 pdf full signature: def pdf(self, points) comments: get pdf at points \u2937 save full signature: def save(self, path) comments: save this fitter (dummy for now since nothing to be saved) \u2937 load full signature: def load(self, path) comments: load this fitter (dummy for now since nothing to be loaded)","title":"IdentityFitter"},{"location":"src/cloudfitters/IdentityFitter/#identityfitter","text":"Class for using classifier scores directly as global scores","title":"IdentityFitter"},{"location":"src/cloudfitters/IdentityFitter/#class-identityfitter","text":"comments: class for propagating classifier output scores (e.g. MSE) to global lumisection score","title":"[class] IdentityFitter"},{"location":"src/cloudfitters/IdentityFitter/#9595init9595","text":"full signature: def __init__(self) comments: empty constructor","title":"&#10551; __init__"},{"location":"src/cloudfitters/IdentityFitter/#fit","text":"full signature: def fit(self, points) comments: fit to a set of points input arguments: - points: a numpy array of shape (npoints,ndims) note that ndims is supposed to be 1, else this type of classifier is not well defined.","title":"&#10551; fit"},{"location":"src/cloudfitters/IdentityFitter/#pdf","text":"full signature: def pdf(self, points) comments: get pdf at points","title":"&#10551; pdf"},{"location":"src/cloudfitters/IdentityFitter/#save","text":"full signature: def save(self, path) comments: save this fitter (dummy for now since nothing to be saved)","title":"&#10551; save"},{"location":"src/cloudfitters/IdentityFitter/#load","text":"full signature: def load(self, path) comments: load this fitter (dummy for now since nothing to be loaded)","title":"&#10551; load"},{"location":"src/cloudfitters/LogNormalFitter/","text":"LogNormalFitter Class for fitting a log-normal distribution to a point cloud A log-normal distribution is constructed by fitting a normal distribution to the logarithm of the point coordinates. [class] LogNormalFitter comments: class for fitting a log-normal distribution to a point cloud parameters: - mean: multidim mean of underlying normal - cov: multidim covariance matrix of underlying normal - mvn: scipy.stats multivariate_normal object built from the mean and cov \u2937 __init__ full signature: def __init__(self) comments: empty constructor \u2937 fit full signature: def fit(self, points) comments: fit to a set of points input arguments: - points: a np array of shape (npoints,ndims) \u2937 pdf full signature: def pdf(self,points) comments: get pdf at points","title":"LogNormalFitter"},{"location":"src/cloudfitters/LogNormalFitter/#lognormalfitter","text":"Class for fitting a log-normal distribution to a point cloud A log-normal distribution is constructed by fitting a normal distribution to the logarithm of the point coordinates.","title":"LogNormalFitter"},{"location":"src/cloudfitters/LogNormalFitter/#class-lognormalfitter","text":"comments: class for fitting a log-normal distribution to a point cloud parameters: - mean: multidim mean of underlying normal - cov: multidim covariance matrix of underlying normal - mvn: scipy.stats multivariate_normal object built from the mean and cov","title":"[class] LogNormalFitter"},{"location":"src/cloudfitters/LogNormalFitter/#9595init9595","text":"full signature: def __init__(self) comments: empty constructor","title":"&#10551; __init__"},{"location":"src/cloudfitters/LogNormalFitter/#fit","text":"full signature: def fit(self, points) comments: fit to a set of points input arguments: - points: a np array of shape (npoints,ndims)","title":"&#10551; fit"},{"location":"src/cloudfitters/LogNormalFitter/#pdf","text":"full signature: def pdf(self,points) comments: get pdf at points","title":"&#10551; pdf"},{"location":"src/cloudfitters/PCAGaussianFitter/","text":"PCAGaussianFitter Class for fitting a multidimensional gaussian distribution to a PCA-reduced point cloud Instead of fitting the full (high-dimensional) point cloud, a PCA-based dimensionality reduction is first applied on it. This has the advantage that the fit can be visualised correctly (in case of 2 reduced dimensions), instead of only projections of it. The potential disadvantage is that the PCA reduction might distort the relative separations. [class] PCAGaussianFitter comments: class for fitting a gaussian distribution to a PCA-reduced point cloud parameters - pca: sklearn.decomposition.pca object - mean: multidim mean of normal distribution - cov: multidim covariance matrix of normal distribution - mvn: scipy.stats multivariate_normal object built from mean and cov \u2937 __init__ full signature: def __init__(self) comments: empty constructor input arguments: \u2937 fit full signature: def fit(self, points, npcadims=2) comments: fit to a set of points input arguments: - points: a np array of shape (npoints,ndims) - npcadims: number of PCA compoments to keep \u2937 pdf full signature: def pdf(self, points) comments: get pdf at points note: points can be both of shape (npoints,ndims) or of shape (npoints,npcadims); in the latter case it is assumed that the points are already PCA-transformed, and only the gaussian kernel density is applied on them. \u2937 transform full signature: def transform(self, points) comments: perform PCA transformation","title":"PCAGaussianFitter"},{"location":"src/cloudfitters/PCAGaussianFitter/#pcagaussianfitter","text":"Class for fitting a multidimensional gaussian distribution to a PCA-reduced point cloud Instead of fitting the full (high-dimensional) point cloud, a PCA-based dimensionality reduction is first applied on it. This has the advantage that the fit can be visualised correctly (in case of 2 reduced dimensions), instead of only projections of it. The potential disadvantage is that the PCA reduction might distort the relative separations.","title":"PCAGaussianFitter"},{"location":"src/cloudfitters/PCAGaussianFitter/#class-pcagaussianfitter","text":"comments: class for fitting a gaussian distribution to a PCA-reduced point cloud parameters - pca: sklearn.decomposition.pca object - mean: multidim mean of normal distribution - cov: multidim covariance matrix of normal distribution - mvn: scipy.stats multivariate_normal object built from mean and cov","title":"[class] PCAGaussianFitter"},{"location":"src/cloudfitters/PCAGaussianFitter/#9595init9595","text":"full signature: def __init__(self) comments: empty constructor input arguments:","title":"&#10551; __init__"},{"location":"src/cloudfitters/PCAGaussianFitter/#fit","text":"full signature: def fit(self, points, npcadims=2) comments: fit to a set of points input arguments: - points: a np array of shape (npoints,ndims) - npcadims: number of PCA compoments to keep","title":"&#10551; fit"},{"location":"src/cloudfitters/PCAGaussianFitter/#pdf","text":"full signature: def pdf(self, points) comments: get pdf at points note: points can be both of shape (npoints,ndims) or of shape (npoints,npcadims); in the latter case it is assumed that the points are already PCA-transformed, and only the gaussian kernel density is applied on them.","title":"&#10551; pdf"},{"location":"src/cloudfitters/PCAGaussianFitter/#transform","text":"full signature: def transform(self, points) comments: perform PCA transformation","title":"&#10551; transform"},{"location":"src/cloudfitters/PCAGaussianKdeFitter/","text":"PCAGaussianKdeFitter Class for fitting a gaussian kernel density to a PCA-reduced point cloud Extension of GaussianKdeFitter: instead of fitting the full point cloud, a PCA-based dimensionality reduction is first applied on it. This has the advantage that the fit can be visualised correctly (in case of 2 reduced dimensions), instead of only projections of it. The potential disadvantage is that the PCA reduction might distort the relative separations. [class] PCAGaussianKdeFitter comments: class for fitting a gaussian kernel density to a PCA-reduced point cloud basically a wrapper for sklean.decomposition.PCA + scipy.stats.gaussian_kde. parameters - pca: sklearn.decomposition.pca object - kernel: scipy.stats.gaussian_kde object - cov: covariance matrix (use np.cov for now, maybe later replace by internal kernel.covariance) \u2937 __init__ full signature: def __init__(self) comments: empty constructor \u2937 fit full signature: def fit(self, points, npcadims=2, bw_method='scott', bw_scott_factor=None) comments: fit to a set of points input arguments: - points: a np array of shape (npoints,ndims) - npcadims: number of PCA compoments to keep - bw_method: method to calculate the bandwidth of the gaussians, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html - bw_scott_factor: additional multiplication factor applied to bandwidth in case it is set to 'scott' \u2937 pdf full signature: def pdf(self, points) comments: get pdf at points note: points can be both of shape (npoints,ndims) or of shape (npoints,npcadims); in the latter case it is assumed that the points are already PCA-transformed, and only the gaussian kernel density is applied on them. \u2937 transform full signature: def transform(self, points) comments: perform PCA transformation","title":"PCAGaussianKdeFitter"},{"location":"src/cloudfitters/PCAGaussianKdeFitter/#pcagaussiankdefitter","text":"Class for fitting a gaussian kernel density to a PCA-reduced point cloud Extension of GaussianKdeFitter: instead of fitting the full point cloud, a PCA-based dimensionality reduction is first applied on it. This has the advantage that the fit can be visualised correctly (in case of 2 reduced dimensions), instead of only projections of it. The potential disadvantage is that the PCA reduction might distort the relative separations.","title":"PCAGaussianKdeFitter"},{"location":"src/cloudfitters/PCAGaussianKdeFitter/#class-pcagaussiankdefitter","text":"comments: class for fitting a gaussian kernel density to a PCA-reduced point cloud basically a wrapper for sklean.decomposition.PCA + scipy.stats.gaussian_kde. parameters - pca: sklearn.decomposition.pca object - kernel: scipy.stats.gaussian_kde object - cov: covariance matrix (use np.cov for now, maybe later replace by internal kernel.covariance)","title":"[class] PCAGaussianKdeFitter"},{"location":"src/cloudfitters/PCAGaussianKdeFitter/#9595init9595","text":"full signature: def __init__(self) comments: empty constructor","title":"&#10551; __init__"},{"location":"src/cloudfitters/PCAGaussianKdeFitter/#fit","text":"full signature: def fit(self, points, npcadims=2, bw_method='scott', bw_scott_factor=None) comments: fit to a set of points input arguments: - points: a np array of shape (npoints,ndims) - npcadims: number of PCA compoments to keep - bw_method: method to calculate the bandwidth of the gaussians, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html - bw_scott_factor: additional multiplication factor applied to bandwidth in case it is set to 'scott'","title":"&#10551; fit"},{"location":"src/cloudfitters/PCAGaussianKdeFitter/#pdf","text":"full signature: def pdf(self, points) comments: get pdf at points note: points can be both of shape (npoints,ndims) or of shape (npoints,npcadims); in the latter case it is assumed that the points are already PCA-transformed, and only the gaussian kernel density is applied on them.","title":"&#10551; pdf"},{"location":"src/cloudfitters/PCAGaussianKdeFitter/#transform","text":"full signature: def transform(self, points) comments: perform PCA transformation","title":"&#10551; transform"},{"location":"src/cloudfitters/SeminormalFitter/","text":"SeminormalFitter Class for fitting a 'seminormal' distribution to a point cloud This is not strictly speaking a probability distribution, only the first quadrant of the result of fitting a normal distribution to the data + its mirror image wrt the origin. [class] SeminormalFitter comments: class for fitting a 'seminormal' distribution to a point cloud this is not strictly speaking a probability distribution, only the first quadrant of the result of fitting a normal distribution to the data + its mirror image wrt the origin. parameters - cov: multidim covariance matrix of normal distribution - mvn: scipy.stats multivariate_normal object built from the cov \u2937 __init__ full signature: def __init__(self) comments: empty constructor \u2937 fit full signature: def fit(self,points) comments: make the fit input arguments: - points: a np array of shape (npoints,ndims) \u2937 pdf full signature: def pdf(self,points) comments: get pdf at points input arguments: - points: a np array of shape (npoints,ndims) \u2937 save full signature: def save(self,path) comments: save the covariance matrix as a .npy file specified by path \u2937 load full signature: def load(self,path) comments: load a covariance matrix from a .npy file specified by path and build the fit from it","title":"SeminormalFitter"},{"location":"src/cloudfitters/SeminormalFitter/#seminormalfitter","text":"Class for fitting a 'seminormal' distribution to a point cloud This is not strictly speaking a probability distribution, only the first quadrant of the result of fitting a normal distribution to the data + its mirror image wrt the origin.","title":"SeminormalFitter"},{"location":"src/cloudfitters/SeminormalFitter/#class-seminormalfitter","text":"comments: class for fitting a 'seminormal' distribution to a point cloud this is not strictly speaking a probability distribution, only the first quadrant of the result of fitting a normal distribution to the data + its mirror image wrt the origin. parameters - cov: multidim covariance matrix of normal distribution - mvn: scipy.stats multivariate_normal object built from the cov","title":"[class] SeminormalFitter"},{"location":"src/cloudfitters/SeminormalFitter/#9595init9595","text":"full signature: def __init__(self) comments: empty constructor","title":"&#10551; __init__"},{"location":"src/cloudfitters/SeminormalFitter/#fit","text":"full signature: def fit(self,points) comments: make the fit input arguments: - points: a np array of shape (npoints,ndims)","title":"&#10551; fit"},{"location":"src/cloudfitters/SeminormalFitter/#pdf","text":"full signature: def pdf(self,points) comments: get pdf at points input arguments: - points: a np array of shape (npoints,ndims)","title":"&#10551; pdf"},{"location":"src/cloudfitters/SeminormalFitter/#save","text":"full signature: def save(self,path) comments: save the covariance matrix as a .npy file specified by path","title":"&#10551; save"},{"location":"src/cloudfitters/SeminormalFitter/#load","text":"full signature: def load(self,path) comments: load a covariance matrix from a .npy file specified by path and build the fit from it","title":"&#10551; load"},{"location":"tutorials/autoencoder_1d/","text":"Train and test an autoencoder on a set of 1D monitoring elements This notebook walks you through the basics of the autoencoder approach to detecting anomalies for 1D monitoring elements. It consists of the following steps: Loading the data Applying selections (e.g. DCS-bit on and sufficient statistics) Preprocessing (e.g. normalizing) Building an autoencoder model with keras Investigate the output ### imports # external modules import sys import os import numpy as np import matplotlib.pyplot as plt from keras import backend as K import tensorflow as tf from tensorflow.keras.models import load_model # local modules sys.path.append('../utils') import dataframe_utils as dfu import hist_utils as hu import autoencoder_utils as aeu import plot_utils as pu import generate_data_utils as gdu sys.path.append('../src') import DataLoader 2022-07-26 17:21:47.396941: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc8-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc8-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc8-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.30-e5b21/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc8-opt/lib64/R/library/readr/rcon 2022-07-26 17:21:47.397006: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. ### read the data # note: this cell assumes you have a csv file stored at the specified location, # containing only histograms of the specified type; # see the tutorial read_and_write_data for examples on how to create such files! histname = 'chargeInner_PXLayer_2' filename = 'DF2017_'+histname+'.csv' datadir = '../data' dloader = DataLoader.DataLoader() df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_2.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. raw input data shape: (225954, 102) ### filtering: select only DCS-bit on data and filter out low statistics df = dfu.select_dcson(df) print('number of passing lumisections after DCS selection: {}'.format( len(df) )) df = dfu.select_highstat(df, entries_to_bins_ratio=100) print('number of passing lumisections after high statistics selection: {}'.format( len(df) )) number of passing lumisections after DCS selection: 215144 number of passing lumisections after high statistics selection: 211371 ### preprocessing of the data: rebinning and normalizing rebinningfactor = 1 X_train = hu.preparedatafromdf(df, rebinningfactor=rebinningfactor, donormalize=True, doplot=True) (ntrain,nbins) = X_train.shape print('size of training set: '+str(X_train.shape)) size of training set: (211371, 102) ### build the model and train it input_size = X_train.shape[1] arch = [int(X_train.shape[1]/2.)] act = ['tanh']*len(arch) opt = 'adam' loss = aeu.mseTop10 autoencoder = aeu.getautoencoder(input_size,arch,act,opt,loss) history = autoencoder.fit(X_train, X_train, epochs=20, batch_size=500, shuffle=False, verbose=1, validation_split=0.1) pu.plot_loss(history, title = 'model loss') 2022-07-26 17:22:34.789066: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc8-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc8-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc8-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.30-e5b21/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc8-opt/lib64/R/library/readr/rcon 2022-07-26 17:22:34.789185: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303) 2022-07-26 17:22:34.789326: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (59ec73fb7463): /proc/driver/nvidia/version does not exist 2022-07-26 17:22:34.791360: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 51) 5253 _________________________________________________________________ dense_1 (Dense) (None, 102) 5304 ================================================================= Total params: 10,557 Trainable params: 10,557 Non-trainable params: 0 _________________________________________________________________ 2022-07-26 17:22:35.219952: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2) 2022-07-26 17:22:35.221458: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2399845000 Hz Epoch 1/20 381/381 [==============================] - 4s 7ms/step - loss: 1.3303e-04 - val_loss: 1.8835e-05 Epoch 2/20 381/381 [==============================] - 3s 7ms/step - loss: 3.5243e-05 - val_loss: 8.5544e-06 Epoch 3/20 381/381 [==============================] - 3s 8ms/step - loss: 1.7558e-05 - val_loss: 4.2058e-06 Epoch 4/20 381/381 [==============================] - 3s 9ms/step - loss: 1.0271e-05 - val_loss: 2.6051e-06 Epoch 5/20 381/381 [==============================] - 3s 9ms/step - loss: 7.0206e-06 - val_loss: 1.8891e-06 Epoch 6/20 381/381 [==============================] - 3s 9ms/step - loss: 5.1686e-06 - val_loss: 1.5451e-06 Epoch 7/20 381/381 [==============================] - 3s 9ms/step - loss: 4.0133e-06 - val_loss: 1.4708e-06 Epoch 8/20 381/381 [==============================] - 4s 11ms/step - loss: 3.1906e-06 - val_loss: 1.3947e-06 Epoch 9/20 381/381 [==============================] - 4s 12ms/step - loss: 2.6316e-06 - val_loss: 1.3483e-06 Epoch 10/20 381/381 [==============================] - 4s 12ms/step - loss: 2.1945e-06 - val_loss: 1.2472e-06 Epoch 11/20 381/381 [==============================] - 5s 13ms/step - loss: 1.9028e-06 - val_loss: 1.1730e-06 Epoch 12/20 381/381 [==============================] - 5s 14ms/step - loss: 1.7457e-06 - val_loss: 1.1381e-06 Epoch 13/20 381/381 [==============================] - 5s 13ms/step - loss: 1.5103e-06 - val_loss: 1.1024e-06 Epoch 14/20 381/381 [==============================] - 5s 12ms/step - loss: 1.4419e-06 - val_loss: 9.9945e-07 Epoch 15/20 381/381 [==============================] - 5s 12ms/step - loss: 1.3293e-06 - val_loss: 9.4934e-07 Epoch 16/20 381/381 [==============================] - 6s 17ms/step - loss: 1.3654e-06 - val_loss: 9.3066e-07 Epoch 17/20 381/381 [==============================] - 6s 16ms/step - loss: 1.1381e-06 - val_loss: 8.6669e-07 Epoch 18/20 381/381 [==============================] - 6s 17ms/step - loss: 1.0834e-06 - val_loss: 7.9371e-07 Epoch 19/20 381/381 [==============================] - 7s 17ms/step - loss: 1.0866e-06 - val_loss: 7.6348e-07 Epoch 20/20 381/381 [==============================] - 6s 17ms/step - loss: 1.0285e-06 - val_loss: 7.3859e-07 (<Figure size 432x288 with 1 Axes>, <AxesSubplot:title={'center':'model loss'}, xlabel='Epoch', ylabel='Loss'>) ### evaluate the model on the training set prediction_train = autoencoder.predict(X_train) mse_train = aeu.mseTop10Raw(X_train, prediction_train) ### plot the global MSE trend pu.plot_mse(mse_train, rmlargest=0.005) (mean,std) = pu.plot_mse(mse_train, doplot=False, rmlargest=0.005) print('mean mse: {}'.format(mean)) print('std mse: {}'.format(std)) /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/IPython/core/pylabtools.py:134: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data. fig.canvas.print_figure(bytes_io, **kw) /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/IPython/core/pylabtools.py:134: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data. fig.canvas.print_figure(bytes_io, **kw) mean mse: 1.562267880327337e-06 std mse: 1.013996116717572e-06 ### impose a mse upper boundary and plot random examples of passing and failing histograms # note: at this point, only the training set is considered! # for a test set: see cell below. cutvalue = mean + 3*std print('The mse threshold is: '+str(cutvalue)) goodindices = np.arange(0,len(mse_train))[mse_train<cutvalue] badindices = np.arange(0,len(mse_train))[mse_train>cutvalue] print('Number of passing histograms: '+str(len(goodindices))) print('Number of failing histograms: '+str(len(badindices))) nplot = 5 print('examples of good histograms and reconstruction:') randint = np.random.choice(goodindices,size=nplot,replace=False) for i in randint: histlist = [X_train[int(i),:],prediction_train[int(i),:]] labellist = ['data','reconstruction'] colorlist = ['black','blue'] pu.plot_hists(histlist,colorlist=colorlist,labellist=labellist) plt.show() print('examples of bad histograms and reconstruction:') randint = np.random.choice(badindices,size=nplot,replace=False) for i in randint: histlist = [X_train[int(i),:],prediction_train[int(i),:]] labellist = ['data','reconstruction'] colorlist = ['black','blue'] pu.plot_hists(histlist,colorlist=colorlist,labellist=labellist) plt.show() The mse threshold is: 4.6042562304800525e-06 Number of passing histograms: 207175 Number of failing histograms: 4196 examples of good histograms and reconstruction: examples of bad histograms and reconstruction: ### get a test set and evaluate the model goodrunsls = { \"297056\":[[-1]], \"297177\":[[-1]], \"301449\":[[-1]] } badrunsls = { \"297287\":[[-1]], \"297288\":[[-1]], \"297289\":[[-1]], \"299316\":[[-1]], \"299317\":[[-1]], \"299318\":[[-1]], \"299324\":[[-1]], } # re-read the dataframe # (in case the selections are different than for the training set) dloader = DataLoader.DataLoader() df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) df = dfu.select_dcson(df) df = dfu.select_highstat(df,entries_to_bins_ratio=100) # good histograms option 1: predefined runs/lumisections #X_test_good = hu.preparedatafromdf( dfu.select_runsls(df,goodrunsls),donormalize=True ) # good histograms option 2: averages of total set X_test_good = hu.averagehists( hu.preparedatafromdf(df, donormalize=True), 15 ) # bad histograms: predefined runs/lumisections (X_test_bad, runnbs_bad,lsnbs_bad) = hu.preparedatafromdf( dfu.select_runsls(df,badrunsls), donormalize=True, returnrunls = True ) print('shape of good test set: {}'.format(X_test_good.shape)) print('shape of bad test set: {}'.format(X_test_bad.shape)) pu.plot_sets([X_test_good,X_test_bad],colorlist=['b','r'], labellist=['Histograms in test set labeled \"good\"','Histograms in test set labeled \"bad\"']) plt.show() prediction_test_good = autoencoder.predict(X_test_good) mse_test_good = aeu.mseTopNRaw(X_test_good, prediction_test_good, n=10 ) prediction_test_bad = autoencoder.predict(X_test_bad) mse_test_bad = aeu.mseTopNRaw(X_test_bad, prediction_test_bad, n=10 ) print('average mse on good set: '+str(np.mean(mse_test_good))) print('average mse on bad set: '+str(np.mean(mse_test_bad))) nplot = 10 print('examples of good histograms and reconstruction:') randint = np.random.choice(np.arange(len(X_test_good)),size=nplot,replace=False) for i in randint: histlist = [X_test_good[int(i),:],prediction_test_good[int(i),:]] labellist = ['data','reconstruction'] colorlist = ['black','blue'] pu.plot_hists(histlist,colorlist=colorlist,labellist=labellist) plt.show() print('examples of bad histograms and reconstruction:') randint = np.random.choice(np.arange(len(X_test_bad)),size=nplot,replace=False) for i in randint: histlist = [X_test_bad[int(i),:],prediction_test_bad[int(i),:]] labellist = ['data','reconstruction'] colorlist = ['black','blue'] pu.plot_hists(histlist,colorlist=colorlist,labellist=labellist) plt.show() INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_2.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. shape of good test set: (15, 102) shape of bad test set: (178, 102) average mse on good set: 9.394014806693854e-07 average mse on bad set: 2.8245170411831176e-05 examples of good histograms and reconstruction: examples of bad histograms and reconstruction: ### use artificial data to assess the model performance (goodhists,_,_) = gdu.upsample_hist_set( X_test_good, ntarget=5e3, fourierstdfactor=20., doplot=True ) (badhists,_,_) = gdu.upsample_hist_set( X_test_bad, ntarget=5e3, fourierstdfactor=20., doplot=True ) print('number of good histograms: '+str(len(goodhists))) print('number of bad histograms: '+str(len(badhists))) validation_data = np.vstack((goodhists,badhists)) labels = np.hstack((np.zeros(len(goodhists)),np.ones(len(badhists)))) prediction = autoencoder.predict(validation_data) mse = aeu.mseTopNRaw(validation_data, prediction, n=10 ) shuffled_indices = np.arange(len(validation_data)) _ = np.random.shuffle(shuffled_indices) validation_data = validation_data[shuffled_indices] labels = labels[shuffled_indices] prediction = prediction[shuffled_indices] mse = mse[shuffled_indices] # distribution of output scores pu.plot_score_dist(mse, labels, siglabel='anomalous', sigcolor='r', bcklabel='good', bckcolor='g', nbins=200, normalize=True) print('minimum mse on bad set: {}'.format(np.amin(mse[np.where(labels==1)]))) print('maximum mse on good set: {}'.format(np.amax(mse[np.where(labels==0)]))) # classical ROC curve: signal efficiency (good data marked as good) vs background efficiency (bad data marked as good) auc = aeu.get_roc(mse, labels, npoints=500, bootstrap_samples=100) number of good histograms: 4995 number of bad histograms: 4984 minimum mse on bad set: 2.3218854381344123e-06 maximum mse on good set: 8.690468927991255e-06 calculating ROC curve on 100 bootstrap samples of size 9979 ### continution of previous cell: choose wp and plot confusion matrix aeu.get_confusion_matrix_from_hists( validation_data, labels, prediction, msewp='maxauc' ) ### plot some histograms in the bad test set with their reconstruction inds = np.random.choice( np.arange(len(lsnbs_bad)), 10, replace=False ) for i in inds: runnb = runnbs_bad[i] lsnb = lsnbs_bad[i] histogram = X_test_bad[i:i+1,:] reco = autoencoder.predict(histogram) mse = aeu.mseTopNRaw(histogram, reco, n=10 ) pu.plot_sets([histogram,reco], labellist=['hist {}/{}'.format(runnb,lsnb),'reco'], colorlist=['black','red'], ) plt.show() print('MSE: {}'.format(mse)) MSE: [3.0169114e-06] MSE: [1.96651261e-05] MSE: [3.39562765e-06] MSE: [1.45044959e-05] MSE: [3.05993993e-06] MSE: [3.27946765e-06] MSE: [1.25162146e-05] MSE: [4.071005e-06] MSE: [8.70430108e-05] MSE: [3.24063155e-06] ### plot some histograms in the good test set with their reconstruction # note: depends on whether the good test set was obtained from real lumisections, # or from averages from entire set. inds = np.random.choice( np.arange(len(X_test_good)), 10, replace=False ) for i in inds: try: runnb = runnbs_good[i] lsnb = lsnbs_good[i] histlabel = 'hist {}/{}'.format(runnb,lsnb) except: runnb = 0 lsnb = 0 histlabel = 'hist (artificial)' histogram = X_test_good[i:i+1,:] reco = autoencoder.predict(histogram) mse = aeu.mseTopNRaw(histogram, reco, n=10 ) pu.plot_sets([histogram,reco], labellist=[histlabel,'reco'], colorlist=['black','red'], ) plt.show() print('MSE: {}'.format(mse)) MSE: [1.26397457e-07] MSE: [2.5634339e-07] MSE: [1.93309463e-07] MSE: [2.39758838e-06] MSE: [5.69567167e-07] MSE: [9.56595139e-08] MSE: [2.41395981e-06] MSE: [1.68067087e-06] MSE: [1.14024023e-06] MSE: [1.10999989e-06]","title":"autoencoder_1d"},{"location":"tutorials/autoencoder_1d/#train-and-test-an-autoencoder-on-a-set-of-1d-monitoring-elements","text":"This notebook walks you through the basics of the autoencoder approach to detecting anomalies for 1D monitoring elements. It consists of the following steps: Loading the data Applying selections (e.g. DCS-bit on and sufficient statistics) Preprocessing (e.g. normalizing) Building an autoencoder model with keras Investigate the output ### imports # external modules import sys import os import numpy as np import matplotlib.pyplot as plt from keras import backend as K import tensorflow as tf from tensorflow.keras.models import load_model # local modules sys.path.append('../utils') import dataframe_utils as dfu import hist_utils as hu import autoencoder_utils as aeu import plot_utils as pu import generate_data_utils as gdu sys.path.append('../src') import DataLoader 2022-07-26 17:21:47.396941: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc8-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc8-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc8-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.30-e5b21/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc8-opt/lib64/R/library/readr/rcon 2022-07-26 17:21:47.397006: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. ### read the data # note: this cell assumes you have a csv file stored at the specified location, # containing only histograms of the specified type; # see the tutorial read_and_write_data for examples on how to create such files! histname = 'chargeInner_PXLayer_2' filename = 'DF2017_'+histname+'.csv' datadir = '../data' dloader = DataLoader.DataLoader() df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_2.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. raw input data shape: (225954, 102) ### filtering: select only DCS-bit on data and filter out low statistics df = dfu.select_dcson(df) print('number of passing lumisections after DCS selection: {}'.format( len(df) )) df = dfu.select_highstat(df, entries_to_bins_ratio=100) print('number of passing lumisections after high statistics selection: {}'.format( len(df) )) number of passing lumisections after DCS selection: 215144 number of passing lumisections after high statistics selection: 211371 ### preprocessing of the data: rebinning and normalizing rebinningfactor = 1 X_train = hu.preparedatafromdf(df, rebinningfactor=rebinningfactor, donormalize=True, doplot=True) (ntrain,nbins) = X_train.shape print('size of training set: '+str(X_train.shape)) size of training set: (211371, 102) ### build the model and train it input_size = X_train.shape[1] arch = [int(X_train.shape[1]/2.)] act = ['tanh']*len(arch) opt = 'adam' loss = aeu.mseTop10 autoencoder = aeu.getautoencoder(input_size,arch,act,opt,loss) history = autoencoder.fit(X_train, X_train, epochs=20, batch_size=500, shuffle=False, verbose=1, validation_split=0.1) pu.plot_loss(history, title = 'model loss') 2022-07-26 17:22:34.789066: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc8-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc8-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc8-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.30-e5b21/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc8-opt/lib64/R/library/readr/rcon 2022-07-26 17:22:34.789185: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303) 2022-07-26 17:22:34.789326: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (59ec73fb7463): /proc/driver/nvidia/version does not exist 2022-07-26 17:22:34.791360: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 51) 5253 _________________________________________________________________ dense_1 (Dense) (None, 102) 5304 ================================================================= Total params: 10,557 Trainable params: 10,557 Non-trainable params: 0 _________________________________________________________________ 2022-07-26 17:22:35.219952: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2) 2022-07-26 17:22:35.221458: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2399845000 Hz Epoch 1/20 381/381 [==============================] - 4s 7ms/step - loss: 1.3303e-04 - val_loss: 1.8835e-05 Epoch 2/20 381/381 [==============================] - 3s 7ms/step - loss: 3.5243e-05 - val_loss: 8.5544e-06 Epoch 3/20 381/381 [==============================] - 3s 8ms/step - loss: 1.7558e-05 - val_loss: 4.2058e-06 Epoch 4/20 381/381 [==============================] - 3s 9ms/step - loss: 1.0271e-05 - val_loss: 2.6051e-06 Epoch 5/20 381/381 [==============================] - 3s 9ms/step - loss: 7.0206e-06 - val_loss: 1.8891e-06 Epoch 6/20 381/381 [==============================] - 3s 9ms/step - loss: 5.1686e-06 - val_loss: 1.5451e-06 Epoch 7/20 381/381 [==============================] - 3s 9ms/step - loss: 4.0133e-06 - val_loss: 1.4708e-06 Epoch 8/20 381/381 [==============================] - 4s 11ms/step - loss: 3.1906e-06 - val_loss: 1.3947e-06 Epoch 9/20 381/381 [==============================] - 4s 12ms/step - loss: 2.6316e-06 - val_loss: 1.3483e-06 Epoch 10/20 381/381 [==============================] - 4s 12ms/step - loss: 2.1945e-06 - val_loss: 1.2472e-06 Epoch 11/20 381/381 [==============================] - 5s 13ms/step - loss: 1.9028e-06 - val_loss: 1.1730e-06 Epoch 12/20 381/381 [==============================] - 5s 14ms/step - loss: 1.7457e-06 - val_loss: 1.1381e-06 Epoch 13/20 381/381 [==============================] - 5s 13ms/step - loss: 1.5103e-06 - val_loss: 1.1024e-06 Epoch 14/20 381/381 [==============================] - 5s 12ms/step - loss: 1.4419e-06 - val_loss: 9.9945e-07 Epoch 15/20 381/381 [==============================] - 5s 12ms/step - loss: 1.3293e-06 - val_loss: 9.4934e-07 Epoch 16/20 381/381 [==============================] - 6s 17ms/step - loss: 1.3654e-06 - val_loss: 9.3066e-07 Epoch 17/20 381/381 [==============================] - 6s 16ms/step - loss: 1.1381e-06 - val_loss: 8.6669e-07 Epoch 18/20 381/381 [==============================] - 6s 17ms/step - loss: 1.0834e-06 - val_loss: 7.9371e-07 Epoch 19/20 381/381 [==============================] - 7s 17ms/step - loss: 1.0866e-06 - val_loss: 7.6348e-07 Epoch 20/20 381/381 [==============================] - 6s 17ms/step - loss: 1.0285e-06 - val_loss: 7.3859e-07 (<Figure size 432x288 with 1 Axes>, <AxesSubplot:title={'center':'model loss'}, xlabel='Epoch', ylabel='Loss'>) ### evaluate the model on the training set prediction_train = autoencoder.predict(X_train) mse_train = aeu.mseTop10Raw(X_train, prediction_train) ### plot the global MSE trend pu.plot_mse(mse_train, rmlargest=0.005) (mean,std) = pu.plot_mse(mse_train, doplot=False, rmlargest=0.005) print('mean mse: {}'.format(mean)) print('std mse: {}'.format(std)) /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/IPython/core/pylabtools.py:134: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data. fig.canvas.print_figure(bytes_io, **kw) /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/IPython/core/pylabtools.py:134: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data. fig.canvas.print_figure(bytes_io, **kw) mean mse: 1.562267880327337e-06 std mse: 1.013996116717572e-06 ### impose a mse upper boundary and plot random examples of passing and failing histograms # note: at this point, only the training set is considered! # for a test set: see cell below. cutvalue = mean + 3*std print('The mse threshold is: '+str(cutvalue)) goodindices = np.arange(0,len(mse_train))[mse_train<cutvalue] badindices = np.arange(0,len(mse_train))[mse_train>cutvalue] print('Number of passing histograms: '+str(len(goodindices))) print('Number of failing histograms: '+str(len(badindices))) nplot = 5 print('examples of good histograms and reconstruction:') randint = np.random.choice(goodindices,size=nplot,replace=False) for i in randint: histlist = [X_train[int(i),:],prediction_train[int(i),:]] labellist = ['data','reconstruction'] colorlist = ['black','blue'] pu.plot_hists(histlist,colorlist=colorlist,labellist=labellist) plt.show() print('examples of bad histograms and reconstruction:') randint = np.random.choice(badindices,size=nplot,replace=False) for i in randint: histlist = [X_train[int(i),:],prediction_train[int(i),:]] labellist = ['data','reconstruction'] colorlist = ['black','blue'] pu.plot_hists(histlist,colorlist=colorlist,labellist=labellist) plt.show() The mse threshold is: 4.6042562304800525e-06 Number of passing histograms: 207175 Number of failing histograms: 4196 examples of good histograms and reconstruction: examples of bad histograms and reconstruction: ### get a test set and evaluate the model goodrunsls = { \"297056\":[[-1]], \"297177\":[[-1]], \"301449\":[[-1]] } badrunsls = { \"297287\":[[-1]], \"297288\":[[-1]], \"297289\":[[-1]], \"299316\":[[-1]], \"299317\":[[-1]], \"299318\":[[-1]], \"299324\":[[-1]], } # re-read the dataframe # (in case the selections are different than for the training set) dloader = DataLoader.DataLoader() df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) df = dfu.select_dcson(df) df = dfu.select_highstat(df,entries_to_bins_ratio=100) # good histograms option 1: predefined runs/lumisections #X_test_good = hu.preparedatafromdf( dfu.select_runsls(df,goodrunsls),donormalize=True ) # good histograms option 2: averages of total set X_test_good = hu.averagehists( hu.preparedatafromdf(df, donormalize=True), 15 ) # bad histograms: predefined runs/lumisections (X_test_bad, runnbs_bad,lsnbs_bad) = hu.preparedatafromdf( dfu.select_runsls(df,badrunsls), donormalize=True, returnrunls = True ) print('shape of good test set: {}'.format(X_test_good.shape)) print('shape of bad test set: {}'.format(X_test_bad.shape)) pu.plot_sets([X_test_good,X_test_bad],colorlist=['b','r'], labellist=['Histograms in test set labeled \"good\"','Histograms in test set labeled \"bad\"']) plt.show() prediction_test_good = autoencoder.predict(X_test_good) mse_test_good = aeu.mseTopNRaw(X_test_good, prediction_test_good, n=10 ) prediction_test_bad = autoencoder.predict(X_test_bad) mse_test_bad = aeu.mseTopNRaw(X_test_bad, prediction_test_bad, n=10 ) print('average mse on good set: '+str(np.mean(mse_test_good))) print('average mse on bad set: '+str(np.mean(mse_test_bad))) nplot = 10 print('examples of good histograms and reconstruction:') randint = np.random.choice(np.arange(len(X_test_good)),size=nplot,replace=False) for i in randint: histlist = [X_test_good[int(i),:],prediction_test_good[int(i),:]] labellist = ['data','reconstruction'] colorlist = ['black','blue'] pu.plot_hists(histlist,colorlist=colorlist,labellist=labellist) plt.show() print('examples of bad histograms and reconstruction:') randint = np.random.choice(np.arange(len(X_test_bad)),size=nplot,replace=False) for i in randint: histlist = [X_test_bad[int(i),:],prediction_test_bad[int(i),:]] labellist = ['data','reconstruction'] colorlist = ['black','blue'] pu.plot_hists(histlist,colorlist=colorlist,labellist=labellist) plt.show() INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_2.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. shape of good test set: (15, 102) shape of bad test set: (178, 102) average mse on good set: 9.394014806693854e-07 average mse on bad set: 2.8245170411831176e-05 examples of good histograms and reconstruction: examples of bad histograms and reconstruction: ### use artificial data to assess the model performance (goodhists,_,_) = gdu.upsample_hist_set( X_test_good, ntarget=5e3, fourierstdfactor=20., doplot=True ) (badhists,_,_) = gdu.upsample_hist_set( X_test_bad, ntarget=5e3, fourierstdfactor=20., doplot=True ) print('number of good histograms: '+str(len(goodhists))) print('number of bad histograms: '+str(len(badhists))) validation_data = np.vstack((goodhists,badhists)) labels = np.hstack((np.zeros(len(goodhists)),np.ones(len(badhists)))) prediction = autoencoder.predict(validation_data) mse = aeu.mseTopNRaw(validation_data, prediction, n=10 ) shuffled_indices = np.arange(len(validation_data)) _ = np.random.shuffle(shuffled_indices) validation_data = validation_data[shuffled_indices] labels = labels[shuffled_indices] prediction = prediction[shuffled_indices] mse = mse[shuffled_indices] # distribution of output scores pu.plot_score_dist(mse, labels, siglabel='anomalous', sigcolor='r', bcklabel='good', bckcolor='g', nbins=200, normalize=True) print('minimum mse on bad set: {}'.format(np.amin(mse[np.where(labels==1)]))) print('maximum mse on good set: {}'.format(np.amax(mse[np.where(labels==0)]))) # classical ROC curve: signal efficiency (good data marked as good) vs background efficiency (bad data marked as good) auc = aeu.get_roc(mse, labels, npoints=500, bootstrap_samples=100) number of good histograms: 4995 number of bad histograms: 4984 minimum mse on bad set: 2.3218854381344123e-06 maximum mse on good set: 8.690468927991255e-06 calculating ROC curve on 100 bootstrap samples of size 9979 ### continution of previous cell: choose wp and plot confusion matrix aeu.get_confusion_matrix_from_hists( validation_data, labels, prediction, msewp='maxauc' ) ### plot some histograms in the bad test set with their reconstruction inds = np.random.choice( np.arange(len(lsnbs_bad)), 10, replace=False ) for i in inds: runnb = runnbs_bad[i] lsnb = lsnbs_bad[i] histogram = X_test_bad[i:i+1,:] reco = autoencoder.predict(histogram) mse = aeu.mseTopNRaw(histogram, reco, n=10 ) pu.plot_sets([histogram,reco], labellist=['hist {}/{}'.format(runnb,lsnb),'reco'], colorlist=['black','red'], ) plt.show() print('MSE: {}'.format(mse)) MSE: [3.0169114e-06] MSE: [1.96651261e-05] MSE: [3.39562765e-06] MSE: [1.45044959e-05] MSE: [3.05993993e-06] MSE: [3.27946765e-06] MSE: [1.25162146e-05] MSE: [4.071005e-06] MSE: [8.70430108e-05] MSE: [3.24063155e-06] ### plot some histograms in the good test set with their reconstruction # note: depends on whether the good test set was obtained from real lumisections, # or from averages from entire set. inds = np.random.choice( np.arange(len(X_test_good)), 10, replace=False ) for i in inds: try: runnb = runnbs_good[i] lsnb = lsnbs_good[i] histlabel = 'hist {}/{}'.format(runnb,lsnb) except: runnb = 0 lsnb = 0 histlabel = 'hist (artificial)' histogram = X_test_good[i:i+1,:] reco = autoencoder.predict(histogram) mse = aeu.mseTopNRaw(histogram, reco, n=10 ) pu.plot_sets([histogram,reco], labellist=[histlabel,'reco'], colorlist=['black','red'], ) plt.show() print('MSE: {}'.format(mse)) MSE: [1.26397457e-07] MSE: [2.5634339e-07] MSE: [1.93309463e-07] MSE: [2.39758838e-06] MSE: [5.69567167e-07] MSE: [9.56595139e-08] MSE: [2.41395981e-06] MSE: [1.68067087e-06] MSE: [1.14024023e-06] MSE: [1.10999989e-06]","title":"Train and test an autoencoder on a set of 1D monitoring elements"},{"location":"tutorials/autoencoder_iterative/","text":"Train and test an autoencoder iteratively on local datasets This notebook investigates the possibility to perform local autoencoder training, i.e. training on a small number of runs instead of training on a large dataset (e.g. a full year of data taking). This notebook consists of three parts: Reading and preparing the data (common to part 2 and 3). Train an autoencoder on the first 5, 10, 15 etc. runs of 2017 data taking. Choose a random run to test on, use 5 previous runs for training. This is a first step naive attempt towards using dedicated reference runs for training. Part 1: Reading and preparing the data ### imports # external modules import sys import os import numpy as np import matplotlib.pyplot as plt # local modules sys.path.append('../utils') import dataframe_utils as dfu import hist_utils as hu import plot_utils as pu import autoencoder_utils as aeu import generate_data_utils as gdu sys.path.append('../src') import DataLoader 2022-07-26 17:22:47.287801: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc8-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc8-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc8-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.30-e5b21/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc8-opt/lib64/R/library/readr/rcon 2022-07-26 17:22:47.287868: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. ### read the data and perform some selections # note: this cell assumes you have a csv file stored at the specified location, # containing only histograms of the specified type; # see the tutorial read_and_write_data for examples on how to create such files! histname = 'chargeInner_PXLayer_2' filename = 'DF2017_'+histname+'.csv' datadir = '../data' dloader = DataLoader.DataLoader() df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) df = dfu.select_dcson(df) df = dfu.select_highstat(df) print('number of passing lumisections after selection: {}'.format( len(df) )) runs_all = dfu.get_runs(df) (hists_all,runnbs_all,lsnbs_all) = hu.preparedatafromdf(df,returnrunls=True,rebinningfactor=1,donormalize=True) print('shape of histogram array: {}'.format(hists_all.shape)) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_2.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. raw input data shape: (225954, 102) number of passing lumisections after selection: 211371 shape of histogram array: (211371, 102) Part 2: Updating the training set ### get a test set goodrunsls = {'2017': { \"297056\":[[-1]], }} badrunsls = {'2017': { \"297287\":[[-1]], \"297288\":[[-1]], \"297289\":[[-1]], \"299316\":[[-1]], \"299324\":[[-1]], }} # select the correct data-taking year relevant for the file chosen above year = '2017' # load good and bad sets from df (hists_good,runnbs_good,lsnbs_good) = hu.preparedatafromdf( dfu.select_runsls(df,goodrunsls[year]), returnrunls=True, donormalize=True) (hists_bad,runnbs_bad,lsnbs_bad) = hu.preparedatafromdf( dfu.select_runsls(df,badrunsls[year]), returnrunls=True, donormalize=True) print('shape of good test set '+str(hists_good.shape)) print('shape of bad test set '+str(hists_bad.shape)) # make plot pu.plot_sets([hists_good,hists_bad], colorlist=['b','r'], labellist=['good','bad'], transparencylist=[], xlims=(0,-1)) # use resampling tool to upsample and add more variation (hists_good,_,_) = gdu.upsample_hist_set(hists_good,ntarget=2e3,fourierstdfactor=15., doplot=False) (hists_bad,_,_) = gdu.upsample_hist_set(hists_bad,ntarget=2e3,fourierstdfactor=5., doplot=False) print('shape of good test set '+str(hists_good.shape)) print('shape of bad test set '+str(hists_bad.shape)) # make plot pu.plot_sets([hists_good,hists_bad], colorlist=['b','r'], labellist=['good','bad'], transparencylist=[0.1,0.1], xlims=(0,-1)) shape of good test set (184, 102) shape of bad test set (97, 102) shape of good test set (1840, 102) shape of bad test set (1940, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### function to purify training set by removing a given fraction of high mse histograms def purify_training_set(hists,model,rmfraction): mse = aeu.mseTop10Raw(hists,model.predict(hists)) threshold = np.quantile(mse,1-rmfraction) keepindices = np.where(mse<threshold) return hists[keepindices] ### functions to test performance on test set def test_autoencoder(hists_good,hists_bad,model): mse_good = aeu.mseTop10Raw(hists_good,model.predict(hists_good)) mse_bad = aeu.mseTop10Raw(hists_bad,model.predict(hists_bad)) labels_good = np.zeros(len(mse_good)) labels_bad = np.ones(len(mse_bad)) labels = np.concatenate(tuple([labels_good,labels_bad])) scores = np.concatenate(tuple([mse_good,mse_bad])) maxnoninf = np.max(np.where(scores==np.inf,np.min(scores),scores)) scores = np.where(scores==np.inf,maxnoninf,scores) auc = aeu.get_roc(scores, labels, mode='full', bootstrap_samples=100) plt.show() def plot_examples(hists_good,hists_bad,model): # set parameters nexamples = 6 fig,axs = plt.subplots(2,nexamples,figsize=(24,12)) inds_good = np.random.choice(range(len(hists_good)),nexamples) inds_bad = np.random.choice(range(len(hists_bad)),nexamples) inds_ref = np.random.choice(range(len(hists_good)),20) # determine whether to show run/lumi number in label (not possible when using resampled sets) truelabel = True if( len(hists_good)!=len(runnbs_good) or len(hists_bad)!=len(runnbs_good) ): truelabel = False # plot examples for i in range(nexamples): hist_good = hists_good[inds_good[i]:inds_good[i]+1] reco_good = model.predict(hist_good) hist_bad = hists_bad[inds_bad[i]:inds_bad[i]+1] reco_bad = model.predict(hist_bad) hist_good_label = hist_bad_label = 'hist' if truelabel: hist_good_label += ' (run: '+str(int(runnbs_good[inds_good[i]]))+', ls: '+str(int(lsnbs_good[inds_good[i]]))+')' hist_bad_label += ' (run: '+str(int(runnbs_bad[inds_bad[i]]))+', ls: '+str(int(lsnbs_bad[inds_bad[i]]))+')' pu.plot_sets([hist_good,reco_good,hists_good[inds_ref]], fig=fig,ax=axs[0,i], title='', colorlist=['black','red','blue'], labellist=[hist_good_label,'reco','good hists'], transparencylist=[1.,1.,0.1]) pu.plot_sets([hist_bad,reco_bad,hists_good[inds_ref]], fig=fig,ax=axs[1,i], title='', colorlist=['black','red','blue'], labellist=[hist_bad_label,'reco','good hists'], transparencylist=[1.,1.,0.1]) plt.show() ### iterate over growing amount of data nruns = [5,10] # first iteration manually X_train = hists_all[np.where(runnbs_all<runs_all[nruns[0]])] print('size of training set (intial): '+str(len(X_train))) (X_train_ext,_,_) = gdu.upsample_hist_set(X_train, 1e5) #X_train_ext = X_train model = aeu.train_simple_autoencoder(X_train_ext,nepochs=10) print('evaluating model on test set') test_autoencoder(hists_good,hists_bad,model) plot_examples(hists_good,hists_bad,model) # next iterations in a loop for i in range(1,len(nruns)): this_upperbound = runs_all[nruns[i]] this_lowerbound = runs_all[nruns[i-1]] print('adding runs {} to {}'.format(this_lowerbound,this_upperbound)) print('(training on {} runs in total)'.format(nruns[i])) newhists = hists_all[np.where( (runnbs_all<this_upperbound) & (runnbs_all>=this_lowerbound) )] print('number of new histograms added to training set: '+str(len(newhists))) X_train = np.concatenate( (X_train,newhists), axis=0 ) X_train = purify_training_set(X_train,model,0.1) print('size of training set (intial): '+str(len(X_train))) (X_train_ext,_,_) = gdu.upsample_hist_set(X_train, 1e5) #X_train_ext = X_train model = aeu.train_simple_autoencoder(X_train_ext) print('size of training set (after training and purifying): '+str(len(X_train))) print('evaluating model on test set') test_autoencoder(hists_good,hists_bad,model) plot_examples(hists_good,hists_bad,model) size of training set (intial): 1017 2022-07-26 17:29:35.167654: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc8-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc8-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc8-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.30-e5b21/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc8-opt/lib64/R/library/readr/rcon 2022-07-26 17:29:35.167722: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303) 2022-07-26 17:29:35.167818: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (59ec73fb7463): /proc/driver/nvidia/version does not exist 2022-07-26 17:29:35.169168: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 51) 5253 _________________________________________________________________ dense_1 (Dense) (None, 102) 5304 ================================================================= Total params: 10,557 Trainable params: 10,557 Non-trainable params: 0 _________________________________________________________________ 2022-07-26 17:29:37.178509: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2) 2022-07-26 17:29:37.180016: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2399845000 Hz Epoch 1/10 180/180 [==============================] - 10s 31ms/step - loss: 1.5311e-04 - val_loss: 4.9320e-06 Epoch 2/10 180/180 [==============================] - 5s 27ms/step - loss: 4.5635e-06 - val_loss: 4.2272e-06 Epoch 3/10 180/180 [==============================] - 5s 26ms/step - loss: 3.9521e-06 - val_loss: 3.6740e-06 Epoch 4/10 180/180 [==============================] - 6s 32ms/step - loss: 3.4399e-06 - val_loss: 3.2126e-06 Epoch 5/10 180/180 [==============================] - 8s 42ms/step - loss: 3.0351e-06 - val_loss: 2.8201e-06 Epoch 6/10 180/180 [==============================] - 6s 32ms/step - loss: 2.6835e-06 - val_loss: 2.4938e-06s - loss: 2.6835e-0 Epoch 7/10 180/180 [==============================] - 5s 30ms/step - loss: 2.3665e-06 - val_loss: 2.2352e-06 Epoch 8/10 180/180 [==============================] - 5s 30ms/step - loss: 2.1452e-06 - val_loss: 2.0101e-06 Epoch 9/10 180/180 [==============================] - 6s 36ms/step - loss: 1.9231e-06 - val_loss: 1.8349e-06 Epoch 10/10 180/180 [==============================] - 9s 52ms/step - loss: 1.8513e-06 - val_loss: 1.8462e-06 evaluating model on test set calculating ROC curve on 100 bootstrap samples of size 3780 adding runs 297057 to 297114 (training on 10 runs in total) number of new histograms added to training set: 2657 size of training set (intial): 3306 Model: \"sequential_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 51) 5253 _________________________________________________________________ dense_3 (Dense) (None, 102) 5304 ================================================================= Total params: 10,557 Trainable params: 10,557 Non-trainable params: 0 _________________________________________________________________ Epoch 1/40 179/179 [==============================] - 7s 25ms/step - loss: 1.1214e-04 - val_loss: 5.1926e-06 Epoch 2/40 179/179 [==============================] - 4s 21ms/step - loss: 4.9028e-06 - val_loss: 4.6196e-06 Epoch 3/40 179/179 [==============================] - 3s 18ms/step - loss: 4.3412e-06 - val_loss: 4.0848e-06 Epoch 4/40 179/179 [==============================] - 3s 17ms/step - loss: 3.8516e-06 - val_loss: 3.6149e-06 Epoch 5/40 179/179 [==============================] - 3s 18ms/step - loss: 3.3940e-06 - val_loss: 3.1668e-06 Epoch 6/40 179/179 [==============================] - 3s 19ms/step - loss: 2.9781e-06 - val_loss: 2.7924e-06 Epoch 7/40 179/179 [==============================] - 3s 19ms/step - loss: 2.6401e-06 - val_loss: 2.4741e-06 Epoch 8/40 179/179 [==============================] - 3s 18ms/step - loss: 2.4025e-06 - val_loss: 2.2292e-06 Epoch 9/40 179/179 [==============================] - 3s 18ms/step - loss: 2.0766e-06 - val_loss: 1.9773e-06 Epoch 10/40 179/179 [==============================] - 3s 17ms/step - loss: 1.8608e-06 - val_loss: 1.7755e-06 Epoch 11/40 179/179 [==============================] - 3s 18ms/step - loss: 1.8868e-06 - val_loss: 1.8752e-06 Epoch 12/40 179/179 [==============================] - 3s 19ms/step - loss: 1.5737e-06 - val_loss: 1.4608e-06 Epoch 13/40 179/179 [==============================] - 3s 18ms/step - loss: 1.3841e-06 - val_loss: 1.3289e-06 Epoch 14/40 179/179 [==============================] - 3s 20ms/step - loss: 1.2598e-06 - val_loss: 1.2143e-06 Epoch 15/40 179/179 [==============================] - 3s 18ms/step - loss: 1.1874e-06 - val_loss: 1.7827e-06 Epoch 16/40 179/179 [==============================] - 3s 15ms/step - loss: 1.4452e-06 - val_loss: 1.0406e-06 Epoch 17/40 179/179 [==============================] - 3s 19ms/step - loss: 9.9276e-07 - val_loss: 9.7675e-07 Epoch 18/40 179/179 [==============================] - 3s 17ms/step - loss: 9.2855e-07 - val_loss: 9.1790e-07 Epoch 19/40 179/179 [==============================] - 3s 18ms/step - loss: 8.7365e-07 - val_loss: 8.6198e-07 Epoch 20/40 179/179 [==============================] - 3s 18ms/step - loss: 8.2775e-07 - val_loss: 8.1381e-07 Epoch 21/40 179/179 [==============================] - 3s 18ms/step - loss: 7.8679e-07 - val_loss: 7.8189e-07 Epoch 22/40 179/179 [==============================] - 4s 20ms/step - loss: 7.5235e-07 - val_loss: 7.4516e-07 Epoch 23/40 179/179 [==============================] - 3s 15ms/step - loss: 7.2027e-07 - val_loss: 7.0982e-07 Epoch 24/40 179/179 [==============================] - 3s 18ms/step - loss: 6.9286e-07 - val_loss: 6.8907e-07 Epoch 25/40 179/179 [==============================] - 3s 16ms/step - loss: 2.5623e-06 - val_loss: 5.6969e-06 Epoch 26/40 179/179 [==============================] - 3s 18ms/step - loss: 1.2265e-06 - val_loss: 6.3184e-07 Epoch 27/40 179/179 [==============================] - 3s 19ms/step - loss: 6.1497e-07 - val_loss: 6.1188e-07 Epoch 28/40 179/179 [==============================] - 3s 19ms/step - loss: 5.9777e-07 - val_loss: 5.9449e-07 Epoch 29/40 179/179 [==============================] - 4s 20ms/step - loss: 5.8060e-07 - val_loss: 5.7774e-07 Epoch 30/40 179/179 [==============================] - 3s 19ms/step - loss: 5.6749e-07 - val_loss: 5.6069e-07 Epoch 31/40 179/179 [==============================] - 3s 18ms/step - loss: 5.4924e-07 - val_loss: 5.4804e-07 Epoch 32/40 179/179 [==============================] - 3s 19ms/step - loss: 5.3318e-07 - val_loss: 5.2841e-07 Epoch 33/40 179/179 [==============================] - 3s 19ms/step - loss: 5.1866e-07 - val_loss: 5.1238e-07 Epoch 34/40 179/179 [==============================] - 3s 16ms/step - loss: 5.1258e-07 - val_loss: 4.9960e-07 Epoch 35/40 179/179 [==============================] - 3s 19ms/step - loss: 4.9696e-07 - val_loss: 4.8245e-07 Epoch 36/40 179/179 [==============================] - 3s 19ms/step - loss: 4.7906e-07 - val_loss: 4.6904e-07 Epoch 37/40 179/179 [==============================] - 3s 19ms/step - loss: 4.7037e-07 - val_loss: 4.5758e-07 Epoch 38/40 179/179 [==============================] - 3s 18ms/step - loss: 4.7814e-07 - val_loss: 4.5095e-07 Epoch 39/40 179/179 [==============================] - 3s 17ms/step - loss: 4.1904e-06 - val_loss: 1.0307e-06 Epoch 40/40 179/179 [==============================] - 3s 17ms/step - loss: 4.8127e-07 - val_loss: 4.1665e-07 size of training set (after training and purifying): 3306 evaluating model on test set calculating ROC curve on 100 bootstrap samples of size 3780 Part 3: Local training on nearby runs ### choose random run for testing, train on previous runs # (testing: see next cell) # choose runs print('number of available runs: '+str(len(runs_all))) runindex = np.random.choice(range(5,len(runs_all))) #runindex = runs_all.index(305364) test_run = runs_all[runindex] print('chosen run index: '+str(runindex)+', corresponding to run: '+str(test_run)) training_runs = runs_all[runindex-5:runindex] print('runs used for training: '+str(training_runs)) # get the data df = dfu.select_dcson(df) (hists_train,runnbs_train,lsnbs_train) = hu.preparedatafromdf( dfu.select_runs(df,training_runs),returnrunls=True,donormalize=True) (hists_test,runnbs_test,lsnbs_test) = hu.preparedatafromdf( dfu.select_runs(df,[test_run]),returnrunls=True,donormalize=True) print('shape of training set '+str(hists_train.shape)) print('shape of test set '+str(hists_test.shape)) # make plot pu.plot_sets([hists_train,hists_test],ax=None,title='',colorlist=['b','g'],labellist=['train','test'],transparencylist=[0.5,0.5],xlims=(0,-1)) plt.show() # train the autoencoder (X_train,_,_) = gdu.upsample_hist_set(hists_train, 1e5) #X_train = hists_train model = aeu.train_simple_autoencoder(X_train) number of available runs: 564 chosen run index: 412, corresponding to run: 304209 runs used for training: [304197, 304198, 304199, 304200, 304204] shape of training set (909, 102) shape of test set (425, 102) Model: \"sequential_2\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_4 (Dense) (None, 51) 5253 _________________________________________________________________ dense_5 (Dense) (None, 102) 5304 ================================================================= Total params: 10,557 Trainable params: 10,557 Non-trainable params: 0 _________________________________________________________________ Epoch 1/40 180/180 [==============================] - 2s 7ms/step - loss: 1.6187e-04 - val_loss: 5.1549e-06 Epoch 2/40 180/180 [==============================] - 1s 6ms/step - loss: 4.8933e-06 - val_loss: 4.6664e-06 Epoch 3/40 180/180 [==============================] - 1s 6ms/step - loss: 4.4298e-06 - val_loss: 4.2151e-06 Epoch 4/40 180/180 [==============================] - 1s 6ms/step - loss: 3.9914e-06 - val_loss: 3.7808e-06 Epoch 5/40 180/180 [==============================] - 1s 6ms/step - loss: 3.5813e-06 - val_loss: 3.3762e-06 Epoch 6/40 180/180 [==============================] - 1s 6ms/step - loss: 3.2062e-06 - val_loss: 3.0032e-06 Epoch 7/40 180/180 [==============================] - 1s 6ms/step - loss: 2.8450e-06 - val_loss: 2.6555e-06 Epoch 8/40 180/180 [==============================] - 1s 7ms/step - loss: 2.5000e-06 - val_loss: 2.3279e-06 Epoch 9/40 180/180 [==============================] - 1s 7ms/step - loss: 2.1955e-06 - val_loss: 2.0381e-06 Epoch 10/40 180/180 [==============================] - 1s 7ms/step - loss: 1.9759e-06 - val_loss: 1.8084e-06 Epoch 11/40 180/180 [==============================] - 1s 6ms/step - loss: 1.7138e-06 - val_loss: 1.5692e-06 Epoch 12/40 180/180 [==============================] - 1s 6ms/step - loss: 1.6443e-06 - val_loss: 1.3883e-06 Epoch 13/40 180/180 [==============================] - 1s 7ms/step - loss: 1.3263e-06 - val_loss: 1.2380e-06 Epoch 14/40 180/180 [==============================] - 1s 6ms/step - loss: 1.1858e-06 - val_loss: 1.1198e-06 Epoch 15/40 180/180 [==============================] - 1s 7ms/step - loss: 1.0711e-06 - val_loss: 1.0200e-06 Epoch 16/40 180/180 [==============================] - 1s 6ms/step - loss: 1.2255e-06 - val_loss: 9.8934e-07 Epoch 17/40 180/180 [==============================] - 1s 5ms/step - loss: 9.1090e-07 - val_loss: 8.4949e-07 Epoch 18/40 180/180 [==============================] - 1s 7ms/step - loss: 8.5840e-07 - val_loss: 9.2149e-07 Epoch 19/40 180/180 [==============================] - 1s 6ms/step - loss: 1.1008e-06 - val_loss: 7.3578e-07 Epoch 20/40 180/180 [==============================] - 1s 7ms/step - loss: 7.1226e-07 - val_loss: 6.9035e-07 Epoch 21/40 180/180 [==============================] - 1s 6ms/step - loss: 6.6963e-07 - val_loss: 6.5100e-07 Epoch 22/40 180/180 [==============================] - 1s 7ms/step - loss: 6.3295e-07 - val_loss: 6.1755e-07 Epoch 23/40 180/180 [==============================] - 1s 7ms/step - loss: 6.0135e-07 - val_loss: 5.9092e-07 Epoch 24/40 180/180 [==============================] - 1s 6ms/step - loss: 5.7492e-07 - val_loss: 5.6528e-07 Epoch 25/40 180/180 [==============================] - 1s 6ms/step - loss: 5.5186e-07 - val_loss: 5.4899e-07 Epoch 26/40 180/180 [==============================] - 1s 6ms/step - loss: 5.3143e-07 - val_loss: 5.1981e-07 Epoch 27/40 180/180 [==============================] - 1s 6ms/step - loss: 5.1755e-07 - val_loss: 5.0836e-07 Epoch 28/40 180/180 [==============================] - 1s 6ms/step - loss: 1.2912e-06 - val_loss: 1.8068e-05 Epoch 29/40 180/180 [==============================] - 1s 6ms/step - loss: 4.3261e-06 - val_loss: 4.7346e-07 Epoch 30/40 180/180 [==============================] - 1s 6ms/step - loss: 4.6356e-07 - val_loss: 4.5966e-07 Epoch 31/40 180/180 [==============================] - 1s 6ms/step - loss: 4.5268e-07 - val_loss: 4.4805e-07 Epoch 32/40 180/180 [==============================] - 1s 6ms/step - loss: 4.4301e-07 - val_loss: 4.4015e-07 Epoch 33/40 180/180 [==============================] - 1s 6ms/step - loss: 4.3409e-07 - val_loss: 4.3113e-07 Epoch 34/40 180/180 [==============================] - 1s 7ms/step - loss: 5.3247e-07 - val_loss: 6.7669e-07 Epoch 35/40 180/180 [==============================] - 1s 6ms/step - loss: 5.2115e-07 - val_loss: 4.1663e-07 Epoch 36/40 180/180 [==============================] - 1s 6ms/step - loss: 4.0731e-07 - val_loss: 4.0308e-07 Epoch 37/40 180/180 [==============================] - 1s 6ms/step - loss: 3.9863e-07 - val_loss: 3.9545e-07 Epoch 38/40 180/180 [==============================] - 1s 7ms/step - loss: 3.9706e-07 - val_loss: 3.9387e-07 Epoch 39/40 180/180 [==============================] - 1s 6ms/step - loss: 3.8418e-07 - val_loss: 3.9225e-07 Epoch 40/40 180/180 [==============================] - 1s 6ms/step - loss: 3.7796e-07 - val_loss: 3.7572e-07 ### test the autoencoder trained in the previous cell # note: do not simply use hists_good for a good test set, # as the model might not be trained on all shape variations that are supposed to be 'good', # resulting in artificially bad performance. # we can use hists_test instead, but it is not at all guaranteed that all of them are good. # hence the label 'good hists' in the plot below is not necessarily correct. test_autoencoder(hists_test,hists_bad,model) plot_examples(hists_test,hists_bad,model) calculating ROC curve on 100 bootstrap samples of size 2365","title":"autoencoder_iterative"},{"location":"tutorials/autoencoder_iterative/#train-and-test-an-autoencoder-iteratively-on-local-datasets","text":"This notebook investigates the possibility to perform local autoencoder training, i.e. training on a small number of runs instead of training on a large dataset (e.g. a full year of data taking). This notebook consists of three parts: Reading and preparing the data (common to part 2 and 3). Train an autoencoder on the first 5, 10, 15 etc. runs of 2017 data taking. Choose a random run to test on, use 5 previous runs for training. This is a first step naive attempt towards using dedicated reference runs for training.","title":"Train and test an autoencoder iteratively on local datasets"},{"location":"tutorials/autoencoder_iterative/#part-1-reading-and-preparing-the-data","text":"### imports # external modules import sys import os import numpy as np import matplotlib.pyplot as plt # local modules sys.path.append('../utils') import dataframe_utils as dfu import hist_utils as hu import plot_utils as pu import autoencoder_utils as aeu import generate_data_utils as gdu sys.path.append('../src') import DataLoader 2022-07-26 17:22:47.287801: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc8-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc8-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc8-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.30-e5b21/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc8-opt/lib64/R/library/readr/rcon 2022-07-26 17:22:47.287868: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. ### read the data and perform some selections # note: this cell assumes you have a csv file stored at the specified location, # containing only histograms of the specified type; # see the tutorial read_and_write_data for examples on how to create such files! histname = 'chargeInner_PXLayer_2' filename = 'DF2017_'+histname+'.csv' datadir = '../data' dloader = DataLoader.DataLoader() df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) df = dfu.select_dcson(df) df = dfu.select_highstat(df) print('number of passing lumisections after selection: {}'.format( len(df) )) runs_all = dfu.get_runs(df) (hists_all,runnbs_all,lsnbs_all) = hu.preparedatafromdf(df,returnrunls=True,rebinningfactor=1,donormalize=True) print('shape of histogram array: {}'.format(hists_all.shape)) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_2.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. raw input data shape: (225954, 102) number of passing lumisections after selection: 211371 shape of histogram array: (211371, 102)","title":"Part 1: Reading and preparing the data"},{"location":"tutorials/autoencoder_iterative/#part-2-updating-the-training-set","text":"### get a test set goodrunsls = {'2017': { \"297056\":[[-1]], }} badrunsls = {'2017': { \"297287\":[[-1]], \"297288\":[[-1]], \"297289\":[[-1]], \"299316\":[[-1]], \"299324\":[[-1]], }} # select the correct data-taking year relevant for the file chosen above year = '2017' # load good and bad sets from df (hists_good,runnbs_good,lsnbs_good) = hu.preparedatafromdf( dfu.select_runsls(df,goodrunsls[year]), returnrunls=True, donormalize=True) (hists_bad,runnbs_bad,lsnbs_bad) = hu.preparedatafromdf( dfu.select_runsls(df,badrunsls[year]), returnrunls=True, donormalize=True) print('shape of good test set '+str(hists_good.shape)) print('shape of bad test set '+str(hists_bad.shape)) # make plot pu.plot_sets([hists_good,hists_bad], colorlist=['b','r'], labellist=['good','bad'], transparencylist=[], xlims=(0,-1)) # use resampling tool to upsample and add more variation (hists_good,_,_) = gdu.upsample_hist_set(hists_good,ntarget=2e3,fourierstdfactor=15., doplot=False) (hists_bad,_,_) = gdu.upsample_hist_set(hists_bad,ntarget=2e3,fourierstdfactor=5., doplot=False) print('shape of good test set '+str(hists_good.shape)) print('shape of bad test set '+str(hists_bad.shape)) # make plot pu.plot_sets([hists_good,hists_bad], colorlist=['b','r'], labellist=['good','bad'], transparencylist=[0.1,0.1], xlims=(0,-1)) shape of good test set (184, 102) shape of bad test set (97, 102) shape of good test set (1840, 102) shape of bad test set (1940, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### function to purify training set by removing a given fraction of high mse histograms def purify_training_set(hists,model,rmfraction): mse = aeu.mseTop10Raw(hists,model.predict(hists)) threshold = np.quantile(mse,1-rmfraction) keepindices = np.where(mse<threshold) return hists[keepindices] ### functions to test performance on test set def test_autoencoder(hists_good,hists_bad,model): mse_good = aeu.mseTop10Raw(hists_good,model.predict(hists_good)) mse_bad = aeu.mseTop10Raw(hists_bad,model.predict(hists_bad)) labels_good = np.zeros(len(mse_good)) labels_bad = np.ones(len(mse_bad)) labels = np.concatenate(tuple([labels_good,labels_bad])) scores = np.concatenate(tuple([mse_good,mse_bad])) maxnoninf = np.max(np.where(scores==np.inf,np.min(scores),scores)) scores = np.where(scores==np.inf,maxnoninf,scores) auc = aeu.get_roc(scores, labels, mode='full', bootstrap_samples=100) plt.show() def plot_examples(hists_good,hists_bad,model): # set parameters nexamples = 6 fig,axs = plt.subplots(2,nexamples,figsize=(24,12)) inds_good = np.random.choice(range(len(hists_good)),nexamples) inds_bad = np.random.choice(range(len(hists_bad)),nexamples) inds_ref = np.random.choice(range(len(hists_good)),20) # determine whether to show run/lumi number in label (not possible when using resampled sets) truelabel = True if( len(hists_good)!=len(runnbs_good) or len(hists_bad)!=len(runnbs_good) ): truelabel = False # plot examples for i in range(nexamples): hist_good = hists_good[inds_good[i]:inds_good[i]+1] reco_good = model.predict(hist_good) hist_bad = hists_bad[inds_bad[i]:inds_bad[i]+1] reco_bad = model.predict(hist_bad) hist_good_label = hist_bad_label = 'hist' if truelabel: hist_good_label += ' (run: '+str(int(runnbs_good[inds_good[i]]))+', ls: '+str(int(lsnbs_good[inds_good[i]]))+')' hist_bad_label += ' (run: '+str(int(runnbs_bad[inds_bad[i]]))+', ls: '+str(int(lsnbs_bad[inds_bad[i]]))+')' pu.plot_sets([hist_good,reco_good,hists_good[inds_ref]], fig=fig,ax=axs[0,i], title='', colorlist=['black','red','blue'], labellist=[hist_good_label,'reco','good hists'], transparencylist=[1.,1.,0.1]) pu.plot_sets([hist_bad,reco_bad,hists_good[inds_ref]], fig=fig,ax=axs[1,i], title='', colorlist=['black','red','blue'], labellist=[hist_bad_label,'reco','good hists'], transparencylist=[1.,1.,0.1]) plt.show() ### iterate over growing amount of data nruns = [5,10] # first iteration manually X_train = hists_all[np.where(runnbs_all<runs_all[nruns[0]])] print('size of training set (intial): '+str(len(X_train))) (X_train_ext,_,_) = gdu.upsample_hist_set(X_train, 1e5) #X_train_ext = X_train model = aeu.train_simple_autoencoder(X_train_ext,nepochs=10) print('evaluating model on test set') test_autoencoder(hists_good,hists_bad,model) plot_examples(hists_good,hists_bad,model) # next iterations in a loop for i in range(1,len(nruns)): this_upperbound = runs_all[nruns[i]] this_lowerbound = runs_all[nruns[i-1]] print('adding runs {} to {}'.format(this_lowerbound,this_upperbound)) print('(training on {} runs in total)'.format(nruns[i])) newhists = hists_all[np.where( (runnbs_all<this_upperbound) & (runnbs_all>=this_lowerbound) )] print('number of new histograms added to training set: '+str(len(newhists))) X_train = np.concatenate( (X_train,newhists), axis=0 ) X_train = purify_training_set(X_train,model,0.1) print('size of training set (intial): '+str(len(X_train))) (X_train_ext,_,_) = gdu.upsample_hist_set(X_train, 1e5) #X_train_ext = X_train model = aeu.train_simple_autoencoder(X_train_ext) print('size of training set (after training and purifying): '+str(len(X_train))) print('evaluating model on test set') test_autoencoder(hists_good,hists_bad,model) plot_examples(hists_good,hists_bad,model) size of training set (intial): 1017 2022-07-26 17:29:35.167654: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc8-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc8-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc8-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.30-e5b21/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc8-opt/lib64/R/library/readr/rcon 2022-07-26 17:29:35.167722: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303) 2022-07-26 17:29:35.167818: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (59ec73fb7463): /proc/driver/nvidia/version does not exist 2022-07-26 17:29:35.169168: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 51) 5253 _________________________________________________________________ dense_1 (Dense) (None, 102) 5304 ================================================================= Total params: 10,557 Trainable params: 10,557 Non-trainable params: 0 _________________________________________________________________ 2022-07-26 17:29:37.178509: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2) 2022-07-26 17:29:37.180016: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2399845000 Hz Epoch 1/10 180/180 [==============================] - 10s 31ms/step - loss: 1.5311e-04 - val_loss: 4.9320e-06 Epoch 2/10 180/180 [==============================] - 5s 27ms/step - loss: 4.5635e-06 - val_loss: 4.2272e-06 Epoch 3/10 180/180 [==============================] - 5s 26ms/step - loss: 3.9521e-06 - val_loss: 3.6740e-06 Epoch 4/10 180/180 [==============================] - 6s 32ms/step - loss: 3.4399e-06 - val_loss: 3.2126e-06 Epoch 5/10 180/180 [==============================] - 8s 42ms/step - loss: 3.0351e-06 - val_loss: 2.8201e-06 Epoch 6/10 180/180 [==============================] - 6s 32ms/step - loss: 2.6835e-06 - val_loss: 2.4938e-06s - loss: 2.6835e-0 Epoch 7/10 180/180 [==============================] - 5s 30ms/step - loss: 2.3665e-06 - val_loss: 2.2352e-06 Epoch 8/10 180/180 [==============================] - 5s 30ms/step - loss: 2.1452e-06 - val_loss: 2.0101e-06 Epoch 9/10 180/180 [==============================] - 6s 36ms/step - loss: 1.9231e-06 - val_loss: 1.8349e-06 Epoch 10/10 180/180 [==============================] - 9s 52ms/step - loss: 1.8513e-06 - val_loss: 1.8462e-06 evaluating model on test set calculating ROC curve on 100 bootstrap samples of size 3780 adding runs 297057 to 297114 (training on 10 runs in total) number of new histograms added to training set: 2657 size of training set (intial): 3306 Model: \"sequential_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 51) 5253 _________________________________________________________________ dense_3 (Dense) (None, 102) 5304 ================================================================= Total params: 10,557 Trainable params: 10,557 Non-trainable params: 0 _________________________________________________________________ Epoch 1/40 179/179 [==============================] - 7s 25ms/step - loss: 1.1214e-04 - val_loss: 5.1926e-06 Epoch 2/40 179/179 [==============================] - 4s 21ms/step - loss: 4.9028e-06 - val_loss: 4.6196e-06 Epoch 3/40 179/179 [==============================] - 3s 18ms/step - loss: 4.3412e-06 - val_loss: 4.0848e-06 Epoch 4/40 179/179 [==============================] - 3s 17ms/step - loss: 3.8516e-06 - val_loss: 3.6149e-06 Epoch 5/40 179/179 [==============================] - 3s 18ms/step - loss: 3.3940e-06 - val_loss: 3.1668e-06 Epoch 6/40 179/179 [==============================] - 3s 19ms/step - loss: 2.9781e-06 - val_loss: 2.7924e-06 Epoch 7/40 179/179 [==============================] - 3s 19ms/step - loss: 2.6401e-06 - val_loss: 2.4741e-06 Epoch 8/40 179/179 [==============================] - 3s 18ms/step - loss: 2.4025e-06 - val_loss: 2.2292e-06 Epoch 9/40 179/179 [==============================] - 3s 18ms/step - loss: 2.0766e-06 - val_loss: 1.9773e-06 Epoch 10/40 179/179 [==============================] - 3s 17ms/step - loss: 1.8608e-06 - val_loss: 1.7755e-06 Epoch 11/40 179/179 [==============================] - 3s 18ms/step - loss: 1.8868e-06 - val_loss: 1.8752e-06 Epoch 12/40 179/179 [==============================] - 3s 19ms/step - loss: 1.5737e-06 - val_loss: 1.4608e-06 Epoch 13/40 179/179 [==============================] - 3s 18ms/step - loss: 1.3841e-06 - val_loss: 1.3289e-06 Epoch 14/40 179/179 [==============================] - 3s 20ms/step - loss: 1.2598e-06 - val_loss: 1.2143e-06 Epoch 15/40 179/179 [==============================] - 3s 18ms/step - loss: 1.1874e-06 - val_loss: 1.7827e-06 Epoch 16/40 179/179 [==============================] - 3s 15ms/step - loss: 1.4452e-06 - val_loss: 1.0406e-06 Epoch 17/40 179/179 [==============================] - 3s 19ms/step - loss: 9.9276e-07 - val_loss: 9.7675e-07 Epoch 18/40 179/179 [==============================] - 3s 17ms/step - loss: 9.2855e-07 - val_loss: 9.1790e-07 Epoch 19/40 179/179 [==============================] - 3s 18ms/step - loss: 8.7365e-07 - val_loss: 8.6198e-07 Epoch 20/40 179/179 [==============================] - 3s 18ms/step - loss: 8.2775e-07 - val_loss: 8.1381e-07 Epoch 21/40 179/179 [==============================] - 3s 18ms/step - loss: 7.8679e-07 - val_loss: 7.8189e-07 Epoch 22/40 179/179 [==============================] - 4s 20ms/step - loss: 7.5235e-07 - val_loss: 7.4516e-07 Epoch 23/40 179/179 [==============================] - 3s 15ms/step - loss: 7.2027e-07 - val_loss: 7.0982e-07 Epoch 24/40 179/179 [==============================] - 3s 18ms/step - loss: 6.9286e-07 - val_loss: 6.8907e-07 Epoch 25/40 179/179 [==============================] - 3s 16ms/step - loss: 2.5623e-06 - val_loss: 5.6969e-06 Epoch 26/40 179/179 [==============================] - 3s 18ms/step - loss: 1.2265e-06 - val_loss: 6.3184e-07 Epoch 27/40 179/179 [==============================] - 3s 19ms/step - loss: 6.1497e-07 - val_loss: 6.1188e-07 Epoch 28/40 179/179 [==============================] - 3s 19ms/step - loss: 5.9777e-07 - val_loss: 5.9449e-07 Epoch 29/40 179/179 [==============================] - 4s 20ms/step - loss: 5.8060e-07 - val_loss: 5.7774e-07 Epoch 30/40 179/179 [==============================] - 3s 19ms/step - loss: 5.6749e-07 - val_loss: 5.6069e-07 Epoch 31/40 179/179 [==============================] - 3s 18ms/step - loss: 5.4924e-07 - val_loss: 5.4804e-07 Epoch 32/40 179/179 [==============================] - 3s 19ms/step - loss: 5.3318e-07 - val_loss: 5.2841e-07 Epoch 33/40 179/179 [==============================] - 3s 19ms/step - loss: 5.1866e-07 - val_loss: 5.1238e-07 Epoch 34/40 179/179 [==============================] - 3s 16ms/step - loss: 5.1258e-07 - val_loss: 4.9960e-07 Epoch 35/40 179/179 [==============================] - 3s 19ms/step - loss: 4.9696e-07 - val_loss: 4.8245e-07 Epoch 36/40 179/179 [==============================] - 3s 19ms/step - loss: 4.7906e-07 - val_loss: 4.6904e-07 Epoch 37/40 179/179 [==============================] - 3s 19ms/step - loss: 4.7037e-07 - val_loss: 4.5758e-07 Epoch 38/40 179/179 [==============================] - 3s 18ms/step - loss: 4.7814e-07 - val_loss: 4.5095e-07 Epoch 39/40 179/179 [==============================] - 3s 17ms/step - loss: 4.1904e-06 - val_loss: 1.0307e-06 Epoch 40/40 179/179 [==============================] - 3s 17ms/step - loss: 4.8127e-07 - val_loss: 4.1665e-07 size of training set (after training and purifying): 3306 evaluating model on test set calculating ROC curve on 100 bootstrap samples of size 3780","title":"Part 2: Updating the training set"},{"location":"tutorials/autoencoder_iterative/#part-3-local-training-on-nearby-runs","text":"### choose random run for testing, train on previous runs # (testing: see next cell) # choose runs print('number of available runs: '+str(len(runs_all))) runindex = np.random.choice(range(5,len(runs_all))) #runindex = runs_all.index(305364) test_run = runs_all[runindex] print('chosen run index: '+str(runindex)+', corresponding to run: '+str(test_run)) training_runs = runs_all[runindex-5:runindex] print('runs used for training: '+str(training_runs)) # get the data df = dfu.select_dcson(df) (hists_train,runnbs_train,lsnbs_train) = hu.preparedatafromdf( dfu.select_runs(df,training_runs),returnrunls=True,donormalize=True) (hists_test,runnbs_test,lsnbs_test) = hu.preparedatafromdf( dfu.select_runs(df,[test_run]),returnrunls=True,donormalize=True) print('shape of training set '+str(hists_train.shape)) print('shape of test set '+str(hists_test.shape)) # make plot pu.plot_sets([hists_train,hists_test],ax=None,title='',colorlist=['b','g'],labellist=['train','test'],transparencylist=[0.5,0.5],xlims=(0,-1)) plt.show() # train the autoencoder (X_train,_,_) = gdu.upsample_hist_set(hists_train, 1e5) #X_train = hists_train model = aeu.train_simple_autoencoder(X_train) number of available runs: 564 chosen run index: 412, corresponding to run: 304209 runs used for training: [304197, 304198, 304199, 304200, 304204] shape of training set (909, 102) shape of test set (425, 102) Model: \"sequential_2\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_4 (Dense) (None, 51) 5253 _________________________________________________________________ dense_5 (Dense) (None, 102) 5304 ================================================================= Total params: 10,557 Trainable params: 10,557 Non-trainable params: 0 _________________________________________________________________ Epoch 1/40 180/180 [==============================] - 2s 7ms/step - loss: 1.6187e-04 - val_loss: 5.1549e-06 Epoch 2/40 180/180 [==============================] - 1s 6ms/step - loss: 4.8933e-06 - val_loss: 4.6664e-06 Epoch 3/40 180/180 [==============================] - 1s 6ms/step - loss: 4.4298e-06 - val_loss: 4.2151e-06 Epoch 4/40 180/180 [==============================] - 1s 6ms/step - loss: 3.9914e-06 - val_loss: 3.7808e-06 Epoch 5/40 180/180 [==============================] - 1s 6ms/step - loss: 3.5813e-06 - val_loss: 3.3762e-06 Epoch 6/40 180/180 [==============================] - 1s 6ms/step - loss: 3.2062e-06 - val_loss: 3.0032e-06 Epoch 7/40 180/180 [==============================] - 1s 6ms/step - loss: 2.8450e-06 - val_loss: 2.6555e-06 Epoch 8/40 180/180 [==============================] - 1s 7ms/step - loss: 2.5000e-06 - val_loss: 2.3279e-06 Epoch 9/40 180/180 [==============================] - 1s 7ms/step - loss: 2.1955e-06 - val_loss: 2.0381e-06 Epoch 10/40 180/180 [==============================] - 1s 7ms/step - loss: 1.9759e-06 - val_loss: 1.8084e-06 Epoch 11/40 180/180 [==============================] - 1s 6ms/step - loss: 1.7138e-06 - val_loss: 1.5692e-06 Epoch 12/40 180/180 [==============================] - 1s 6ms/step - loss: 1.6443e-06 - val_loss: 1.3883e-06 Epoch 13/40 180/180 [==============================] - 1s 7ms/step - loss: 1.3263e-06 - val_loss: 1.2380e-06 Epoch 14/40 180/180 [==============================] - 1s 6ms/step - loss: 1.1858e-06 - val_loss: 1.1198e-06 Epoch 15/40 180/180 [==============================] - 1s 7ms/step - loss: 1.0711e-06 - val_loss: 1.0200e-06 Epoch 16/40 180/180 [==============================] - 1s 6ms/step - loss: 1.2255e-06 - val_loss: 9.8934e-07 Epoch 17/40 180/180 [==============================] - 1s 5ms/step - loss: 9.1090e-07 - val_loss: 8.4949e-07 Epoch 18/40 180/180 [==============================] - 1s 7ms/step - loss: 8.5840e-07 - val_loss: 9.2149e-07 Epoch 19/40 180/180 [==============================] - 1s 6ms/step - loss: 1.1008e-06 - val_loss: 7.3578e-07 Epoch 20/40 180/180 [==============================] - 1s 7ms/step - loss: 7.1226e-07 - val_loss: 6.9035e-07 Epoch 21/40 180/180 [==============================] - 1s 6ms/step - loss: 6.6963e-07 - val_loss: 6.5100e-07 Epoch 22/40 180/180 [==============================] - 1s 7ms/step - loss: 6.3295e-07 - val_loss: 6.1755e-07 Epoch 23/40 180/180 [==============================] - 1s 7ms/step - loss: 6.0135e-07 - val_loss: 5.9092e-07 Epoch 24/40 180/180 [==============================] - 1s 6ms/step - loss: 5.7492e-07 - val_loss: 5.6528e-07 Epoch 25/40 180/180 [==============================] - 1s 6ms/step - loss: 5.5186e-07 - val_loss: 5.4899e-07 Epoch 26/40 180/180 [==============================] - 1s 6ms/step - loss: 5.3143e-07 - val_loss: 5.1981e-07 Epoch 27/40 180/180 [==============================] - 1s 6ms/step - loss: 5.1755e-07 - val_loss: 5.0836e-07 Epoch 28/40 180/180 [==============================] - 1s 6ms/step - loss: 1.2912e-06 - val_loss: 1.8068e-05 Epoch 29/40 180/180 [==============================] - 1s 6ms/step - loss: 4.3261e-06 - val_loss: 4.7346e-07 Epoch 30/40 180/180 [==============================] - 1s 6ms/step - loss: 4.6356e-07 - val_loss: 4.5966e-07 Epoch 31/40 180/180 [==============================] - 1s 6ms/step - loss: 4.5268e-07 - val_loss: 4.4805e-07 Epoch 32/40 180/180 [==============================] - 1s 6ms/step - loss: 4.4301e-07 - val_loss: 4.4015e-07 Epoch 33/40 180/180 [==============================] - 1s 6ms/step - loss: 4.3409e-07 - val_loss: 4.3113e-07 Epoch 34/40 180/180 [==============================] - 1s 7ms/step - loss: 5.3247e-07 - val_loss: 6.7669e-07 Epoch 35/40 180/180 [==============================] - 1s 6ms/step - loss: 5.2115e-07 - val_loss: 4.1663e-07 Epoch 36/40 180/180 [==============================] - 1s 6ms/step - loss: 4.0731e-07 - val_loss: 4.0308e-07 Epoch 37/40 180/180 [==============================] - 1s 6ms/step - loss: 3.9863e-07 - val_loss: 3.9545e-07 Epoch 38/40 180/180 [==============================] - 1s 7ms/step - loss: 3.9706e-07 - val_loss: 3.9387e-07 Epoch 39/40 180/180 [==============================] - 1s 6ms/step - loss: 3.8418e-07 - val_loss: 3.9225e-07 Epoch 40/40 180/180 [==============================] - 1s 6ms/step - loss: 3.7796e-07 - val_loss: 3.7572e-07 ### test the autoencoder trained in the previous cell # note: do not simply use hists_good for a good test set, # as the model might not be trained on all shape variations that are supposed to be 'good', # resulting in artificially bad performance. # we can use hists_test instead, but it is not at all guaranteed that all of them are good. # hence the label 'good hists' in the plot below is not necessarily correct. test_autoencoder(hists_test,hists_bad,model) plot_examples(hists_test,hists_bad,model) calculating ROC curve on 100 bootstrap samples of size 2365","title":"Part 3: Local training on nearby runs"},{"location":"tutorials/generate_data/","text":"Exploration of data generation ('resampling') methods This notebook lists and plots some of the implemented methods for resampling histograms in order to artificially increase the statistics of training or testing sets. ### imports # external modules import sys import os import matplotlib.pyplot as plt # local modules sys.path.append('../utils') import hist_utils as hu import dataframe_utils as dfu import generate_data_utils as gdu import plot_utils as pu sys.path.append('../src') import DataLoader ### load the data # note: this cell assumes you have a csv file stored at the specified location, # containing only histograms of the specified type; # see the tutorial read_and_write_data for examples on how to create such files! histname = 'chargeInner_PXLayer_2' filename = 'DF2017_'+histname+'.csv' datadir = '../data' dloader = DataLoader.DataLoader() df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) allhists = hu.preparedatafromdf(df,donormalize=True) # note: depending on which histogram you are looking at, the 'good' and 'bad' runs defined below might not be good or bad at all! # you will need to find a set of clearly good and bad runs for you type(s) of histogram. goodrunsls = {'2017': { \"297056\":[[-1]], \"297177\":[[-1]], \"301449\":[[-1]] }} badrunsls = {'2017': { \"297287\":[[-1]], \"297288\":[[-1]], \"297289\":[[-1]], \"299316\":[[-1]], \"299324\":[[-1]], }} goodhists = hu.preparedatafromdf(dfu.select_runsls(df,goodrunsls['2017']),donormalize=True) badhists = hu.preparedatafromdf(dfu.select_runsls(df,badrunsls['2017']),donormalize=True) # plot some together pu.plot_sets([goodhists,badhists],colorlist=['b','r'],labellist=['\"good\" histograms','\"bad\" histograms']) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_2.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. raw input data shape: (225954, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### also select a seed seed = dfu.select_runsls(df,{\"297056\":[[100,100]]}) run = dfu.select_runs(df,[297056]) seedhist = hu.preparedatafromdf(seed,donormalize=True) runhists = hu.preparedatafromdf(run,donormalize=True) # plot some together pu.plot_sets([runhists,seedhist],colorlist=['lightblue','k'],labellist=['histograms','seed histogram']) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for fourier_noise_on_mean (reshists,_,_) = gdu.fourier_noise_on_mean(allhists, nresamples=10, nonnegative=True, doplot=True) print('size of original set: {}'.format(allhists.shape)) print('size of resampled set: {}'.format(reshists.shape)) pu.plot_sets([hu.select_random(allhists, nselect=3), hu.select_random(reshists, nselect=3)], colorlist=['k','b'], labellist=['original histograms','resampled histograms'], transparencylist=[0.5,0.5]) size of original set: (225954, 102) size of resampled set: (10, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for fourier_noise (greshists,_,_) = gdu.fourier_noise(goodhists, nresamples=10, nonnegative=True, doplot=True) (breshists,_,_) = gdu.fourier_noise(badhists, nresamples=9, nonnegative=True, stdfactor=3., doplot=True) print('size of resampled good set: {}'.format(greshists.shape)) print('size of resampled bad set: {}'.format(breshists.shape)) pu.plot_sets([hu.select_random(greshists, nselect=100), hu.select_random(breshists, nselect=100)], colorlist=['b','r'], labellist=['resampled good histograms','resampled bad histograms'], transparencylist=[0.5,0.5]) size of resampled good set: (7380, 102) size of resampled bad set: (1449, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for resample_bin_per_bin (reshists,_,_) = gdu.resample_bin_per_bin(allhists, nresamples=10, nonnegative=True, smoothinghalfwidth=0, doplot=True) print('size of original set: {}'.format(allhists.shape)) print('size of resampled set: {}'.format(reshists.shape)) pu.plot_sets([hu.select_random(allhists, nselect=3), hu.select_random(reshists, nselect=3)], colorlist=['k','b'], labellist=['original histograms','resampled histograms'], transparencylist=[0.5,0.5]) size of original set: (225954, 102) size of resampled set: (10, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for resample_similar_bin_per_bin (greshists,_,_) = gdu.resample_similar_bin_per_bin(allhists, goodhists, nresamples=3, nonnegative=True, keeppercentage=0.005, doplot=True) (breshists,_,_) = gdu.resample_similar_bin_per_bin(allhists, badhists, nresamples=3, nonnegative=True, keeppercentage=0.003, doplot=True) print('size of resampled good set: {}'.format(greshists.shape)) print('size of resampled bad set: {}'.format(breshists.shape)) pu.plot_sets([hu.select_random(greshists, nselect=100), hu.select_random(breshists, nselect=100)], colorlist=['b','r'], labellist=['resampled good histograms','resampled bad histograms'], transparencylist=[0.5,0.5]) Note: bin-per-bin resampling performed on 12 histograms. If this number is too low, existing histograms are drawn with too small variation. If this number is too high, systematic shifts of histograms can be averaged out. Note: bin-per-bin resampling performed on 7 histograms. If this number is too low, existing histograms are drawn with too small variation. If this number is too high, systematic shifts of histograms can be averaged out. size of resampled good set: (2214, 102) size of resampled bad set: (483, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for resample_similar_fourier_noise (greshists,_,_) = gdu.resample_similar_fourier_noise(allhists, goodhists, nresamples=3, nonnegative=True, keeppercentage=0.001, doplot=True) (breshists,_,_) = gdu.resample_similar_fourier_noise(allhists, badhists, nresamples=3, nonnegative=True, keeppercentage=0.001, doplot=True) print('size of resampled good set: {}'.format(greshists.shape)) print('size of resampled bad set: {}'.format(breshists.shape)) pu.plot_sets([hu.select_random(greshists, nselect=100), hu.select_random(breshists, nselect=100)], colorlist=['b','r'], labellist=['resampled good histograms','resampled bad histograms'], transparencylist=[0.5,0.5]) /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice. return _methods._mean(a, axis=axis, dtype=dtype, /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide ret = um.true_divide( /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:262: RuntimeWarning: Degrees of freedom <= 0 for slice ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof, /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe', /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:251: RuntimeWarning: invalid value encountered in true_divide ret = um.true_divide( Note: mean and std calculation is performed on 3 histograms. If this number is too low, histograms might be too similar for averaging to have effect. If this number is too high, systematic shifts of histogram shapes are included into the averaging. /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice. return _methods._mean(a, axis=axis, dtype=dtype, /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide ret = um.true_divide( /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:262: RuntimeWarning: Degrees of freedom <= 0 for slice ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof, /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe', /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:251: RuntimeWarning: invalid value encountered in true_divide ret = um.true_divide( Note: mean and std calculation is performed on 3 histograms. If this number is too low, histograms might be too similar for averaging to have effect. If this number is too high, systematic shifts of histogram shapes are included into the averaging. size of resampled good set: (2214, 102) size of resampled bad set: (483, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for resample_similar_lico (greshists,_,_) = gdu.resample_similar_lico(allhists,goodhists,nresamples=10,nonnegative=True,keeppercentage=0.1, doplot=True) (breshists,_,_) = gdu.resample_similar_lico(allhists,badhists,nresamples=1,nonnegative=False,keeppercentage=0.001, doplot=True) print('size of resampled good set: {}'.format(greshists.shape)) print('size of resampled bad set: {}'.format(breshists.shape)) pu.plot_sets([hu.select_random(greshists, nselect=100), hu.select_random(breshists, nselect=100)], colorlist=['b','r'], labellist=['resampled good histograms','resampled bad histograms'], transparencylist=[0.5,0.5]) Note: linear combination is taken between 226 histograms. If this number is too low, histograms might be too similar for combination to have effect. If this number is too high, systematic shifts of histogram shapes are included into the combination Note: linear combination is taken between 3 histograms. If this number is too low, histograms might be too similar for combination to have effect. If this number is too high, systematic shifts of histogram shapes are included into the combination size of resampled good set: (7380, 102) size of resampled bad set: (161, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for mc_sampling (reshists,_,_) = gdu.mc_sampling(seedhist, nresamples=10, nMC=10000, doplot=True) print('size of resampled set: {}'.format(reshists.shape)) pu.plot_sets([seedhist, hu.select_random(reshists, nselect=3)], colorlist=['k','b'], labellist=['original histogram','resampled histograms'], transparencylist=[0.5,0.5]) size of resampled set: (10, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for white_noise (greshists,_,_) = gdu.white_noise(goodhists, stdfactor=15, doplot=True) (breshists,_,_) = gdu.white_noise(badhists, stdfactor=3., doplot=True) print('size of resampled good set: {}'.format(greshists.shape)) print('size of resampled bad set: {}'.format(breshists.shape)) pu.plot_sets([hu.select_random(greshists, nselect=100), hu.select_random(breshists, nselect=100)], colorlist=['b','r'], labellist=['resampled good histograms','resampled bad histograms'], transparencylist=[0.5,0.5]) size of resampled good set: (738, 102) size of resampled bad set: (161, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>)","title":"generate_data"},{"location":"tutorials/generate_data/#exploration-of-data-generation-resampling-methods","text":"This notebook lists and plots some of the implemented methods for resampling histograms in order to artificially increase the statistics of training or testing sets. ### imports # external modules import sys import os import matplotlib.pyplot as plt # local modules sys.path.append('../utils') import hist_utils as hu import dataframe_utils as dfu import generate_data_utils as gdu import plot_utils as pu sys.path.append('../src') import DataLoader ### load the data # note: this cell assumes you have a csv file stored at the specified location, # containing only histograms of the specified type; # see the tutorial read_and_write_data for examples on how to create such files! histname = 'chargeInner_PXLayer_2' filename = 'DF2017_'+histname+'.csv' datadir = '../data' dloader = DataLoader.DataLoader() df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) allhists = hu.preparedatafromdf(df,donormalize=True) # note: depending on which histogram you are looking at, the 'good' and 'bad' runs defined below might not be good or bad at all! # you will need to find a set of clearly good and bad runs for you type(s) of histogram. goodrunsls = {'2017': { \"297056\":[[-1]], \"297177\":[[-1]], \"301449\":[[-1]] }} badrunsls = {'2017': { \"297287\":[[-1]], \"297288\":[[-1]], \"297289\":[[-1]], \"299316\":[[-1]], \"299324\":[[-1]], }} goodhists = hu.preparedatafromdf(dfu.select_runsls(df,goodrunsls['2017']),donormalize=True) badhists = hu.preparedatafromdf(dfu.select_runsls(df,badrunsls['2017']),donormalize=True) # plot some together pu.plot_sets([goodhists,badhists],colorlist=['b','r'],labellist=['\"good\" histograms','\"bad\" histograms']) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_2.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. raw input data shape: (225954, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### also select a seed seed = dfu.select_runsls(df,{\"297056\":[[100,100]]}) run = dfu.select_runs(df,[297056]) seedhist = hu.preparedatafromdf(seed,donormalize=True) runhists = hu.preparedatafromdf(run,donormalize=True) # plot some together pu.plot_sets([runhists,seedhist],colorlist=['lightblue','k'],labellist=['histograms','seed histogram']) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for fourier_noise_on_mean (reshists,_,_) = gdu.fourier_noise_on_mean(allhists, nresamples=10, nonnegative=True, doplot=True) print('size of original set: {}'.format(allhists.shape)) print('size of resampled set: {}'.format(reshists.shape)) pu.plot_sets([hu.select_random(allhists, nselect=3), hu.select_random(reshists, nselect=3)], colorlist=['k','b'], labellist=['original histograms','resampled histograms'], transparencylist=[0.5,0.5]) size of original set: (225954, 102) size of resampled set: (10, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for fourier_noise (greshists,_,_) = gdu.fourier_noise(goodhists, nresamples=10, nonnegative=True, doplot=True) (breshists,_,_) = gdu.fourier_noise(badhists, nresamples=9, nonnegative=True, stdfactor=3., doplot=True) print('size of resampled good set: {}'.format(greshists.shape)) print('size of resampled bad set: {}'.format(breshists.shape)) pu.plot_sets([hu.select_random(greshists, nselect=100), hu.select_random(breshists, nselect=100)], colorlist=['b','r'], labellist=['resampled good histograms','resampled bad histograms'], transparencylist=[0.5,0.5]) size of resampled good set: (7380, 102) size of resampled bad set: (1449, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for resample_bin_per_bin (reshists,_,_) = gdu.resample_bin_per_bin(allhists, nresamples=10, nonnegative=True, smoothinghalfwidth=0, doplot=True) print('size of original set: {}'.format(allhists.shape)) print('size of resampled set: {}'.format(reshists.shape)) pu.plot_sets([hu.select_random(allhists, nselect=3), hu.select_random(reshists, nselect=3)], colorlist=['k','b'], labellist=['original histograms','resampled histograms'], transparencylist=[0.5,0.5]) size of original set: (225954, 102) size of resampled set: (10, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for resample_similar_bin_per_bin (greshists,_,_) = gdu.resample_similar_bin_per_bin(allhists, goodhists, nresamples=3, nonnegative=True, keeppercentage=0.005, doplot=True) (breshists,_,_) = gdu.resample_similar_bin_per_bin(allhists, badhists, nresamples=3, nonnegative=True, keeppercentage=0.003, doplot=True) print('size of resampled good set: {}'.format(greshists.shape)) print('size of resampled bad set: {}'.format(breshists.shape)) pu.plot_sets([hu.select_random(greshists, nselect=100), hu.select_random(breshists, nselect=100)], colorlist=['b','r'], labellist=['resampled good histograms','resampled bad histograms'], transparencylist=[0.5,0.5]) Note: bin-per-bin resampling performed on 12 histograms. If this number is too low, existing histograms are drawn with too small variation. If this number is too high, systematic shifts of histograms can be averaged out. Note: bin-per-bin resampling performed on 7 histograms. If this number is too low, existing histograms are drawn with too small variation. If this number is too high, systematic shifts of histograms can be averaged out. size of resampled good set: (2214, 102) size of resampled bad set: (483, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for resample_similar_fourier_noise (greshists,_,_) = gdu.resample_similar_fourier_noise(allhists, goodhists, nresamples=3, nonnegative=True, keeppercentage=0.001, doplot=True) (breshists,_,_) = gdu.resample_similar_fourier_noise(allhists, badhists, nresamples=3, nonnegative=True, keeppercentage=0.001, doplot=True) print('size of resampled good set: {}'.format(greshists.shape)) print('size of resampled bad set: {}'.format(breshists.shape)) pu.plot_sets([hu.select_random(greshists, nselect=100), hu.select_random(breshists, nselect=100)], colorlist=['b','r'], labellist=['resampled good histograms','resampled bad histograms'], transparencylist=[0.5,0.5]) /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice. return _methods._mean(a, axis=axis, dtype=dtype, /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide ret = um.true_divide( /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:262: RuntimeWarning: Degrees of freedom <= 0 for slice ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof, /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe', /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:251: RuntimeWarning: invalid value encountered in true_divide ret = um.true_divide( Note: mean and std calculation is performed on 3 histograms. If this number is too low, histograms might be too similar for averaging to have effect. If this number is too high, systematic shifts of histogram shapes are included into the averaging. /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice. return _methods._mean(a, axis=axis, dtype=dtype, /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide ret = um.true_divide( /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:262: RuntimeWarning: Degrees of freedom <= 0 for slice ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof, /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe', /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/numpy/core/_methods.py:251: RuntimeWarning: invalid value encountered in true_divide ret = um.true_divide( Note: mean and std calculation is performed on 3 histograms. If this number is too low, histograms might be too similar for averaging to have effect. If this number is too high, systematic shifts of histogram shapes are included into the averaging. size of resampled good set: (2214, 102) size of resampled bad set: (483, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for resample_similar_lico (greshists,_,_) = gdu.resample_similar_lico(allhists,goodhists,nresamples=10,nonnegative=True,keeppercentage=0.1, doplot=True) (breshists,_,_) = gdu.resample_similar_lico(allhists,badhists,nresamples=1,nonnegative=False,keeppercentage=0.001, doplot=True) print('size of resampled good set: {}'.format(greshists.shape)) print('size of resampled bad set: {}'.format(breshists.shape)) pu.plot_sets([hu.select_random(greshists, nselect=100), hu.select_random(breshists, nselect=100)], colorlist=['b','r'], labellist=['resampled good histograms','resampled bad histograms'], transparencylist=[0.5,0.5]) Note: linear combination is taken between 226 histograms. If this number is too low, histograms might be too similar for combination to have effect. If this number is too high, systematic shifts of histogram shapes are included into the combination Note: linear combination is taken between 3 histograms. If this number is too low, histograms might be too similar for combination to have effect. If this number is too high, systematic shifts of histogram shapes are included into the combination size of resampled good set: (7380, 102) size of resampled bad set: (161, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for mc_sampling (reshists,_,_) = gdu.mc_sampling(seedhist, nresamples=10, nMC=10000, doplot=True) print('size of resampled set: {}'.format(reshists.shape)) pu.plot_sets([seedhist, hu.select_random(reshists, nselect=3)], colorlist=['k','b'], labellist=['original histogram','resampled histograms'], transparencylist=[0.5,0.5]) size of resampled set: (10, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>) ### testing section for white_noise (greshists,_,_) = gdu.white_noise(goodhists, stdfactor=15, doplot=True) (breshists,_,_) = gdu.white_noise(badhists, stdfactor=3., doplot=True) print('size of resampled good set: {}'.format(greshists.shape)) print('size of resampled bad set: {}'.format(breshists.shape)) pu.plot_sets([hu.select_random(greshists, nselect=100), hu.select_random(breshists, nselect=100)], colorlist=['b','r'], labellist=['resampled good histograms','resampled bad histograms'], transparencylist=[0.5,0.5]) size of resampled good set: (738, 102) size of resampled bad set: (161, 102) (<Figure size 432x288 with 1 Axes>, <AxesSubplot:>)","title":"Exploration of data generation ('resampling') methods"},{"location":"tutorials/global_combined_training/","text":"Train a combined model with global training and evaluate its predictions This notebook uses multiple types of histograms, trains a single-histogram classifier for each of them, combines the output and assesses the performance of this combined model. ### imports # external modules import os import sys import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt # local modules sys.path.append('../utils') import csv_utils as csvu import json_utils as jsonu import dataframe_utils as dfu import hist_utils as hu import autoencoder_utils as aeu import plot_utils as pu import generate_data_utils as gdu import refruns_utils as rru # import general source sys.path.append('../src') sys.path.append('../src/classifiers') sys.path.append('../src/cloudfitters') import Model import ModelInterface import HistStruct import DataLoader import PlotStyleParser # import classifiers import AutoEncoder import TemplateBasedClassifier import NMFClassifier import LandauClassifier import MomentClassifier # import combination methods import SeminormalFitter 2022-07-26 17:23:22.471603: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc8-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc8-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc8-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.30-e5b21/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc8-opt/lib64/R/library/readr/rcon 2022-07-26 17:23:22.471647: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. ### define titles and axis properties for figures plotstyleparser = PlotStyleParser.PlotStyleParser('plotstyle.json') titledict = plotstyleparser.get_title() xaxtitledict = plotstyleparser.get_xaxtitle() yaxtitledict = plotstyleparser.get_yaxtitle() for key in yaxtitledict.keys(): yaxtitledict[key] += ' (normalized)' # if normalized extratextdict = plotstyleparser.get_extratext() ### define run properties # in this cell all major run properties are going to be set, # e.g. what runs to train on and what runs to test on # define core test set of bad runs badrunsls = {'2017': { \"297287\":[[-1]], \"297288\":[[-1]], \"297289\":[[-1]], \"299316\":[[-1]], \"299317\":[[-1]], \"299318\":[[-1]], \"299324\":[[-1]], \"299325\":[[-1]], \"299326\":[[-1]], \"300373\":[[-1]], \"300374\":[[-1]], \"300397\":[[-1]], \"300398\":[[-1]] } } # set year to use year = '2017' # set histogram names to use histnames = ([ #'chargeInner_PXLayer_1', 'chargeInner_PXLayer_2', 'chargeInner_PXLayer_3', #'chargeInner_PXLayer_4', #'charge_PXDisk_+1','charge_PXDisk_+2','charge_PXDisk_+3', #'charge_PXDisk_-1','charge_PXDisk_-2','charge_PXDisk_-3', ]) # redefine badrunsls for this year only badrunsls = badrunsls[year] print('selected runs/lumisections for training: all') selected runs/lumisections for training: all ### read the data based on the configuration defined above readnew = True if readnew: ### add the histograms # initializations dloader = DataLoader.DataLoader() histstruct = HistStruct.HistStruct() # loop over the histogram types to take into account for histname in histnames: print('adding {}...'.format(histname)) # read the histograms from the correct csv files filename = '../data/DF'+year+'_'+histname+'.csv' df = dloader.get_dataframe_from_file( filename ) # define slice to remove under- and overflow bins cropslices = [slice(1,-1)] # add the dataframe to the histstruct histstruct.add_dataframe( df, cropslices=cropslices ) print('found {} histograms'.format(len(histstruct.runnbs))) ### add masks histstruct.add_dcsonjson_mask( 'dcson' ) histstruct.add_goldenjson_mask('golden' ) histstruct.add_highstat_mask( 'highstat' ) histstruct.add_stat_mask( 'lowstat', max_entries_to_bins_ratio=100 ) nbadruns = 0 histstruct.add_json_mask( 'bad', badrunsls ) # special case for bad runs: add a mask per run (different bad runs have different characteristics) nbadruns = len(badrunsls.keys()) for badrun in badrunsls.keys(): histstruct.add_json_mask( 'bad{}'.format(badrun), {badrun:badrunsls[badrun]} ) print('created a histstruct with the following properties:') print('- number of histogram types: {}'.format(len(histstruct.histnames))) print('- number of lumisections: {}'.format(len(histstruct.lsnbs))) print('- masks: {}'.format(list(histstruct.masks.keys()))) if not readnew: histstruct = HistStruct.HistStruct.load( 'histstruct_global_20220201.zip', verbose=False ) nbadruns = len([name for name in list(histstruct.masks.keys()) if ('bad' in name and name!='bad')]) print('loaded a histstruct with the following properties:') print(histstruct) adding chargeInner_PXLayer_2... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_2.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. adding chargeInner_PXLayer_3... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_3.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. found 225954 histograms created a histstruct with the following properties: - number of histogram types: 2 - number of lumisections: 225954 - masks: ['dcson', 'golden', 'highstat', 'lowstat', 'bad', 'bad297287', 'bad297288', 'bad297289', 'bad299316', 'bad299317', 'bad299318', 'bad299324', 'bad299325', 'bad299326', 'bad300373', 'bad300374', 'bad300397', 'bad300398'] ### plot the training and/or test sets doplot = True if doplot: index_mask = np.random.choice( np.arange(len(histstruct.get_lsnbs())), size=20, replace=False ) histstruct.add_index_mask( 'random_plotting', index_mask ) # bad test runs for badrun in badrunsls.keys(): fig,axs = histstruct.plot_histograms( masknames=[['dcson','highstat','random_plotting'],['dcson','highstat','bad{}'.format(badrun)]], labellist = ['Example histograms','Run {}'.format(badrun)], colorlist = ['blue','red'], transparencylist = [0.1,1.], ncols=3, opaque_legend=True, titledict = titledict, titlesize=15, physicalxax = True, xaxtitledict = xaxtitledict, xaxtitlesize=17, yaxtitledict = yaxtitledict, yaxtitlesize=17, ymaxfactor = 1.2, legendsize = 14 ) # stylistic modifications counter = -1 for i in range(axs.shape[0]): for j in range(axs.shape[1]): counter += 1 if counter>=len(histstruct.histnames): break ax = axs[i,j] pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, extratextdict[histstruct.histnames[counter]], (0.95,0.6), fontsize=15, horizontalalignment='right' ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) fig.subplots_adjust(wspace=0.3, hspace=0.3) histstruct.remove_mask('random_plotting') ### define a good test set as averages from training set npartitions = 50 for histname in histstruct.histnames: histograms = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['dcson','highstat']), npartitions ) histstruct.add_exthistograms( 'partitions', histname, histograms, overwrite=True ) ### make a plot of the good test set doplot = True if doplot: fig,axs = histstruct.plot_histograms( histograms=[histstruct.get_histograms(setnames=['partitions'])], labellist = ['Good test set'], colorlist = ['blue'], transparencylist = [0.1], ncols=3, opaque_legend=True, titledict = titledict, titlesize=15, physicalxax = True, xaxtitledict = xaxtitledict, xaxtitlesize=17, yaxtitledict = yaxtitledict, yaxtitlesize=17, #ymaxfactor = 1.2, legendsize = 14 ) # stylistic modifications counter = -1 for i in range(axs.shape[0]): for j in range(axs.shape[1]): counter += 1 if counter>=len(histstruct.histnames): break ax = axs[i,j] pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, extratextdict[histstruct.histnames[counter]], (0.95,0.6), fontsize=15, horizontalalignment='right' ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) fig.subplots_adjust(wspace=0.3, hspace=0.3) ### print some numbers print(len(histstruct.get_lsnbs())) print(len(histstruct.get_lsnbs(masknames=['dcson']))) print(len(histstruct.get_lsnbs(masknames=['dcson','highstat']))) trainingmasks = ['dcson','highstat'] print(len(histstruct.get_lsnbs(masknames=trainingmasks))) 225954 215144 211171 211171 ### define and train an autoencoder for each element classifymethod = 'nmf' modelname = classifymethod if modelname in histstruct.modelnames: raise Exception('WARNING: modelname \"{}\" is already present in histstruct.'.format(modelname) +' Choose a different name or remove the duplicate.') model = ModelInterface.ModelInterface(histstruct.histnames) classifiers = {} if classifymethod == 'autoencoder': for histname in histstruct.histnames: print('building Autoencoder model for histogram type {}'.format(histname)) hists = histstruct.get_histograms(histname=histname, masknames=['dcson','highstat']) print('size of training set: {}'.format(hists.shape)) # choose whether to save the model modelname = modelbasename+'_'+histname+'.h5' modelname = os.path.join(modelloc, modelname) if not save: modelname = '' # empty string means do not save models # train the model (aemodel, history) = aeu.train_simple_autoencoder(hists, nepochs=20, modelname=modelname, batch_size=2000, shuffle=True, returnhistory=True ) # make a loss plot fig,ax = pu.plot_loss(history, xaxtitlesize=15, yaxtitlesize=15, legendsize=15, doshow=False) pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) pu.add_text( ax, pu.make_text_latex_safe(histname), (0.95,0.6), fontsize=15, horizontalalignment='right' ) plt.show() # add the model to the histstruct classifiers[histname] = AutoEncoder.AutoEncoder( model=aemodel ) if classifymethod == 'templates': for histname in histstruct.histnames: hists = histstruct.get_histograms(histname=histname, masknames=['dcson','highstat']) hists = hu.averagehists( hists, 25 ) classifier = TemplateBasedClassifier.TemplateBasedClassifier() classifier.train( hists ) classifiers[histname] = classifier if classifymethod == 'nmf': for histname in histstruct.histnames: print('building NMF model for histogram type {}'.format(histname)) hists = histstruct.get_histograms( histname=histname, masknames=['dcson','highstat']) # option: reduce size of training set by choosing randoms random_indices = np.random.choice(len(hists), size=int(5e4), replace=False) hists = hists[random_indices] print('size of training set: {}'.format(hists.shape)) classifier = NMFClassifier.NMFClassifier( ncomponents=3, nmax=10 ) classifier.train( hists ) classifiers[histname] = classifier if classifymethod == 'landaufit': print('initializing Landau fit classifier') classifier = LandauClassifier.LandauClassifier( dogauss=True ) for histname in histstruct.histnames: classifiers[histname] = classifier if classifymethod == 'moments': for histname in histstruct.histnames: print('calculating moments for histogram type {}'.format(histname)) hists = histstruct.get_histograms(histname=histname, masknames=['dcson','highstat']) classifier = MomentClassifier.MomentClassifier( orders=[1,2] ) classifier.train(hists) classifiers[histname] = classifier model.set_classifiers(classifiers) histstruct.add_model( modelname, model ) building NMF model for histogram type chargeInner_PXLayer_2 size of training set: (50000, 100) /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26). warnings.warn((\"The 'init' value, when 'init=None' and \" /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence. warnings.warn(\"Maximum number of iterations %d reached. Increase it to\" building NMF model for histogram type chargeInner_PXLayer_3 size of training set: (50000, 100) /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26). warnings.warn((\"The 'init' value, when 'init=None' and \" /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence. warnings.warn(\"Maximum number of iterations %d reached. Increase it to\" adding model \"nmf\" to the HistStruct ### evaluate the models on all histograms in the (non-extended) histstruct and on the partitions # evaluate classifiers print('evaluating classifiers') histstruct.evaluate_classifiers( modelname ) histstruct.evaluate_classifiers( modelname, setnames=['partitions'] ) evaluating classifiers ### plot the multidemensional mse and fit a distribution # set method and other properties doplot = True # initializations dimslist = [] nhisttypes = len(histstruct.histnames) for i in range(0,nhisttypes-1): for j in range(i+1,nhisttypes): dimslist.append((i,j)) plt.close('all') # define training masks training_masks = ['dcson','highstat'] # train the actual fitter histstruct.set_fitter( modelname, SeminormalFitter.SeminormalFitter() ) histstruct.train_fitter( modelname, masknames=training_masks ) histstruct.evaluate_fitter( modelname ) # train and plot the partial fitters if doplot: histstruct.train_partial_fitters( modelname, dimslist, masknames=training_masks ) for dims in dimslist: fig,ax = histstruct.plot_partial_fit( modelname, dims, clusters=[{'masknames':training_masks}], colors=['b'], labels=['Training lumisections'], logprob=True, clipprob=True, xlims=30, ylims=30, onlypositive=True, transparency=0.5, xaxtitle=pu.make_text_latex_safe(histstruct.histnames[dims[0]])+' MSE', xaxtitlesize=12, yaxtitle=pu.make_text_latex_safe(histstruct.histnames[dims[1]])+' MSE', yaxtitlesize=12, caxtitle='Logarithm of probability density', caxtitlesize=12) pu.add_cms_label( ax, pos=(0.05,0.9), extratext='Preliminary', fontsize=12, background_alpha=0.75 ) pu.add_text( ax, 'Density fit of lumisection MSE', (0.05,0.8), fontsize=12, background_alpha=0.75 ) pu.add_text( ax, '2017 (13 TeV)', (0.73,1.01), fontsize=12 ) #plt.close('all') # release plot memory /eos/home-l/llambrec/SWAN_projects/ML4DQM-DC/tutorials/../utils/plot_utils.py:904: RuntimeWarning: divide by zero encountered in log if logprob: evalpoints = np.log(evalpoints) NOTE: scores of -inf were reset to -745.4400719213812 ### extend the test set using artificial data generation and evaluate the model on the extended test set # make the extra data trainingmasknames = ['dcson','highstat'] for histname in histstruct.histnames: print('generating data for '+histname) # good data option 1: resample partitions #goodhists = histstruct.get_histograms( setnames=['partitions'], histname=histname ) #(goodexthists,_,_) = gdu.upsample_hist_set( goodhists, ntarget=nbadruns*5e3, fourierstdfactor=20., doplot=False) # good data option 2: use training set goodexthists = histstruct.get_histograms( histname=histname, masknames=trainingmasknames ) histstruct.add_exthistograms( 'test_good_ext', histname, goodexthists, overwrite=True ) for badrun in badrunsls.keys(): badhists = histstruct.get_histograms( histname=histname,masknames=['dcson','highstat','bad{}'.format(badrun)] ) (badexthists,_,_) = gdu.upsample_hist_set( badhists, ntarget=5e3, fourierstdfactor=20., doplot=False) histstruct.add_exthistograms( 'test_bad{}_ext'.format(badrun), histname, badexthists, overwrite=True ) # evaluate the classifiers print('evaluating classifiers') histstruct.evaluate_classifiers( modelname, setnames=['test_good_ext'] ) # shortcut if using training set as good test set: copy scores #histstruct.models[modelname].scores['test_good_ext'] = histstruct.get_scores( modelname, masknames=trainingmasknames ) for badrun in badrunsls.keys(): histstruct.evaluate_classifiers( modelname, setnames=['test_bad{}_ext'.format(badrun)]) # evaluate the fitter print('evaluating fitter') histstruct.evaluate_fitter( modelname, setnames=['test_good_ext'] ) for badrun in badrunsls.keys(): histstruct.evaluate_fitter( modelname, setnames=['test_bad{}_ext'.format(badrun)] ) generating data for chargeInner_PXLayer_2 generating data for chargeInner_PXLayer_3 evaluating classifiers evaluating fitter ### (re-)define the test set badrunstokeep = [ \"297287\", \"297288\", \"297289\", \"299316\", \"299317\", \"299318\", \"299324\", \"299325\", \"299326\", \"300373\", \"300374\", \"300397\", \"300398\" ] # make a new mask that is the union of the selected bad runs histstruct.remove_mask( 'badselected' ) histstruct.add_mask( 'badselected', histstruct.get_union_mask(['bad{}'.format(badrun) for badrun in badrunstokeep]) ) # training set trainset_args = {'masknames': training_masks} goodset_args = {'setnames': ['test_good_ext']} badset_args = {'setnames': ['test_bad{}_ext'.format(badrun) for badrun in badrunstokeep]} badset_parts_args = [] for badrun in badrunstokeep: badset_parts_args.append( {'setnames': ['test_bad{}_ext'.format(badrun)]} ) # get the log probability for good set prob_good = histstruct.get_globalscores( modelname, **goodset_args ) logprob_good = np.log(prob_good) # get the log probability for bad set prob_bad = histstruct.get_globalscores( modelname, **badset_args ) logprob_bad = np.log(prob_bad) print('--- good lumesections ---') print('length of log prob array: '+str(len(logprob_good))) print('minimum of log prob: '+str(np.min(logprob_good))) print('--- bad lumisections ---') print('length of log prob array: '+str(len(logprob_bad))) print('maximum of log prob: '+str(np.max(logprob_bad))) WARNING in HistStruct.remove_mask: name badselected is not in list of masks... --- good lumesections --- length of log prob array: 211171 minimum of log prob: -inf --- bad lumisections --- length of log prob array: 64814 maximum of log prob: 18.192293308278305 /tmp/ipykernel_27081/3315064921.py:33: RuntimeWarning: divide by zero encountered in log logprob_good = np.log(prob_good) /tmp/ipykernel_27081/3315064921.py:36: RuntimeWarning: divide by zero encountered in log logprob_bad = np.log(prob_bad) ### make 1D score (MSE) distributions for all histogram types for i,histname in enumerate(histstruct.histnames): fig,axs = plt.subplots(ncols=3, figsize=(15,4)) scores_train = histstruct.get_scores( modelname, histname=histname, **trainset_args ) scores_good = histstruct.get_scores( modelname, histname=histname, **goodset_args ) scores_bad = histstruct.get_scores( modelname, histname=histname, **badset_args ) # plot training and good set scores = np.concatenate((scores_train,scores_good)) labels = np.concatenate((np.ones(len(scores_train)),np.zeros(len(scores_good)))) pu.plot_score_dist( scores, labels, fig=fig, ax=axs[0], nbins=100, normalize=True, siglabel='Training set', sigcolor='b', bcklabel='Good test set', bckcolor='g', title=None, xaxtitle='Mean squared error', xaxtitlesize=14, yaxtitle='Number of lumisections (normalized)', yaxtitlesize=14, legendsize=15, doshow=False) pu.add_cms_label( axs[0], pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( axs[0], pu.make_text_latex_safe(histname), (0.95,0.65), fontsize=15, horizontalalignment='right' ) pu.add_text( axs[0], '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) # plot training and bad set scores = np.concatenate((scores_train,scores_bad)) labels = np.concatenate((np.ones(len(scores_train)),np.zeros(len(scores_bad)))) pu.plot_score_dist( scores, labels, fig=fig, ax=axs[1], nbins=100, normalize=True, siglabel='Training set', sigcolor='b', bcklabel='Bad test set', bckcolor='r', title=None, xaxtitle='Mean squared error', xaxtitlesize=14, yaxtitle='Number of lumisections (normalized)', yaxtitlesize=14, legendsize=15, doshow=False) pu.add_cms_label( axs[1], pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( axs[1], pu.make_text_latex_safe(histname), (0.95,0.65), fontsize=15, horizontalalignment='right' ) pu.add_text( axs[1], '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) # plot good and bad set scores = np.concatenate((scores_good,scores_bad)) labels = np.concatenate((np.ones(len(scores_good)),np.zeros(len(scores_bad)))) pu.plot_score_dist(scores, labels, fig=fig, ax=axs[2], nbins=100, normalize=True, siglabel='Good test set', sigcolor='g', bcklabel='Bad test set', bckcolor='r', title=None, xaxtitle='Mean squared error', xaxtitlesize=14, yaxtitle='Number of lumisections (normalized)', yaxtitlesize=14, legendsize=15, doshow=False) pu.add_cms_label( axs[2], pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( axs[2], pu.make_text_latex_safe(histname), (0.95,0.65), fontsize=15, horizontalalignment='right' ) pu.add_text( axs[2], '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) fig.subplots_adjust(wspace=0.3, hspace=0.3) ### make a new plot of probability contours and overlay data points doplot = True if doplot: # initializations plt.close('all') colormap = mpl.cm.get_cmap('Reds') colorlist = [colormap(i) for i in np.linspace(0.3,0.9,num=len(badrunstokeep))] if len(colorlist)<len(badrunstokeep): raise Exception('ERROR: need more colors...') clusters = [] labels = [] colors = [] # add good set clusters.append(goodset_args) colors.append('green') labels.append('Averages of training set') # add bad sets for j,run in enumerate(badrunstokeep): clusters.append(badset_parts_args[j]) labels.append('Run {}'.format(run)) colors.append(colorlist[j]) # make the plots for dims in dimslist: fig,ax = histstruct.plot_partial_fit( modelname, dims, clusters=clusters, colors=colors, labels=labels, logprob=True, clipprob=True, xlims=30, ylims=30, onlypositive=True, transparency=0.5, xaxtitle=pu.make_text_latex_safe(histstruct.histnames[dims[0]])+' MSE', xaxtitlesize=12, yaxtitle=pu.make_text_latex_safe(histstruct.histnames[dims[1]])+' MSE', yaxtitlesize=12, caxtitle='Logarithm of probability density', caxtitlesize=12) pu.add_cms_label( ax, pos=(0.05,0.9), extratext='Preliminary', fontsize=12, background_alpha=0.75 ) pu.add_text( ax, 'Density fit of lumisection MSE', (0.05,0.8), fontsize=12, background_alpha=0.75 ) pu.add_text( ax, '2017 (13 TeV)', (0.73,1.01), fontsize=12 ) #plt.close('all') # release plot memory /eos/home-l/llambrec/SWAN_projects/ML4DQM-DC/tutorials/../utils/plot_utils.py:904: RuntimeWarning: divide by zero encountered in log if logprob: evalpoints = np.log(evalpoints) NOTE: scores of -inf were reset to -745.4400719213812 ### make a roc curve based on the test results above labels_good = np.zeros(len(logprob_good)) # background: label = 0 labels_bad = np.ones(len(logprob_bad)) # signal: label = 1 labels = np.concatenate((labels_good,labels_bad)) scores = np.concatenate((-logprob_good,-logprob_bad)) scores = aeu.clip_scores( scores ) # plot score distribution fig,ax = pu.plot_score_dist(scores, labels, siglabel='Anomalous', sigcolor='r', bcklabel='Good', bckcolor='g', nbins=250, normalize=True, xaxtitle='Negative logarithmic probability', xaxtitlesize=15, yaxtitle='Number of lumisections (normalized)', yaxtitlesize=15, legendsize=15, doshow=False) ax.set_yscale('log') pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, 'Combined model output score', (0.1,0.65), fontsize=15 ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) plt.show() # plot ROC curve (auc,sigeff,bkgeff,sigeff_unc) = aeu.get_roc(scores, labels, mode='geom', doprint=False, bootstrap_samples=100, doplot=False, returneffs=True) fig,ax = pu.plot_roc( sigeff, bkgeff, sig_eff_unc=sigeff_unc, xaxtitle='False anomaly rate', xaxtitlesize=15, yaxtitle='True anomaly efficiency', yaxtitlesize=15, doshow=False ) auctext = '{:.3f}'.format(auc) if auc>0.99: auctext = '1 - '+'{:.3e}'.format(1-auc) pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) pu.add_text( ax, 'Combined model ROC curve', (0.95,0.25), fontsize=15, horizontalalignment='right' ) pu.add_text( ax, 'AUC: {}'.format(auctext), (0.95,0.15), fontsize=15, horizontalalignment='right' ) plt.show() #wp = aeu.get_confusion_matrix(scores, labels, wp='maxauc') #wp = aeu.get_confusion_matrix(scores, labels, wp=0) #print('working point: {}'.format(wp)) NOTE: scores of +inf were reset to 745.4400719213812 calculating ROC curve on 100 bootstrap samples of size 275985 ### investigate particular lumisections # initialization: general mode = 'ls' run = 299316 # for mode 'ls' (ignored if mode is 'run'): ls = 70 # for mode 'run' (ignored if mode is 'ls'): run_masknames = ['dcson','highstat'] # initialization: reference scores plot_refscores = True refscore_masknames = ['dcson','highstat'] # initialization: reference histograms refhists = {} for histname in histstruct.histnames: refhists[histname] = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['highstat','dcson']), 50 ) if mode=='ls': # plot this particular run/ls fig,axs = histstruct.plot_ls( run, ls, recohist=None, refhists=refhists, opaque_legend=True, titledict = titledict, titlesize=15, ncols = 3, physicalxax = True, xaxtitledict = xaxtitledict, xaxtitlesize=17, yaxtitledict = yaxtitledict, yaxtitlesize=17, ymaxfactor = 1.3, legendsize = 13 ) # stylistic modifications counter = -1 for i in range(axs.shape[0]): for j in range(axs.shape[1]): counter += 1 if counter>=len(histstruct.histnames): break ax = axs[i,j] pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, extratextdict[histstruct.histnames[counter]], (0.95,0.6), fontsize=15, horizontalalignment='right' ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) fig.subplots_adjust(wspace=0.3, hspace=0.3) plt.show() # only for moment classifier: do some prints #hist = histstruct.histograms['chargeInner_PXLayer_2'][histstruct.get_index(run,ls)] #histstruct.classifiers['chargeInner_PXLayer_2'].printout(hist) # print the mses msepoint = histstruct.get_scores_ls( modelname, run, ls ) logprob = np.log( histstruct.evaluate_fitter_on_point( modelname, msepoint ) ) print('-------------') print('MSE values:') for histname in histstruct.histnames: print('{} : {}'.format(histname,msepoint[histname])) print('-------------') print('logprob: '+str(logprob)) # plot mse distribution if plot_refscores: fig,axs = histstruct.plot_ls_score( modelname, run, ls, masknames=refscore_masknames, nbins=100, normalize=True, siglabel='This lumisection', bcklabel='All lumisections', sigcolor='k', bckcolor='b', title=None, xaxtitle='MSE', xaxtitlesize=15, yaxtitle='Normalized number of lumisections', yaxtitlesize=15, doshow=False) # stylistic modifications counter = -1 for i in range(axs.shape[0]): for j in range(axs.shape[1]): counter += 1 if counter>=len(histstruct.histnames): break ax = axs[i,j] pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, extratextdict[histstruct.histnames[counter]], (0.95,0.6), fontsize=15, horizontalalignment='right' ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) fig.subplots_adjust(wspace=0.3, hspace=0.3) if mode=='run': # plot given run runnbs = histstruct.get_runnbs( masknames=run_masknames ) lsnbs = histstruct.get_lsnbs( masknames=run_masknames ) runsel = np.where(runnbs==run) lsnbs = lsnbs[runsel] print('plotting {} lumisections...'.format(len(lsnbs))) for lsnb in lsnbs: fig,ax = histstruct.plot_ls(run, lsnb, recohist=None, refhists=refhists, opaque_legend=True ) plt.show() msepoint = histstruct.get_scores_ls( modelname, run, lsnb ) msepointarray = np.array([msepoint[histname] for histname in histstruct.histnames]) logprob = np.log( histstruct.evaluate_fitter_on_point( modelname, msepoint ) ) print('-------------') print('MSE values:') for histname in histstruct.histnames: print('{} : {}'.format(histname,msepoint[histname])) print('-------------') print('logprob: '+str(logprob)) ------------- MSE values: chargeInner_PXLayer_2 : [0.00027372] chargeInner_PXLayer_3 : [4.50905996e-07] ------------- logprob: 13.075970512248668 ### investigate how the method performs on the golden/custom test set # choose masks for evaluation set masks_eval = ['golden', 'highstat'] # set logprob boundaries logup = -100 logdown = None # set whether to do plotting doplot = True nplotsmax = 10 # set properties of file to save dosave = False pdfname = '' # get the lumisections within chosen logprob range score_up = np.exp(logup) if logup is not None else None score_down = np.exp(logdown) if logdown is not None else None runsls_eval = len(histstruct.get_runnbs(masknames=masks_eval)) runsls_in_range = histstruct.get_globalscores_runsls( modelname, masknames=masks_eval, score_up = score_up, score_down = score_down ) runnbs_in_range = runsls_in_range[0] lsnbs_in_range = runsls_in_range[1] print('{} out of {} LS are within these boundaries'.format(len(runnbs_in_range),runsls_eval)) if doplot: # define reference histograms refhists = {} for histname in histstruct.histnames: refhists[histname] = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['dcson','highstat'] ), 25 ) # make plots from matplotlib.backends.backend_pdf import PdfPages if dosave: pdf = PdfPages(pdfname) for i,(runnb,lsnb) in enumerate(zip(runnbs_in_range, lsnbs_in_range)): if i>=nplotsmax: print('maximum number of plots reached') break print('------------------------') # plot this particular run/ls fig,axs = histstruct.plot_ls( runnb, lsnb, recohist=None, refhists=refhists, opaque_legend=True, titledict = titledict, titlesize=15, ncols = 3, physicalxax = True, xaxtitledict = xaxtitledict, xaxtitlesize=17, yaxtitledict = yaxtitledict, yaxtitlesize=17, ymaxfactor = 1.3, legendsize = 13 ) # stylistic modifications counter = -1 for i in range(axs.shape[0]): for j in range(axs.shape[1]): counter += 1 if counter>=len(histstruct.histnames): break ax = axs[i,j] pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, extratextdict[histstruct.histnames[counter]], (0.95,0.6), fontsize=15, horizontalalignment='right' ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) fig.subplots_adjust(wspace=0.3, hspace=0.3) plt.show() if dosave: pdf.savefig(fig) msepoint = histstruct.get_scores_ls( modelname, runnb, lsnb ) msepointarray = np.array([msepoint[histname] for histname in histstruct.histnames]) logprob = np.log( histstruct.evaluate_fitter_on_point( modelname, msepoint ) ) print('-------------') print('MSE values:') for histname in histstruct.histnames: print('{} : {}'.format(histname,msepoint[histname])) print('-------------') print('logprob: '+str(logprob)) if dosave: pdf.close() 0 out of 201393 LS are within these boundaries","title":"global_combined_training"},{"location":"tutorials/global_combined_training/#train-a-combined-model-with-global-training-and-evaluate-its-predictions","text":"This notebook uses multiple types of histograms, trains a single-histogram classifier for each of them, combines the output and assesses the performance of this combined model. ### imports # external modules import os import sys import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt # local modules sys.path.append('../utils') import csv_utils as csvu import json_utils as jsonu import dataframe_utils as dfu import hist_utils as hu import autoencoder_utils as aeu import plot_utils as pu import generate_data_utils as gdu import refruns_utils as rru # import general source sys.path.append('../src') sys.path.append('../src/classifiers') sys.path.append('../src/cloudfitters') import Model import ModelInterface import HistStruct import DataLoader import PlotStyleParser # import classifiers import AutoEncoder import TemplateBasedClassifier import NMFClassifier import LandauClassifier import MomentClassifier # import combination methods import SeminormalFitter 2022-07-26 17:23:22.471603: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc8-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc8-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc8-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.30-e5b21/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc8-opt/lib64/R/library/readr/rcon 2022-07-26 17:23:22.471647: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. ### define titles and axis properties for figures plotstyleparser = PlotStyleParser.PlotStyleParser('plotstyle.json') titledict = plotstyleparser.get_title() xaxtitledict = plotstyleparser.get_xaxtitle() yaxtitledict = plotstyleparser.get_yaxtitle() for key in yaxtitledict.keys(): yaxtitledict[key] += ' (normalized)' # if normalized extratextdict = plotstyleparser.get_extratext() ### define run properties # in this cell all major run properties are going to be set, # e.g. what runs to train on and what runs to test on # define core test set of bad runs badrunsls = {'2017': { \"297287\":[[-1]], \"297288\":[[-1]], \"297289\":[[-1]], \"299316\":[[-1]], \"299317\":[[-1]], \"299318\":[[-1]], \"299324\":[[-1]], \"299325\":[[-1]], \"299326\":[[-1]], \"300373\":[[-1]], \"300374\":[[-1]], \"300397\":[[-1]], \"300398\":[[-1]] } } # set year to use year = '2017' # set histogram names to use histnames = ([ #'chargeInner_PXLayer_1', 'chargeInner_PXLayer_2', 'chargeInner_PXLayer_3', #'chargeInner_PXLayer_4', #'charge_PXDisk_+1','charge_PXDisk_+2','charge_PXDisk_+3', #'charge_PXDisk_-1','charge_PXDisk_-2','charge_PXDisk_-3', ]) # redefine badrunsls for this year only badrunsls = badrunsls[year] print('selected runs/lumisections for training: all') selected runs/lumisections for training: all ### read the data based on the configuration defined above readnew = True if readnew: ### add the histograms # initializations dloader = DataLoader.DataLoader() histstruct = HistStruct.HistStruct() # loop over the histogram types to take into account for histname in histnames: print('adding {}...'.format(histname)) # read the histograms from the correct csv files filename = '../data/DF'+year+'_'+histname+'.csv' df = dloader.get_dataframe_from_file( filename ) # define slice to remove under- and overflow bins cropslices = [slice(1,-1)] # add the dataframe to the histstruct histstruct.add_dataframe( df, cropslices=cropslices ) print('found {} histograms'.format(len(histstruct.runnbs))) ### add masks histstruct.add_dcsonjson_mask( 'dcson' ) histstruct.add_goldenjson_mask('golden' ) histstruct.add_highstat_mask( 'highstat' ) histstruct.add_stat_mask( 'lowstat', max_entries_to_bins_ratio=100 ) nbadruns = 0 histstruct.add_json_mask( 'bad', badrunsls ) # special case for bad runs: add a mask per run (different bad runs have different characteristics) nbadruns = len(badrunsls.keys()) for badrun in badrunsls.keys(): histstruct.add_json_mask( 'bad{}'.format(badrun), {badrun:badrunsls[badrun]} ) print('created a histstruct with the following properties:') print('- number of histogram types: {}'.format(len(histstruct.histnames))) print('- number of lumisections: {}'.format(len(histstruct.lsnbs))) print('- masks: {}'.format(list(histstruct.masks.keys()))) if not readnew: histstruct = HistStruct.HistStruct.load( 'histstruct_global_20220201.zip', verbose=False ) nbadruns = len([name for name in list(histstruct.masks.keys()) if ('bad' in name and name!='bad')]) print('loaded a histstruct with the following properties:') print(histstruct) adding chargeInner_PXLayer_2... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_2.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. adding chargeInner_PXLayer_3... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_3.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. found 225954 histograms created a histstruct with the following properties: - number of histogram types: 2 - number of lumisections: 225954 - masks: ['dcson', 'golden', 'highstat', 'lowstat', 'bad', 'bad297287', 'bad297288', 'bad297289', 'bad299316', 'bad299317', 'bad299318', 'bad299324', 'bad299325', 'bad299326', 'bad300373', 'bad300374', 'bad300397', 'bad300398'] ### plot the training and/or test sets doplot = True if doplot: index_mask = np.random.choice( np.arange(len(histstruct.get_lsnbs())), size=20, replace=False ) histstruct.add_index_mask( 'random_plotting', index_mask ) # bad test runs for badrun in badrunsls.keys(): fig,axs = histstruct.plot_histograms( masknames=[['dcson','highstat','random_plotting'],['dcson','highstat','bad{}'.format(badrun)]], labellist = ['Example histograms','Run {}'.format(badrun)], colorlist = ['blue','red'], transparencylist = [0.1,1.], ncols=3, opaque_legend=True, titledict = titledict, titlesize=15, physicalxax = True, xaxtitledict = xaxtitledict, xaxtitlesize=17, yaxtitledict = yaxtitledict, yaxtitlesize=17, ymaxfactor = 1.2, legendsize = 14 ) # stylistic modifications counter = -1 for i in range(axs.shape[0]): for j in range(axs.shape[1]): counter += 1 if counter>=len(histstruct.histnames): break ax = axs[i,j] pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, extratextdict[histstruct.histnames[counter]], (0.95,0.6), fontsize=15, horizontalalignment='right' ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) fig.subplots_adjust(wspace=0.3, hspace=0.3) histstruct.remove_mask('random_plotting') ### define a good test set as averages from training set npartitions = 50 for histname in histstruct.histnames: histograms = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['dcson','highstat']), npartitions ) histstruct.add_exthistograms( 'partitions', histname, histograms, overwrite=True ) ### make a plot of the good test set doplot = True if doplot: fig,axs = histstruct.plot_histograms( histograms=[histstruct.get_histograms(setnames=['partitions'])], labellist = ['Good test set'], colorlist = ['blue'], transparencylist = [0.1], ncols=3, opaque_legend=True, titledict = titledict, titlesize=15, physicalxax = True, xaxtitledict = xaxtitledict, xaxtitlesize=17, yaxtitledict = yaxtitledict, yaxtitlesize=17, #ymaxfactor = 1.2, legendsize = 14 ) # stylistic modifications counter = -1 for i in range(axs.shape[0]): for j in range(axs.shape[1]): counter += 1 if counter>=len(histstruct.histnames): break ax = axs[i,j] pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, extratextdict[histstruct.histnames[counter]], (0.95,0.6), fontsize=15, horizontalalignment='right' ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) fig.subplots_adjust(wspace=0.3, hspace=0.3) ### print some numbers print(len(histstruct.get_lsnbs())) print(len(histstruct.get_lsnbs(masknames=['dcson']))) print(len(histstruct.get_lsnbs(masknames=['dcson','highstat']))) trainingmasks = ['dcson','highstat'] print(len(histstruct.get_lsnbs(masknames=trainingmasks))) 225954 215144 211171 211171 ### define and train an autoencoder for each element classifymethod = 'nmf' modelname = classifymethod if modelname in histstruct.modelnames: raise Exception('WARNING: modelname \"{}\" is already present in histstruct.'.format(modelname) +' Choose a different name or remove the duplicate.') model = ModelInterface.ModelInterface(histstruct.histnames) classifiers = {} if classifymethod == 'autoencoder': for histname in histstruct.histnames: print('building Autoencoder model for histogram type {}'.format(histname)) hists = histstruct.get_histograms(histname=histname, masknames=['dcson','highstat']) print('size of training set: {}'.format(hists.shape)) # choose whether to save the model modelname = modelbasename+'_'+histname+'.h5' modelname = os.path.join(modelloc, modelname) if not save: modelname = '' # empty string means do not save models # train the model (aemodel, history) = aeu.train_simple_autoencoder(hists, nepochs=20, modelname=modelname, batch_size=2000, shuffle=True, returnhistory=True ) # make a loss plot fig,ax = pu.plot_loss(history, xaxtitlesize=15, yaxtitlesize=15, legendsize=15, doshow=False) pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) pu.add_text( ax, pu.make_text_latex_safe(histname), (0.95,0.6), fontsize=15, horizontalalignment='right' ) plt.show() # add the model to the histstruct classifiers[histname] = AutoEncoder.AutoEncoder( model=aemodel ) if classifymethod == 'templates': for histname in histstruct.histnames: hists = histstruct.get_histograms(histname=histname, masknames=['dcson','highstat']) hists = hu.averagehists( hists, 25 ) classifier = TemplateBasedClassifier.TemplateBasedClassifier() classifier.train( hists ) classifiers[histname] = classifier if classifymethod == 'nmf': for histname in histstruct.histnames: print('building NMF model for histogram type {}'.format(histname)) hists = histstruct.get_histograms( histname=histname, masknames=['dcson','highstat']) # option: reduce size of training set by choosing randoms random_indices = np.random.choice(len(hists), size=int(5e4), replace=False) hists = hists[random_indices] print('size of training set: {}'.format(hists.shape)) classifier = NMFClassifier.NMFClassifier( ncomponents=3, nmax=10 ) classifier.train( hists ) classifiers[histname] = classifier if classifymethod == 'landaufit': print('initializing Landau fit classifier') classifier = LandauClassifier.LandauClassifier( dogauss=True ) for histname in histstruct.histnames: classifiers[histname] = classifier if classifymethod == 'moments': for histname in histstruct.histnames: print('calculating moments for histogram type {}'.format(histname)) hists = histstruct.get_histograms(histname=histname, masknames=['dcson','highstat']) classifier = MomentClassifier.MomentClassifier( orders=[1,2] ) classifier.train(hists) classifiers[histname] = classifier model.set_classifiers(classifiers) histstruct.add_model( modelname, model ) building NMF model for histogram type chargeInner_PXLayer_2 size of training set: (50000, 100) /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26). warnings.warn((\"The 'init' value, when 'init=None' and \" /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence. warnings.warn(\"Maximum number of iterations %d reached. Increase it to\" building NMF model for histogram type chargeInner_PXLayer_3 size of training set: (50000, 100) /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26). warnings.warn((\"The 'init' value, when 'init=None' and \" /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence. warnings.warn(\"Maximum number of iterations %d reached. Increase it to\" adding model \"nmf\" to the HistStruct ### evaluate the models on all histograms in the (non-extended) histstruct and on the partitions # evaluate classifiers print('evaluating classifiers') histstruct.evaluate_classifiers( modelname ) histstruct.evaluate_classifiers( modelname, setnames=['partitions'] ) evaluating classifiers ### plot the multidemensional mse and fit a distribution # set method and other properties doplot = True # initializations dimslist = [] nhisttypes = len(histstruct.histnames) for i in range(0,nhisttypes-1): for j in range(i+1,nhisttypes): dimslist.append((i,j)) plt.close('all') # define training masks training_masks = ['dcson','highstat'] # train the actual fitter histstruct.set_fitter( modelname, SeminormalFitter.SeminormalFitter() ) histstruct.train_fitter( modelname, masknames=training_masks ) histstruct.evaluate_fitter( modelname ) # train and plot the partial fitters if doplot: histstruct.train_partial_fitters( modelname, dimslist, masknames=training_masks ) for dims in dimslist: fig,ax = histstruct.plot_partial_fit( modelname, dims, clusters=[{'masknames':training_masks}], colors=['b'], labels=['Training lumisections'], logprob=True, clipprob=True, xlims=30, ylims=30, onlypositive=True, transparency=0.5, xaxtitle=pu.make_text_latex_safe(histstruct.histnames[dims[0]])+' MSE', xaxtitlesize=12, yaxtitle=pu.make_text_latex_safe(histstruct.histnames[dims[1]])+' MSE', yaxtitlesize=12, caxtitle='Logarithm of probability density', caxtitlesize=12) pu.add_cms_label( ax, pos=(0.05,0.9), extratext='Preliminary', fontsize=12, background_alpha=0.75 ) pu.add_text( ax, 'Density fit of lumisection MSE', (0.05,0.8), fontsize=12, background_alpha=0.75 ) pu.add_text( ax, '2017 (13 TeV)', (0.73,1.01), fontsize=12 ) #plt.close('all') # release plot memory /eos/home-l/llambrec/SWAN_projects/ML4DQM-DC/tutorials/../utils/plot_utils.py:904: RuntimeWarning: divide by zero encountered in log if logprob: evalpoints = np.log(evalpoints) NOTE: scores of -inf were reset to -745.4400719213812 ### extend the test set using artificial data generation and evaluate the model on the extended test set # make the extra data trainingmasknames = ['dcson','highstat'] for histname in histstruct.histnames: print('generating data for '+histname) # good data option 1: resample partitions #goodhists = histstruct.get_histograms( setnames=['partitions'], histname=histname ) #(goodexthists,_,_) = gdu.upsample_hist_set( goodhists, ntarget=nbadruns*5e3, fourierstdfactor=20., doplot=False) # good data option 2: use training set goodexthists = histstruct.get_histograms( histname=histname, masknames=trainingmasknames ) histstruct.add_exthistograms( 'test_good_ext', histname, goodexthists, overwrite=True ) for badrun in badrunsls.keys(): badhists = histstruct.get_histograms( histname=histname,masknames=['dcson','highstat','bad{}'.format(badrun)] ) (badexthists,_,_) = gdu.upsample_hist_set( badhists, ntarget=5e3, fourierstdfactor=20., doplot=False) histstruct.add_exthistograms( 'test_bad{}_ext'.format(badrun), histname, badexthists, overwrite=True ) # evaluate the classifiers print('evaluating classifiers') histstruct.evaluate_classifiers( modelname, setnames=['test_good_ext'] ) # shortcut if using training set as good test set: copy scores #histstruct.models[modelname].scores['test_good_ext'] = histstruct.get_scores( modelname, masknames=trainingmasknames ) for badrun in badrunsls.keys(): histstruct.evaluate_classifiers( modelname, setnames=['test_bad{}_ext'.format(badrun)]) # evaluate the fitter print('evaluating fitter') histstruct.evaluate_fitter( modelname, setnames=['test_good_ext'] ) for badrun in badrunsls.keys(): histstruct.evaluate_fitter( modelname, setnames=['test_bad{}_ext'.format(badrun)] ) generating data for chargeInner_PXLayer_2 generating data for chargeInner_PXLayer_3 evaluating classifiers evaluating fitter ### (re-)define the test set badrunstokeep = [ \"297287\", \"297288\", \"297289\", \"299316\", \"299317\", \"299318\", \"299324\", \"299325\", \"299326\", \"300373\", \"300374\", \"300397\", \"300398\" ] # make a new mask that is the union of the selected bad runs histstruct.remove_mask( 'badselected' ) histstruct.add_mask( 'badselected', histstruct.get_union_mask(['bad{}'.format(badrun) for badrun in badrunstokeep]) ) # training set trainset_args = {'masknames': training_masks} goodset_args = {'setnames': ['test_good_ext']} badset_args = {'setnames': ['test_bad{}_ext'.format(badrun) for badrun in badrunstokeep]} badset_parts_args = [] for badrun in badrunstokeep: badset_parts_args.append( {'setnames': ['test_bad{}_ext'.format(badrun)]} ) # get the log probability for good set prob_good = histstruct.get_globalscores( modelname, **goodset_args ) logprob_good = np.log(prob_good) # get the log probability for bad set prob_bad = histstruct.get_globalscores( modelname, **badset_args ) logprob_bad = np.log(prob_bad) print('--- good lumesections ---') print('length of log prob array: '+str(len(logprob_good))) print('minimum of log prob: '+str(np.min(logprob_good))) print('--- bad lumisections ---') print('length of log prob array: '+str(len(logprob_bad))) print('maximum of log prob: '+str(np.max(logprob_bad))) WARNING in HistStruct.remove_mask: name badselected is not in list of masks... --- good lumesections --- length of log prob array: 211171 minimum of log prob: -inf --- bad lumisections --- length of log prob array: 64814 maximum of log prob: 18.192293308278305 /tmp/ipykernel_27081/3315064921.py:33: RuntimeWarning: divide by zero encountered in log logprob_good = np.log(prob_good) /tmp/ipykernel_27081/3315064921.py:36: RuntimeWarning: divide by zero encountered in log logprob_bad = np.log(prob_bad) ### make 1D score (MSE) distributions for all histogram types for i,histname in enumerate(histstruct.histnames): fig,axs = plt.subplots(ncols=3, figsize=(15,4)) scores_train = histstruct.get_scores( modelname, histname=histname, **trainset_args ) scores_good = histstruct.get_scores( modelname, histname=histname, **goodset_args ) scores_bad = histstruct.get_scores( modelname, histname=histname, **badset_args ) # plot training and good set scores = np.concatenate((scores_train,scores_good)) labels = np.concatenate((np.ones(len(scores_train)),np.zeros(len(scores_good)))) pu.plot_score_dist( scores, labels, fig=fig, ax=axs[0], nbins=100, normalize=True, siglabel='Training set', sigcolor='b', bcklabel='Good test set', bckcolor='g', title=None, xaxtitle='Mean squared error', xaxtitlesize=14, yaxtitle='Number of lumisections (normalized)', yaxtitlesize=14, legendsize=15, doshow=False) pu.add_cms_label( axs[0], pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( axs[0], pu.make_text_latex_safe(histname), (0.95,0.65), fontsize=15, horizontalalignment='right' ) pu.add_text( axs[0], '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) # plot training and bad set scores = np.concatenate((scores_train,scores_bad)) labels = np.concatenate((np.ones(len(scores_train)),np.zeros(len(scores_bad)))) pu.plot_score_dist( scores, labels, fig=fig, ax=axs[1], nbins=100, normalize=True, siglabel='Training set', sigcolor='b', bcklabel='Bad test set', bckcolor='r', title=None, xaxtitle='Mean squared error', xaxtitlesize=14, yaxtitle='Number of lumisections (normalized)', yaxtitlesize=14, legendsize=15, doshow=False) pu.add_cms_label( axs[1], pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( axs[1], pu.make_text_latex_safe(histname), (0.95,0.65), fontsize=15, horizontalalignment='right' ) pu.add_text( axs[1], '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) # plot good and bad set scores = np.concatenate((scores_good,scores_bad)) labels = np.concatenate((np.ones(len(scores_good)),np.zeros(len(scores_bad)))) pu.plot_score_dist(scores, labels, fig=fig, ax=axs[2], nbins=100, normalize=True, siglabel='Good test set', sigcolor='g', bcklabel='Bad test set', bckcolor='r', title=None, xaxtitle='Mean squared error', xaxtitlesize=14, yaxtitle='Number of lumisections (normalized)', yaxtitlesize=14, legendsize=15, doshow=False) pu.add_cms_label( axs[2], pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( axs[2], pu.make_text_latex_safe(histname), (0.95,0.65), fontsize=15, horizontalalignment='right' ) pu.add_text( axs[2], '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) fig.subplots_adjust(wspace=0.3, hspace=0.3) ### make a new plot of probability contours and overlay data points doplot = True if doplot: # initializations plt.close('all') colormap = mpl.cm.get_cmap('Reds') colorlist = [colormap(i) for i in np.linspace(0.3,0.9,num=len(badrunstokeep))] if len(colorlist)<len(badrunstokeep): raise Exception('ERROR: need more colors...') clusters = [] labels = [] colors = [] # add good set clusters.append(goodset_args) colors.append('green') labels.append('Averages of training set') # add bad sets for j,run in enumerate(badrunstokeep): clusters.append(badset_parts_args[j]) labels.append('Run {}'.format(run)) colors.append(colorlist[j]) # make the plots for dims in dimslist: fig,ax = histstruct.plot_partial_fit( modelname, dims, clusters=clusters, colors=colors, labels=labels, logprob=True, clipprob=True, xlims=30, ylims=30, onlypositive=True, transparency=0.5, xaxtitle=pu.make_text_latex_safe(histstruct.histnames[dims[0]])+' MSE', xaxtitlesize=12, yaxtitle=pu.make_text_latex_safe(histstruct.histnames[dims[1]])+' MSE', yaxtitlesize=12, caxtitle='Logarithm of probability density', caxtitlesize=12) pu.add_cms_label( ax, pos=(0.05,0.9), extratext='Preliminary', fontsize=12, background_alpha=0.75 ) pu.add_text( ax, 'Density fit of lumisection MSE', (0.05,0.8), fontsize=12, background_alpha=0.75 ) pu.add_text( ax, '2017 (13 TeV)', (0.73,1.01), fontsize=12 ) #plt.close('all') # release plot memory /eos/home-l/llambrec/SWAN_projects/ML4DQM-DC/tutorials/../utils/plot_utils.py:904: RuntimeWarning: divide by zero encountered in log if logprob: evalpoints = np.log(evalpoints) NOTE: scores of -inf were reset to -745.4400719213812 ### make a roc curve based on the test results above labels_good = np.zeros(len(logprob_good)) # background: label = 0 labels_bad = np.ones(len(logprob_bad)) # signal: label = 1 labels = np.concatenate((labels_good,labels_bad)) scores = np.concatenate((-logprob_good,-logprob_bad)) scores = aeu.clip_scores( scores ) # plot score distribution fig,ax = pu.plot_score_dist(scores, labels, siglabel='Anomalous', sigcolor='r', bcklabel='Good', bckcolor='g', nbins=250, normalize=True, xaxtitle='Negative logarithmic probability', xaxtitlesize=15, yaxtitle='Number of lumisections (normalized)', yaxtitlesize=15, legendsize=15, doshow=False) ax.set_yscale('log') pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, 'Combined model output score', (0.1,0.65), fontsize=15 ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) plt.show() # plot ROC curve (auc,sigeff,bkgeff,sigeff_unc) = aeu.get_roc(scores, labels, mode='geom', doprint=False, bootstrap_samples=100, doplot=False, returneffs=True) fig,ax = pu.plot_roc( sigeff, bkgeff, sig_eff_unc=sigeff_unc, xaxtitle='False anomaly rate', xaxtitlesize=15, yaxtitle='True anomaly efficiency', yaxtitlesize=15, doshow=False ) auctext = '{:.3f}'.format(auc) if auc>0.99: auctext = '1 - '+'{:.3e}'.format(1-auc) pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) pu.add_text( ax, 'Combined model ROC curve', (0.95,0.25), fontsize=15, horizontalalignment='right' ) pu.add_text( ax, 'AUC: {}'.format(auctext), (0.95,0.15), fontsize=15, horizontalalignment='right' ) plt.show() #wp = aeu.get_confusion_matrix(scores, labels, wp='maxauc') #wp = aeu.get_confusion_matrix(scores, labels, wp=0) #print('working point: {}'.format(wp)) NOTE: scores of +inf were reset to 745.4400719213812 calculating ROC curve on 100 bootstrap samples of size 275985 ### investigate particular lumisections # initialization: general mode = 'ls' run = 299316 # for mode 'ls' (ignored if mode is 'run'): ls = 70 # for mode 'run' (ignored if mode is 'ls'): run_masknames = ['dcson','highstat'] # initialization: reference scores plot_refscores = True refscore_masknames = ['dcson','highstat'] # initialization: reference histograms refhists = {} for histname in histstruct.histnames: refhists[histname] = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['highstat','dcson']), 50 ) if mode=='ls': # plot this particular run/ls fig,axs = histstruct.plot_ls( run, ls, recohist=None, refhists=refhists, opaque_legend=True, titledict = titledict, titlesize=15, ncols = 3, physicalxax = True, xaxtitledict = xaxtitledict, xaxtitlesize=17, yaxtitledict = yaxtitledict, yaxtitlesize=17, ymaxfactor = 1.3, legendsize = 13 ) # stylistic modifications counter = -1 for i in range(axs.shape[0]): for j in range(axs.shape[1]): counter += 1 if counter>=len(histstruct.histnames): break ax = axs[i,j] pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, extratextdict[histstruct.histnames[counter]], (0.95,0.6), fontsize=15, horizontalalignment='right' ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) fig.subplots_adjust(wspace=0.3, hspace=0.3) plt.show() # only for moment classifier: do some prints #hist = histstruct.histograms['chargeInner_PXLayer_2'][histstruct.get_index(run,ls)] #histstruct.classifiers['chargeInner_PXLayer_2'].printout(hist) # print the mses msepoint = histstruct.get_scores_ls( modelname, run, ls ) logprob = np.log( histstruct.evaluate_fitter_on_point( modelname, msepoint ) ) print('-------------') print('MSE values:') for histname in histstruct.histnames: print('{} : {}'.format(histname,msepoint[histname])) print('-------------') print('logprob: '+str(logprob)) # plot mse distribution if plot_refscores: fig,axs = histstruct.plot_ls_score( modelname, run, ls, masknames=refscore_masknames, nbins=100, normalize=True, siglabel='This lumisection', bcklabel='All lumisections', sigcolor='k', bckcolor='b', title=None, xaxtitle='MSE', xaxtitlesize=15, yaxtitle='Normalized number of lumisections', yaxtitlesize=15, doshow=False) # stylistic modifications counter = -1 for i in range(axs.shape[0]): for j in range(axs.shape[1]): counter += 1 if counter>=len(histstruct.histnames): break ax = axs[i,j] pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, extratextdict[histstruct.histnames[counter]], (0.95,0.6), fontsize=15, horizontalalignment='right' ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) fig.subplots_adjust(wspace=0.3, hspace=0.3) if mode=='run': # plot given run runnbs = histstruct.get_runnbs( masknames=run_masknames ) lsnbs = histstruct.get_lsnbs( masknames=run_masknames ) runsel = np.where(runnbs==run) lsnbs = lsnbs[runsel] print('plotting {} lumisections...'.format(len(lsnbs))) for lsnb in lsnbs: fig,ax = histstruct.plot_ls(run, lsnb, recohist=None, refhists=refhists, opaque_legend=True ) plt.show() msepoint = histstruct.get_scores_ls( modelname, run, lsnb ) msepointarray = np.array([msepoint[histname] for histname in histstruct.histnames]) logprob = np.log( histstruct.evaluate_fitter_on_point( modelname, msepoint ) ) print('-------------') print('MSE values:') for histname in histstruct.histnames: print('{} : {}'.format(histname,msepoint[histname])) print('-------------') print('logprob: '+str(logprob)) ------------- MSE values: chargeInner_PXLayer_2 : [0.00027372] chargeInner_PXLayer_3 : [4.50905996e-07] ------------- logprob: 13.075970512248668 ### investigate how the method performs on the golden/custom test set # choose masks for evaluation set masks_eval = ['golden', 'highstat'] # set logprob boundaries logup = -100 logdown = None # set whether to do plotting doplot = True nplotsmax = 10 # set properties of file to save dosave = False pdfname = '' # get the lumisections within chosen logprob range score_up = np.exp(logup) if logup is not None else None score_down = np.exp(logdown) if logdown is not None else None runsls_eval = len(histstruct.get_runnbs(masknames=masks_eval)) runsls_in_range = histstruct.get_globalscores_runsls( modelname, masknames=masks_eval, score_up = score_up, score_down = score_down ) runnbs_in_range = runsls_in_range[0] lsnbs_in_range = runsls_in_range[1] print('{} out of {} LS are within these boundaries'.format(len(runnbs_in_range),runsls_eval)) if doplot: # define reference histograms refhists = {} for histname in histstruct.histnames: refhists[histname] = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['dcson','highstat'] ), 25 ) # make plots from matplotlib.backends.backend_pdf import PdfPages if dosave: pdf = PdfPages(pdfname) for i,(runnb,lsnb) in enumerate(zip(runnbs_in_range, lsnbs_in_range)): if i>=nplotsmax: print('maximum number of plots reached') break print('------------------------') # plot this particular run/ls fig,axs = histstruct.plot_ls( runnb, lsnb, recohist=None, refhists=refhists, opaque_legend=True, titledict = titledict, titlesize=15, ncols = 3, physicalxax = True, xaxtitledict = xaxtitledict, xaxtitlesize=17, yaxtitledict = yaxtitledict, yaxtitlesize=17, ymaxfactor = 1.3, legendsize = 13 ) # stylistic modifications counter = -1 for i in range(axs.shape[0]): for j in range(axs.shape[1]): counter += 1 if counter>=len(histstruct.histnames): break ax = axs[i,j] pu.add_cms_label( ax, pos=(0.02,1.01), extratext='Preliminary', fontsize=16 ) pu.add_text( ax, extratextdict[histstruct.histnames[counter]], (0.95,0.6), fontsize=15, horizontalalignment='right' ) pu.add_text( ax, '2017 (13 TeV)', (1,1.01), fontsize=14, horizontalalignment='right' ) fig.subplots_adjust(wspace=0.3, hspace=0.3) plt.show() if dosave: pdf.savefig(fig) msepoint = histstruct.get_scores_ls( modelname, runnb, lsnb ) msepointarray = np.array([msepoint[histname] for histname in histstruct.histnames]) logprob = np.log( histstruct.evaluate_fitter_on_point( modelname, msepoint ) ) print('-------------') print('MSE values:') for histname in histstruct.histnames: print('{} : {}'.format(histname,msepoint[histname])) print('-------------') print('logprob: '+str(logprob)) if dosave: pdf.close() 0 out of 201393 LS are within these boundaries","title":"Train a combined model with global training and evaluate its predictions"},{"location":"tutorials/maxpull/","text":"Two-dimensional histogram classification based on the max bin-per-bin pull with respect to a reference histogram This notebook investigates a very simple classifier, that just looks at the maximum bin-per-bin value difference with respect to a given reference histogram. ### imports # external modules import sys import os import numpy as np import matplotlib.pyplot as plt import importlib # local modules sys.path.append('../utils') import dataframe_utils as dfu import plot_utils as pu import hist_utils as hu importlib.reload(dfu) importlib.reload(pu) importlib.reload(hu) sys.path.append('../src') import DataLoader importlib.reload(DataLoader) sys.path.append('../src/classifiers') import MaxPullClassifier importlib.reload(MaxPullClassifier) <module 'MaxPullClassifier' from '/eos/home-l/llambrec/SWAN_projects/ML4DQM-DC/tutorials/../src/classifiers/MaxPullClassifier.py'> Part 1: First exploration on a small test file ### load the histogram dataframe histname = 'clusterposition_zphi_ontrack_PXLayer_1' filename = 'DF2017B_'+histname+'_subset.csv' datadir = '../data' dloader = DataLoader.DataLoader() df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) # select DCS-bit on data df = dfu.select_dcson(df) print('number of selected lumisections: '+str(len(df))) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017B_clusterposition_zphi_ontrack_PXLayer_1_subset.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 627 rows and 13 columns. raw input data shape: (627, 202, 302) number of selected lumisections: 599 ### extract the histograms as a numpy array from the dataframe (histograms,runnbs,lsnbs) = dfu.get_hist_values(df) print('shape of histogram array: {}'.format(histograms.shape)) print('shape of run number array: {}'.format(runnbs.shape)) print('shape of lumisection number array: {}'.format(lsnbs.shape)) ### further preprocessing of the data (cropping, rebinning, normalizing) histograms = hu.crophists(histograms,[slice(1,-1,None),slice(1,-1,None)]) # remove under- and overflow bins histograms = hu.crophists(histograms,[slice(None,None,None),slice(80,220,None)]) # cut out uninteresting parts histograms = hu.rebinhists(histograms,(5,5)) print('shape of histogram array: {}'.format(histograms.shape)) shape of histogram array: (599, 202, 302) shape of run number array: (599,) shape of lumisection number array: (599,) shape of histogram array: (599, 40, 28) ### define a reference histogram as the average of the set ### and a max pull classifier based on this reference histogram refhist = hu.averagehists( histograms, nout=1 )[0,:,:] print('shape of averaged histogram: {}'.format(refhist.shape)) classifier = MaxPullClassifier.MaxPullClassifier() classifier.train( np.array([refhist]) ) shape of averaged histogram: (40, 28) ### calculate max pull for each histogram maxpulls = classifier.evaluate( histograms ) pu.plot_distance(maxpulls) avg,std = pu.plot_distance(maxpulls,doplot=False) print(avg) print(std) 127.49942708389064 17.68730286891456 ### plot examples for histograms with large pulls threshold = avg+3*std for i in range(len(histograms)): if maxpulls[i] < threshold: continue histlist = [histograms[i],classifier.refhist,classifier.getpull(histograms[i])] subtitles = ['test histogram','reference histogram','pull'] pu.plot_hists_2d(histlist, ncols=3, title = None, subtitles=subtitles) ### plot examples of histograms with small pulls inds = np.argsort(maxpulls)[:3] for i in inds: histlist = [histograms[i],classifier.refhist,classifier.getpull(histograms[i])] subtitles = ['test histogram','reference histogram','pull'] pu.plot_hists_2d(histlist, ncols=3, title = None, subtitles=subtitles) ### investigate a particular lumisection in more detail idx = 0 print('run: {}, lumisection: {}'.format(runnbs[idx],lsnbs[idx])) print('maximum pull for this lumisection: {}'.format(maxpulls[idx])) histlist = [histograms[idx],classifier.refhist,classifier.getpull(histograms[idx])] subtitles = ['test histogram','reference histogram','pull'] _ = pu.plot_hists_2d( histlist, ncols=3, title = None, subtitles=subtitles) run: 297050, lumisection: 1 maximum pull for this lumisection: 301.22227457594295 Part 2: Use a locally changing reference histogram ### local approach nprev = 5 maxpulls = np.zeros(len(histograms)) classifiers = [None]*nprev for i in range(nprev,len(histograms)): hist = histograms[i] refhist = hu.averagehists( histograms[i-nprev:i], nout=1 )[0,:,:] classifier = MaxPullClassifier.MaxPullClassifier() classifier.train( np.array([refhist]) ) classifiers.append(classifier) maxpulls[i] = classifier.evaluate( np.array([hist]) ) pu.plot_distance(maxpulls) avg,std = pu.plot_distance(maxpulls,doplot=False) plt.show() print('average pull: {}'.format(avg)) print('std dev of pulls: {}'.format(std)) # make plots of histograms with large local pulls threshold = avg+3*std print('histograms with largest pulls:') for i in range(len(histograms)): if maxpulls[i] < threshold: continue histlist = [histograms[i],classifiers[i].refhist,classifiers[i].getpull(histograms[i])] subtitles = ['test histogram','reference histogram','pull'] title = 'index: {}, run: {}, lumisection: {}, max pull: {}'.format(i, runnbs[i],lsnbs[i],maxpulls[i]) pu.plot_hists_2d(histlist, ncols=3, title = title, subtitles=subtitles) plt.show() # make plots of histograms with small local pulls inds = np.argsort(maxpulls)[nprev:nprev+5] print(inds) print('histograms with smalles pulls:') for i in inds: histlist = [histograms[i],classifiers[i].refhist,classifiers[i].getpull(histograms[i])] subtitles = ['test histogram','reference histogram','pull'] title = 'index: {}, run: {}, lumisection: {}, max pull: {}'.format(i, runnbs[i],lsnbs[i],maxpulls[i]) pu.plot_hists_2d(histlist, ncols=3, title = title, subtitles=subtitles) plt.show() average pull: 60.82421471381158 std dev of pulls: 100.6325204200988 histograms with largest pulls: [232 482 233 591 126] histograms with smalles pulls: ### investigate a particular lumisection in more detail idx = 136 if idx < nprev: raise Exception('ERROR: cannot plot index {} since classification is done based on {} previous lumisections'.format(idx,nprev)) print('run: {}, lumisection: {}'.format(runnbs[idx],lsnbs[idx])) print('maximum pull for this lumisection: {}'.format(maxpulls[idx])) histlist = [histograms[idx],classifiers[idx].refhist,classifiers[idx].getpull(histograms[idx])] subtitles = ['test histogram','reference histogram','pull'] print('test histogram, reference histogram and pulls:') _ = pu.plot_hists_2d( histlist, ncols=3, title = None, subtitles=subtitles) plt.show() print('histograms that were averaged to make the reference histograms:') _ = pu.plot_hists_2d( histograms[idx-nprev:idx], ncols=3 ) run: 297056, lumisection: 17 maximum pull for this lumisection: 660.5883469264701 test histogram, reference histogram and pulls: histograms that were averaged to make the reference histograms: Part 3: Load a set of histograms to define a reference, a good test set and a bad test set, and test the discrimination ### load the histograms histname = 'clusterposition_zphi_ontrack_PXLayer_1' datadir = '../data' dloader = DataLoader.DataLoader() # load the training data and train the classifier filename = dffile = 'DF2017B_'+histname+'_subset.csv' df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) df = dfu.select_dcson(df) (hists_ref) = hu.preparedatafromdf(df, cropslices=[slice(1,-1,None),slice(81,221,None)], rebinningfactor=(2,2), donormalize=False, doplot=False) _ = pu.plot_hists_2d(hists_ref[:4], ncols=4, title='some example histograms for averaging') print('number of lumisections in histogram set for averaging: '+str(len(df))) refhist = hu.averagehists( hists_ref, nout=1 )[0,:,:] _ = pu.plot_hist_2d(refhist, title='averaged histogram (used as reference)') print('shape of averaged histogram: {}'.format(refhist.shape)) classifier = MaxPullClassifier.MaxPullClassifier() classifier.train( np.array([refhist]) ) # load the good data filename = dffile = 'DF2017B_'+histname+'_run297056.csv' df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) df = dfu.select_dcson(df) (hists_good, runnbs_good, lsnbs_good) = hu.preparedatafromdf(df, returnrunls=True, cropslices=[slice(1,-1,None),slice(81,221,None)], rebinningfactor=(2,2), donormalize=False, doplot=False) _ = pu.plot_hists_2d(hists_good[:4], ncols=4, title='some example histograms in good test set') print('number of lumisections in good test set: '+str(len(df))) # load the bad data filename = dffile = 'DF2017B_'+histname+'_run297289.csv' df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) df = dfu.select_dcson(df) (hists_bad, runnbs_bad, lsnbs_bad) = hu.preparedatafromdf(df, returnrunls=True, cropslices=[slice(1,-1,None),slice(81,221,None)], rebinningfactor=(2,2), donormalize=False, doplot=False) _ = pu.plot_hists_2d(hists_bad[:4], ncols=4, title='some example histograms in bad test set') print('number of lumisections in bad test set: '+str(len(df))) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017B_clusterposition_zphi_ontrack_PXLayer_1_subset.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 627 rows and 13 columns. raw input data shape: (627, 202, 302) number of lumisections in histogram set for averaging: 599 shape of averaged histogram: (100, 70) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017B_clusterposition_zphi_ontrack_PXLayer_1_run297056.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 196 rows and 14 columns. raw input data shape: (196, 202, 302) number of lumisections in good test set: 185 INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017B_clusterposition_zphi_ontrack_PXLayer_1_run297289.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 15 rows and 14 columns. raw input data shape: (15, 202, 302) number of lumisections in bad test set: 15 ### perform the classification scores_good = classifier.evaluate( hists_good ) labels_good = np.zeros(len(scores_good)) scores_bad = classifier.evaluate( hists_bad ) labels_bad = np.ones(len(scores_bad)) scores = np.concatenate((scores_good,scores_bad)) labels = np.concatenate((labels_good,labels_bad)) _ = pu.plot_score_dist( scores, labels, nbins=50, normalize=True, siglabel='Anomalies', sigcolor='r', bcklabel='Good histograms', bckcolor='g', title='output score distributions for signal and background', xaxtitle='output score', yaxtitle=None) ### check some examples nplot = 5 inds_good = np.random.choice(np.array(list(range(len(hists_good)))),size=nplot) print('example histograms from good test set:') for i in inds_good: histlist = [hists_good[i],classifier.refhist,classifier.getpull(hists_good[i])] subtitles = ['good test histogram','reference histogram','pull'] title = 'index: {}, run: {}, lumisection: {}, max pull: {}'.format(i, runnbs_good[i],lsnbs_good[i],scores_good[i]) pu.plot_hists_2d(histlist, ncols=3, title = title, subtitles=subtitles) plt.show() inds_bad = np.random.choice(np.array(range(len(hists_bad))),size=nplot) print('example histograms from bad test set:') for i in inds_bad: histlist = [hists_bad[i],classifier.refhist,classifier.getpull(hists_bad[i])] subtitles = ['bad test histogram','reference histogram','pull'] title = 'index: {}, run: {}, lumisection: {}, max pull: {}'.format(i, runnbs_bad[i],lsnbs_bad[i],scores_bad[i]) pu.plot_hists_2d(histlist, ncols=3, title = title, subtitles=subtitles) plt.show() example histograms from good test set: example histograms from bad test set: ### re-define the classifier with a nondefault number of maximum pull bins to consider in a loop to determine the optimal value ns = [1,10,20,50,100,500] for n in ns: classifier.set_nmaxpulls( n ) scores_good = classifier.evaluate( hists_good ) labels_good = np.zeros(len(scores_good)) scores_bad = classifier.evaluate( hists_bad ) labels_bad = np.ones(len(scores_bad)) scores = np.concatenate((scores_good,scores_bad)) labels = np.concatenate((labels_good,labels_bad)) _ = pu.plot_score_dist( scores, labels, nbins=50, normalize=True, siglabel='Anomalies', sigcolor='r', bcklabel='Good histograms', bckcolor='g', title='output score distributions for signal and background', xaxtitle='output score', yaxtitle=None)","title":"maxpull"},{"location":"tutorials/maxpull/#two-dimensional-histogram-classification-based-on-the-max-bin-per-bin-pull-with-respect-to-a-reference-histogram","text":"This notebook investigates a very simple classifier, that just looks at the maximum bin-per-bin value difference with respect to a given reference histogram. ### imports # external modules import sys import os import numpy as np import matplotlib.pyplot as plt import importlib # local modules sys.path.append('../utils') import dataframe_utils as dfu import plot_utils as pu import hist_utils as hu importlib.reload(dfu) importlib.reload(pu) importlib.reload(hu) sys.path.append('../src') import DataLoader importlib.reload(DataLoader) sys.path.append('../src/classifiers') import MaxPullClassifier importlib.reload(MaxPullClassifier) <module 'MaxPullClassifier' from '/eos/home-l/llambrec/SWAN_projects/ML4DQM-DC/tutorials/../src/classifiers/MaxPullClassifier.py'>","title":"Two-dimensional histogram classification based on the max bin-per-bin pull with respect to a reference histogram"},{"location":"tutorials/maxpull/#part-1-first-exploration-on-a-small-test-file","text":"### load the histogram dataframe histname = 'clusterposition_zphi_ontrack_PXLayer_1' filename = 'DF2017B_'+histname+'_subset.csv' datadir = '../data' dloader = DataLoader.DataLoader() df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) # select DCS-bit on data df = dfu.select_dcson(df) print('number of selected lumisections: '+str(len(df))) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017B_clusterposition_zphi_ontrack_PXLayer_1_subset.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 627 rows and 13 columns. raw input data shape: (627, 202, 302) number of selected lumisections: 599 ### extract the histograms as a numpy array from the dataframe (histograms,runnbs,lsnbs) = dfu.get_hist_values(df) print('shape of histogram array: {}'.format(histograms.shape)) print('shape of run number array: {}'.format(runnbs.shape)) print('shape of lumisection number array: {}'.format(lsnbs.shape)) ### further preprocessing of the data (cropping, rebinning, normalizing) histograms = hu.crophists(histograms,[slice(1,-1,None),slice(1,-1,None)]) # remove under- and overflow bins histograms = hu.crophists(histograms,[slice(None,None,None),slice(80,220,None)]) # cut out uninteresting parts histograms = hu.rebinhists(histograms,(5,5)) print('shape of histogram array: {}'.format(histograms.shape)) shape of histogram array: (599, 202, 302) shape of run number array: (599,) shape of lumisection number array: (599,) shape of histogram array: (599, 40, 28) ### define a reference histogram as the average of the set ### and a max pull classifier based on this reference histogram refhist = hu.averagehists( histograms, nout=1 )[0,:,:] print('shape of averaged histogram: {}'.format(refhist.shape)) classifier = MaxPullClassifier.MaxPullClassifier() classifier.train( np.array([refhist]) ) shape of averaged histogram: (40, 28) ### calculate max pull for each histogram maxpulls = classifier.evaluate( histograms ) pu.plot_distance(maxpulls) avg,std = pu.plot_distance(maxpulls,doplot=False) print(avg) print(std) 127.49942708389064 17.68730286891456 ### plot examples for histograms with large pulls threshold = avg+3*std for i in range(len(histograms)): if maxpulls[i] < threshold: continue histlist = [histograms[i],classifier.refhist,classifier.getpull(histograms[i])] subtitles = ['test histogram','reference histogram','pull'] pu.plot_hists_2d(histlist, ncols=3, title = None, subtitles=subtitles) ### plot examples of histograms with small pulls inds = np.argsort(maxpulls)[:3] for i in inds: histlist = [histograms[i],classifier.refhist,classifier.getpull(histograms[i])] subtitles = ['test histogram','reference histogram','pull'] pu.plot_hists_2d(histlist, ncols=3, title = None, subtitles=subtitles) ### investigate a particular lumisection in more detail idx = 0 print('run: {}, lumisection: {}'.format(runnbs[idx],lsnbs[idx])) print('maximum pull for this lumisection: {}'.format(maxpulls[idx])) histlist = [histograms[idx],classifier.refhist,classifier.getpull(histograms[idx])] subtitles = ['test histogram','reference histogram','pull'] _ = pu.plot_hists_2d( histlist, ncols=3, title = None, subtitles=subtitles) run: 297050, lumisection: 1 maximum pull for this lumisection: 301.22227457594295","title":"Part 1: First exploration on a small test file"},{"location":"tutorials/maxpull/#part-2-use-a-locally-changing-reference-histogram","text":"### local approach nprev = 5 maxpulls = np.zeros(len(histograms)) classifiers = [None]*nprev for i in range(nprev,len(histograms)): hist = histograms[i] refhist = hu.averagehists( histograms[i-nprev:i], nout=1 )[0,:,:] classifier = MaxPullClassifier.MaxPullClassifier() classifier.train( np.array([refhist]) ) classifiers.append(classifier) maxpulls[i] = classifier.evaluate( np.array([hist]) ) pu.plot_distance(maxpulls) avg,std = pu.plot_distance(maxpulls,doplot=False) plt.show() print('average pull: {}'.format(avg)) print('std dev of pulls: {}'.format(std)) # make plots of histograms with large local pulls threshold = avg+3*std print('histograms with largest pulls:') for i in range(len(histograms)): if maxpulls[i] < threshold: continue histlist = [histograms[i],classifiers[i].refhist,classifiers[i].getpull(histograms[i])] subtitles = ['test histogram','reference histogram','pull'] title = 'index: {}, run: {}, lumisection: {}, max pull: {}'.format(i, runnbs[i],lsnbs[i],maxpulls[i]) pu.plot_hists_2d(histlist, ncols=3, title = title, subtitles=subtitles) plt.show() # make plots of histograms with small local pulls inds = np.argsort(maxpulls)[nprev:nprev+5] print(inds) print('histograms with smalles pulls:') for i in inds: histlist = [histograms[i],classifiers[i].refhist,classifiers[i].getpull(histograms[i])] subtitles = ['test histogram','reference histogram','pull'] title = 'index: {}, run: {}, lumisection: {}, max pull: {}'.format(i, runnbs[i],lsnbs[i],maxpulls[i]) pu.plot_hists_2d(histlist, ncols=3, title = title, subtitles=subtitles) plt.show() average pull: 60.82421471381158 std dev of pulls: 100.6325204200988 histograms with largest pulls: [232 482 233 591 126] histograms with smalles pulls: ### investigate a particular lumisection in more detail idx = 136 if idx < nprev: raise Exception('ERROR: cannot plot index {} since classification is done based on {} previous lumisections'.format(idx,nprev)) print('run: {}, lumisection: {}'.format(runnbs[idx],lsnbs[idx])) print('maximum pull for this lumisection: {}'.format(maxpulls[idx])) histlist = [histograms[idx],classifiers[idx].refhist,classifiers[idx].getpull(histograms[idx])] subtitles = ['test histogram','reference histogram','pull'] print('test histogram, reference histogram and pulls:') _ = pu.plot_hists_2d( histlist, ncols=3, title = None, subtitles=subtitles) plt.show() print('histograms that were averaged to make the reference histograms:') _ = pu.plot_hists_2d( histograms[idx-nprev:idx], ncols=3 ) run: 297056, lumisection: 17 maximum pull for this lumisection: 660.5883469264701 test histogram, reference histogram and pulls: histograms that were averaged to make the reference histograms:","title":"Part 2: Use a locally changing reference histogram"},{"location":"tutorials/maxpull/#part-3-load-a-set-of-histograms-to-define-a-reference-a-good-test-set-and-a-bad-test-set-and-test-the-discrimination","text":"### load the histograms histname = 'clusterposition_zphi_ontrack_PXLayer_1' datadir = '../data' dloader = DataLoader.DataLoader() # load the training data and train the classifier filename = dffile = 'DF2017B_'+histname+'_subset.csv' df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) df = dfu.select_dcson(df) (hists_ref) = hu.preparedatafromdf(df, cropslices=[slice(1,-1,None),slice(81,221,None)], rebinningfactor=(2,2), donormalize=False, doplot=False) _ = pu.plot_hists_2d(hists_ref[:4], ncols=4, title='some example histograms for averaging') print('number of lumisections in histogram set for averaging: '+str(len(df))) refhist = hu.averagehists( hists_ref, nout=1 )[0,:,:] _ = pu.plot_hist_2d(refhist, title='averaged histogram (used as reference)') print('shape of averaged histogram: {}'.format(refhist.shape)) classifier = MaxPullClassifier.MaxPullClassifier() classifier.train( np.array([refhist]) ) # load the good data filename = dffile = 'DF2017B_'+histname+'_run297056.csv' df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) df = dfu.select_dcson(df) (hists_good, runnbs_good, lsnbs_good) = hu.preparedatafromdf(df, returnrunls=True, cropslices=[slice(1,-1,None),slice(81,221,None)], rebinningfactor=(2,2), donormalize=False, doplot=False) _ = pu.plot_hists_2d(hists_good[:4], ncols=4, title='some example histograms in good test set') print('number of lumisections in good test set: '+str(len(df))) # load the bad data filename = dffile = 'DF2017B_'+histname+'_run297289.csv' df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) df = dfu.select_dcson(df) (hists_bad, runnbs_bad, lsnbs_bad) = hu.preparedatafromdf(df, returnrunls=True, cropslices=[slice(1,-1,None),slice(81,221,None)], rebinningfactor=(2,2), donormalize=False, doplot=False) _ = pu.plot_hists_2d(hists_bad[:4], ncols=4, title='some example histograms in bad test set') print('number of lumisections in bad test set: '+str(len(df))) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017B_clusterposition_zphi_ontrack_PXLayer_1_subset.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 627 rows and 13 columns. raw input data shape: (627, 202, 302) number of lumisections in histogram set for averaging: 599 shape of averaged histogram: (100, 70) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017B_clusterposition_zphi_ontrack_PXLayer_1_run297056.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 196 rows and 14 columns. raw input data shape: (196, 202, 302) number of lumisections in good test set: 185 INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017B_clusterposition_zphi_ontrack_PXLayer_1_run297289.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 15 rows and 14 columns. raw input data shape: (15, 202, 302) number of lumisections in bad test set: 15 ### perform the classification scores_good = classifier.evaluate( hists_good ) labels_good = np.zeros(len(scores_good)) scores_bad = classifier.evaluate( hists_bad ) labels_bad = np.ones(len(scores_bad)) scores = np.concatenate((scores_good,scores_bad)) labels = np.concatenate((labels_good,labels_bad)) _ = pu.plot_score_dist( scores, labels, nbins=50, normalize=True, siglabel='Anomalies', sigcolor='r', bcklabel='Good histograms', bckcolor='g', title='output score distributions for signal and background', xaxtitle='output score', yaxtitle=None) ### check some examples nplot = 5 inds_good = np.random.choice(np.array(list(range(len(hists_good)))),size=nplot) print('example histograms from good test set:') for i in inds_good: histlist = [hists_good[i],classifier.refhist,classifier.getpull(hists_good[i])] subtitles = ['good test histogram','reference histogram','pull'] title = 'index: {}, run: {}, lumisection: {}, max pull: {}'.format(i, runnbs_good[i],lsnbs_good[i],scores_good[i]) pu.plot_hists_2d(histlist, ncols=3, title = title, subtitles=subtitles) plt.show() inds_bad = np.random.choice(np.array(range(len(hists_bad))),size=nplot) print('example histograms from bad test set:') for i in inds_bad: histlist = [hists_bad[i],classifier.refhist,classifier.getpull(hists_bad[i])] subtitles = ['bad test histogram','reference histogram','pull'] title = 'index: {}, run: {}, lumisection: {}, max pull: {}'.format(i, runnbs_bad[i],lsnbs_bad[i],scores_bad[i]) pu.plot_hists_2d(histlist, ncols=3, title = title, subtitles=subtitles) plt.show() example histograms from good test set: example histograms from bad test set: ### re-define the classifier with a nondefault number of maximum pull bins to consider in a loop to determine the optimal value ns = [1,10,20,50,100,500] for n in ns: classifier.set_nmaxpulls( n ) scores_good = classifier.evaluate( hists_good ) labels_good = np.zeros(len(scores_good)) scores_bad = classifier.evaluate( hists_bad ) labels_bad = np.ones(len(scores_bad)) scores = np.concatenate((scores_good,scores_bad)) labels = np.concatenate((labels_good,labels_bad)) _ = pu.plot_score_dist( scores, labels, nbins=50, normalize=True, siglabel='Anomalies', sigcolor='r', bcklabel='Good histograms', bckcolor='g', title='output score distributions for signal and background', xaxtitle='output score', yaxtitle=None)","title":"Part 3: Load a set of histograms to define a reference, a good test set and a bad test set, and test the discrimination"},{"location":"tutorials/nmf_1d/","text":"Train and test an NMF model for a type of 1D monitoring element This notebook showcases how to use an NMF model for a 1D monitoring element. It consists of the following steps: Loading the data Applying selections (e.g. DCS-bit on and sufficient statistics) Preprocessing (e.g. normalizing) Building an NMF model Investigating the output You can use this notebook both for global training (training on a large dataset) and for local training (training on a small number of well-chosen runs). ### imports # external modules import sys import os import numpy as np import matplotlib.pyplot as plt # local modules sys.path.append('../utils') import csv_utils as csvu import dataframe_utils as dfu import json_utils as jsonu import hist_utils as hu import autoencoder_utils as aeu import plot_utils as pu import generate_data_utils as gdu sys.path.append('../src') sys.path.append('../src/classifiers') sys.path.append('../src/cloudfitters') import DataLoader import HistStruct import ModelInterface import NMFClassifier import IdentityFitter 2022-07-26 17:24:20.766349: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc8-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc8-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc8-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.30-e5b21/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc8-opt/lib64/R/library/readr/rcon 2022-07-26 17:24:20.766409: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. ### define run properties # in this cell all major run properties are going to be set, # e.g. what runs to train on and what runs to test on # define a list of good 'reference' runs # (e.g. found by eye) goodrunsls = {'2017': { \"297056\":[[-1]], \"297177\":[[-1]], \"301449\":[[-1]], } } # define core test set of clearly bad runs # (e.g. found by eye) badrunsls = {'2017': { \"297287\":[[-1]], \"297288\":[[-1]], \"297289\":[[-1]], \"299316\":[[-1]], \"299324\":[[-1]], #\"299326\":[[-1]], } } # set year to use year = '2017' # set histogram names to use histname = 'chargeInner_PXLayer_2' # set whether to train globally or locally training_mode = 'global' if training_mode == 'global': runsls_training = None # use none to not add a mask for training (can e.g. use DCS-bit on mask) runsls_good = None # use none to not add a mask for good runs (can e.g. use templates) runsls_bad = badrunsls[year] # predefined bad runs print('selected runs/lumisections for training: all') elif training_mode == 'local': # train locally on a small set of runs # for now on n runs preceding a chosen application run, # to be extended with choosing reference runs. # select application run available_runs = dfu.get_runs( dfu.select_dcson( csvu.read_csv('../data/DF'+year+'_'+histname+'.csv') ) ) run_application = 305351 run_application_index = available_runs.index(run_application) # select training set ntraining = 5 runsls_training = jsonu.tuplelist_to_jsondict([(el,[-1]) for el in available_runs[run_application_index-ntraining:run_application_index]]) runsls_bad = badrunsls[year] runsls_good = jsonu.tuplelist_to_jsondict([(run_application,[-1])]) print('selected runs/lumisections for training: ') print(runsls_training) print('selected runs/lumisections as good test set:') print(runsls_good) print('selected runs/lumisections as bad test set:') print(runsls_bad) selected runs/lumisections for training: all ### read the data based on the configuration defined above readnew = True if readnew: # initializations dloader = DataLoader.DataLoader() histstruct = HistStruct.HistStruct() print('adding {}...'.format(histname)) # read the histograms from the correct csv files filename = '../data/DF'+year+'_'+histname+'.csv' df = dloader.get_dataframe_from_file( filename ) # in case of local training, we can remove most of the histograms if( runsls_training is not None and runsls_good is not None and runsls_bad is not None ): runsls_total = {k: v for d in (runsls_training, runsls_good, runsls_bad) for k, v in d.items()} df = dfu.select_runsls( df, runsls_total ) histstruct.add_dataframe( df ) print('found {} histograms'.format(len(histstruct.runnbs))) # add masks histstruct.add_dcsonjson_mask( 'dcson' ) histstruct.add_goldenjson_mask('golden' ) histstruct.add_highstat_mask( 'highstat' ) if runsls_training is not None: histstruct.add_json_mask( 'training', runsls_training ) if runsls_good is not None: histstruct.add_json_mask( 'good', runsls_good ) nbadruns = 0 if runsls_bad is not None: histstruct.add_json_mask( 'bad', runsls_bad ) # special case for bad runs: add a mask per run (different bad runs have different characteristics) nbadruns = len(runsls_bad.keys()) for i,badrun in enumerate(runsls_bad.keys()): histstruct.add_json_mask( 'bad{}'.format(i), {badrun:runsls_bad[badrun]} ) if not readnew: histstruct = HistStruct.HistStruct.load( hsfilename ) nbadruns = len([name for name in list(histstruct.masks.keys()) if 'bad' in name]) print('created a histstruct with the following properties:') print('- number of histogram types: {}'.format(len(histstruct.histnames))) print('- number of lumisections: {}'.format(len(histstruct.lsnbs))) print('- masks: {}'.format(list(histstruct.masks.keys()))) adding chargeInner_PXLayer_2... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_2.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. found 225954 histograms created a histstruct with the following properties: - number of histogram types: 1 - number of lumisections: 225954 - masks: ['dcson', 'golden', 'highstat', 'bad', 'bad0', 'bad1', 'bad2', 'bad3', 'bad4'] skipthiscell = False if( training_mode=='local' and not skipthiscell ): # training and application runs histstruct.plot_histograms( masknames=[['dcson','highstat','training'],['highstat','good']], labellist = ['training','testing'], colorlist = ['blue','green'] ) # application run and bad test runs histstruct.plot_histograms( masknames=[['dcson','highstat','good'],['bad']], labellist = ['good','bad'], colorlist = ['green','red'] ) ### extend the training set using artificial data extendtraining = False if extendtraining: histstruct.exthistograms['training'] = {} print('generating artificial training data for '+histname) hists = histstruct.get_histograms( histname=histname, masknames=['dcson','highstat','training'] ) print(' original number of histograms: {}'.format(len(hists))) (exthists,_,_) = gdu.upsample_hist_set(hists, 5e4, doplot=True ) histstruct.add_exthistograms( 'training', histname, exthists ) print(' -> generated {} histograms'.format(len(histstruct.exthistograms['training'][histname]))) ### define and train an NMF model modelname = 'nmfclassifier' model = ModelInterface.ModelInterface( histstruct.histnames ) if training_mode=='local': training_masks = ['dcson','highstat','training'] hists_train = histstruct.get_histograms( histname=histname, masknames=training_masks ) elif training_mode=='global': training_masks = ['dcson','highstat'] # use all available data for training (with DCS-on and statistics selection) #hists_train = histstruct.get_histograms( histname=histname, masknames=training_masks ) # this can however take a long time... alternatively, use averaged histograms for training hists_train = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=training_masks ), 1000 ) if extendtraining: hists_train = histstruct.get_exthistograms( 'training', histname=histname ) classifier = NMFClassifier.NMFClassifier( ncomponents=3 ) classifier.train( hists_train ) classifier.set_nmax( 10 ) classifier.set_loss_type( 'chi2' ) model.set_classifiers( {histname: classifier} ) histstruct.add_model( modelname, model ) /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26). warnings.warn((\"The 'init' value, when 'init=None' and \" /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence. warnings.warn(\"Maximum number of iterations %d reached. Increase it to\" adding model \"nmfclassifier\" to the HistStruct ### plot the NMF components components = classifier.get_components() _ = pu.plot_hists_multi( components, colorlist=list(range(len(components))), xaxtitle='bin number', yaxtitle='arbitrary units', title='NMF components' ) ### evaluate the models on all histograms in the (non-extended) histstruct print('evaluating model for '+histname) histstruct.evaluate_classifier( modelname, histname ) evaluating model for chargeInner_PXLayer_2 ### train a density fitter on the scores obtained by the NMF model histstruct.set_fitter( modelname, IdentityFitter.IdentityFitter() ) histstruct.train_fitter( modelname, masknames=training_masks ) histstruct.evaluate_fitter( modelname ) ### extend the test set using artificial data generation and evaluate the model on the extended test set # make the extra data print('generating data for '+histname) if 'good' in histstruct.masks.keys(): goodhists = histstruct.get_histograms( histname=histname,masknames=['dcson','highstat','good'] ) else: goodhists = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['dcson','highstat'] ), 15 ) (goodexthists,_,_) = gdu.upsample_hist_set( goodhists,ntarget=nbadruns*5e3,fourierstdfactor=20., doplot=False) histstruct.add_exthistograms( 'good', histname, goodexthists, overwrite=True ) # alternative: copy original good set (e.g. for using resampled bad but original good) #histstruct.add_exthistograms( 'good', histname, goodhists ) for i in range(nbadruns): badhists = histstruct.get_histograms( histname=histname,masknames=['dcson','highstat','bad{}'.format(i)] ) (badexthists,_,_) = gdu.upsample_hist_set( badhists,ntarget=5e3,fourierstdfactor=20., doplot=False) histstruct.add_exthistograms( 'bad{}'.format(i), histname, badexthists, overwrite=True ) # evaluate the classifiers print('evaluating: '+histname) histstruct.evaluate_classifier( modelname, histname, setnames=['good'] ) for i in range(nbadruns): histstruct.evaluate_classifier( modelname, histname, setnames=['bad{}'.format(i)]) # evaluate the fitter print('evaluating fitter') histstruct.evaluate_fitter( modelname, setnames=['good'] ) for i in range(nbadruns): histstruct.evaluate_fitter( modelname, setnames=['bad{}'.format(i)] ) generating data for chargeInner_PXLayer_2 evaluating: chargeInner_PXLayer_2 evaluating fitter ### get the scores # get the log probability for good set prob_good = histstruct.get_globalscores( modelname, setnames=['good'] ) logprob_good = np.log(prob_good) # get the log probability for bad set prob_bad = histstruct.get_globalscores( modelname, setnames=['bad{}'.format(i) for i in range(nbadruns)] ) logprob_bad = np.log(prob_bad) ### make a roc curve based on the test results above labels_good = np.zeros(len(logprob_good)) # background: label = 0 labels_bad = np.ones(len(logprob_bad)) # signal: label = 1 labels = np.concatenate(tuple([labels_good,labels_bad])) scores = np.concatenate(tuple([-logprob_good,-logprob_bad])) pu.plot_score_dist(scores, labels, nbins=1000, normalize=True) auc = aeu.get_roc(scores, labels, mode='geom', bootstrap_samples=100, doprint=False) aeu.get_confusion_matrix(scores, labels, wp='maxauc') calculating ROC curve on 100 bootstrap samples of size 49958 (-4.156618406017773, <Figure size 432x288 with 2 Axes>, <AxesSubplot:>) ### plot some random examples # define reference histograms refhists = {} if( 'good' in histstruct.masks.keys() ): refhists[histname] = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['highstat','dcson','good']), 15 ) else: refhists[histname] = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['dcson','highstat'] ), 15 ) # define number of plots to make nplot = 10 # make plots for good histograms print('example histograms from the good test set:') runnbs_good = histstruct.get_runnbs( masknames=['highstat','dcson'] ) lsnbs_good = histstruct.get_lsnbs( masknames=['highstat','dcson'] ) indices = np.random.choice( np.arange(len(runnbs_good)), size=nplot, replace=False ) for i in indices: _ = histstruct.plot_ls( runnbs_good[i], lsnbs_good[i], recohist='nmfclassifier', refhists=refhists ) plt.show() # make plots for bad histograms print('example histograms from the bad test set:') runnbs_bad = histstruct.get_runnbs( masknames=['dcson','bad'] ) lsnbs_bad = histstruct.get_lsnbs( masknames=['dcson','bad'] ) indices = np.random.choice( np.arange(len(runnbs_bad)), size=nplot, replace=False ) for i in indices: _ = histstruct.plot_ls( runnbs_bad[i], lsnbs_bad[i], recohist='nmfclassifier', refhists=refhists ) plt.show() example histograms from the good test set: example histograms from the bad test set: ### investigate particular lumisections # initialization: general mode = 'run' run = 299316 # for mode 'ls' (ignored if mode is 'run'): ls = 70 # for mode 'run' (ignored if mode is 'ls'): run_masknames = ['dcson','highstat'] # initialization: reference scores plot_refscores = True refscore_masknames = ['dcson','highstat'] # initialization: reference histograms refhists = {} for histname in histstruct.histnames: refhists[histname] = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['highstat','dcson']), 50 ) if mode=='ls': # plot this particular run/ls fig,axs = histstruct.plot_ls( run, ls, recohist=None, refhists=refhists, opaque_legend=True, ncols = 3, physicalxax = True, ymaxfactor = 1.3, legendsize = 13 ) # print the mses msepoint = histstruct.get_scores_ls( modelname, run, ls ) logprob = np.log( histstruct.evaluate_fitter_on_point( modelname, msepoint ) ) print('-------------') print('MSE values:') for histname in histstruct.histnames: print('{} : {}'.format(histname,msepoint[histname])) print('-------------') print('logprob: '+str(logprob)) # plot mse distribution if plot_refscores: fig,axs = histstruct.plot_ls_score( modelname, run, ls, masknames=refscore_masknames, nbins=100, normalize=True, siglabel='This lumisection', bcklabel='All lumisections', sigcolor='k', bckcolor='b', title=None, xaxtitle='MSE', xaxtitlesize=15, yaxtitle='Normalized number of lumisections', yaxtitlesize=15, doshow=False) if mode=='run': # plot given run runnbs = histstruct.get_runnbs( masknames=run_masknames ) lsnbs = histstruct.get_lsnbs( masknames=run_masknames ) runsel = np.where(runnbs==run) lsnbs = lsnbs[runsel] print('plotting {} lumisections...'.format(len(lsnbs))) for lsnb in lsnbs: fig,ax = histstruct.plot_ls(run, lsnb, recohist=None, refhists=refhists, opaque_legend=True ) plt.show() msepoint = histstruct.get_scores_ls( modelname, run, lsnb ) msepointarray = np.array([msepoint[histname] for histname in histstruct.histnames]) logprob = np.log( histstruct.evaluate_fitter_on_point( modelname, msepoint ) ) print('-------------') print('MSE values:') for histname in histstruct.histnames: print('{} : {}'.format(histname,msepoint[histname])) print('-------------') print('logprob: '+str(logprob)) plotting 43 lumisections... ------------- MSE values: chargeInner_PXLayer_2 : [0.11742876] ------------- logprob: [2.1419234] ------------- MSE values: chargeInner_PXLayer_2 : [0.11759748] ------------- logprob: [2.14048768] ------------- MSE values: chargeInner_PXLayer_2 : [0.11930701] ------------- logprob: [2.12605523] ------------- MSE values: chargeInner_PXLayer_2 : [0.11448997] ------------- logprob: [2.16726804] ------------- MSE values: chargeInner_PXLayer_2 : [0.11787179] ------------- logprob: [2.1381578] ------------- MSE values: chargeInner_PXLayer_2 : [0.11709544] ------------- logprob: [2.14476592] ------------- MSE values: chargeInner_PXLayer_2 : [0.11491754] ------------- logprob: [2.16354048] ------------- MSE values: chargeInner_PXLayer_2 : [0.11651301] ------------- logprob: [2.14975236] ------------- MSE values: chargeInner_PXLayer_2 : [0.11659569] ------------- logprob: [2.14904299] ------------- MSE values: chargeInner_PXLayer_2 : [0.11531861] ------------- logprob: [2.16005649] ------------- MSE values: chargeInner_PXLayer_2 : [0.11699337] ------------- logprob: [2.14563803] ------------- MSE values: chargeInner_PXLayer_2 : [0.11511875] ------------- logprob: [2.16179106] ------------- MSE values: chargeInner_PXLayer_2 : [0.11746122] ------------- logprob: [2.141647] ------------- MSE values: chargeInner_PXLayer_2 : [0.11491104] ------------- logprob: [2.16359701] ------------- MSE values: chargeInner_PXLayer_2 : [0.11559568] ------------- logprob: [2.15765668] ------------- MSE values: chargeInner_PXLayer_2 : [0.11706182] ------------- logprob: [2.14505314] ------------- MSE values: chargeInner_PXLayer_2 : [0.11533168] ------------- logprob: [2.15994317] ------------- MSE values: chargeInner_PXLayer_2 : [0.11316278] ------------- logprob: [2.178928] ------------- MSE values: chargeInner_PXLayer_2 : [0.11744022] ------------- logprob: [2.14182586] ------------- MSE values: chargeInner_PXLayer_2 : [0.11295069] ------------- logprob: [2.18080389] ------------- MSE values: chargeInner_PXLayer_2 : [0.11432042] ------------- logprob: [2.16875009] ------------- MSE values: chargeInner_PXLayer_2 : [0.11577394] ------------- logprob: [2.15611574] ------------- MSE values: chargeInner_PXLayer_2 : [0.11596593] ------------- logprob: [2.15445883] ------------- MSE values: chargeInner_PXLayer_2 : [0.11252853] ------------- logprob: [2.1845485] ------------- MSE values: chargeInner_PXLayer_2 : [0.11418823] ------------- logprob: [2.16990708] ------------- MSE values: chargeInner_PXLayer_2 : [0.11592322] ------------- logprob: [2.15482724] ------------- MSE values: chargeInner_PXLayer_2 : [0.11453416] ------------- logprob: [2.16688216] ------------- MSE values: chargeInner_PXLayer_2 : [0.11061984] ------------- logprob: [2.20165583] ------------- MSE values: chargeInner_PXLayer_2 : [0.11639396] ------------- logprob: [2.15077467] ------------- MSE values: chargeInner_PXLayer_2 : [0.11509686] ------------- logprob: [2.16198121] ------------- MSE values: chargeInner_PXLayer_2 : [0.11489376] ------------- logprob: [2.16374741] ------------- MSE values: chargeInner_PXLayer_2 : [0.11569846] ------------- logprob: [2.15676797] ------------- MSE values: chargeInner_PXLayer_2 : [0.11598947] ------------- logprob: [2.15425584] ------------- MSE values: chargeInner_PXLayer_2 : [0.11440008] ------------- logprob: [2.16805346] ------------- MSE values: chargeInner_PXLayer_2 : [0.11418719] ------------- logprob: [2.16991614] ------------- MSE values: chargeInner_PXLayer_2 : [0.11207464] ------------- logprob: [2.18859017] ------------- MSE values: chargeInner_PXLayer_2 : [0.11372892] ------------- logprob: [2.17393758] ------------- MSE values: chargeInner_PXLayer_2 : [0.11627835] ------------- logprob: [2.1517684] ------------- MSE values: chargeInner_PXLayer_2 : [0.11539924] ------------- logprob: [2.15935751] ------------- MSE values: chargeInner_PXLayer_2 : [0.11177484] ------------- logprob: [2.1912688] ------------- MSE values: chargeInner_PXLayer_2 : [0.11515268] ------------- logprob: [2.16149634] ------------- MSE values: chargeInner_PXLayer_2 : [0.11584298] ------------- logprob: [2.15551965] ------------- MSE values: chargeInner_PXLayer_2 : [0.11319422] ------------- logprob: [2.17865016]","title":"nmf_1d"},{"location":"tutorials/nmf_1d/#train-and-test-an-nmf-model-for-a-type-of-1d-monitoring-element","text":"This notebook showcases how to use an NMF model for a 1D monitoring element. It consists of the following steps: Loading the data Applying selections (e.g. DCS-bit on and sufficient statistics) Preprocessing (e.g. normalizing) Building an NMF model Investigating the output You can use this notebook both for global training (training on a large dataset) and for local training (training on a small number of well-chosen runs). ### imports # external modules import sys import os import numpy as np import matplotlib.pyplot as plt # local modules sys.path.append('../utils') import csv_utils as csvu import dataframe_utils as dfu import json_utils as jsonu import hist_utils as hu import autoencoder_utils as aeu import plot_utils as pu import generate_data_utils as gdu sys.path.append('../src') sys.path.append('../src/classifiers') sys.path.append('../src/cloudfitters') import DataLoader import HistStruct import ModelInterface import NMFClassifier import IdentityFitter 2022-07-26 17:24:20.766349: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc8-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc8-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc8-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.30-e5b21/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc8-opt/lib64/R/library/readr/rcon 2022-07-26 17:24:20.766409: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. ### define run properties # in this cell all major run properties are going to be set, # e.g. what runs to train on and what runs to test on # define a list of good 'reference' runs # (e.g. found by eye) goodrunsls = {'2017': { \"297056\":[[-1]], \"297177\":[[-1]], \"301449\":[[-1]], } } # define core test set of clearly bad runs # (e.g. found by eye) badrunsls = {'2017': { \"297287\":[[-1]], \"297288\":[[-1]], \"297289\":[[-1]], \"299316\":[[-1]], \"299324\":[[-1]], #\"299326\":[[-1]], } } # set year to use year = '2017' # set histogram names to use histname = 'chargeInner_PXLayer_2' # set whether to train globally or locally training_mode = 'global' if training_mode == 'global': runsls_training = None # use none to not add a mask for training (can e.g. use DCS-bit on mask) runsls_good = None # use none to not add a mask for good runs (can e.g. use templates) runsls_bad = badrunsls[year] # predefined bad runs print('selected runs/lumisections for training: all') elif training_mode == 'local': # train locally on a small set of runs # for now on n runs preceding a chosen application run, # to be extended with choosing reference runs. # select application run available_runs = dfu.get_runs( dfu.select_dcson( csvu.read_csv('../data/DF'+year+'_'+histname+'.csv') ) ) run_application = 305351 run_application_index = available_runs.index(run_application) # select training set ntraining = 5 runsls_training = jsonu.tuplelist_to_jsondict([(el,[-1]) for el in available_runs[run_application_index-ntraining:run_application_index]]) runsls_bad = badrunsls[year] runsls_good = jsonu.tuplelist_to_jsondict([(run_application,[-1])]) print('selected runs/lumisections for training: ') print(runsls_training) print('selected runs/lumisections as good test set:') print(runsls_good) print('selected runs/lumisections as bad test set:') print(runsls_bad) selected runs/lumisections for training: all ### read the data based on the configuration defined above readnew = True if readnew: # initializations dloader = DataLoader.DataLoader() histstruct = HistStruct.HistStruct() print('adding {}...'.format(histname)) # read the histograms from the correct csv files filename = '../data/DF'+year+'_'+histname+'.csv' df = dloader.get_dataframe_from_file( filename ) # in case of local training, we can remove most of the histograms if( runsls_training is not None and runsls_good is not None and runsls_bad is not None ): runsls_total = {k: v for d in (runsls_training, runsls_good, runsls_bad) for k, v in d.items()} df = dfu.select_runsls( df, runsls_total ) histstruct.add_dataframe( df ) print('found {} histograms'.format(len(histstruct.runnbs))) # add masks histstruct.add_dcsonjson_mask( 'dcson' ) histstruct.add_goldenjson_mask('golden' ) histstruct.add_highstat_mask( 'highstat' ) if runsls_training is not None: histstruct.add_json_mask( 'training', runsls_training ) if runsls_good is not None: histstruct.add_json_mask( 'good', runsls_good ) nbadruns = 0 if runsls_bad is not None: histstruct.add_json_mask( 'bad', runsls_bad ) # special case for bad runs: add a mask per run (different bad runs have different characteristics) nbadruns = len(runsls_bad.keys()) for i,badrun in enumerate(runsls_bad.keys()): histstruct.add_json_mask( 'bad{}'.format(i), {badrun:runsls_bad[badrun]} ) if not readnew: histstruct = HistStruct.HistStruct.load( hsfilename ) nbadruns = len([name for name in list(histstruct.masks.keys()) if 'bad' in name]) print('created a histstruct with the following properties:') print('- number of histogram types: {}'.format(len(histstruct.histnames))) print('- number of lumisections: {}'.format(len(histstruct.lsnbs))) print('- masks: {}'.format(list(histstruct.masks.keys()))) adding chargeInner_PXLayer_2... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_2.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. found 225954 histograms created a histstruct with the following properties: - number of histogram types: 1 - number of lumisections: 225954 - masks: ['dcson', 'golden', 'highstat', 'bad', 'bad0', 'bad1', 'bad2', 'bad3', 'bad4'] skipthiscell = False if( training_mode=='local' and not skipthiscell ): # training and application runs histstruct.plot_histograms( masknames=[['dcson','highstat','training'],['highstat','good']], labellist = ['training','testing'], colorlist = ['blue','green'] ) # application run and bad test runs histstruct.plot_histograms( masknames=[['dcson','highstat','good'],['bad']], labellist = ['good','bad'], colorlist = ['green','red'] ) ### extend the training set using artificial data extendtraining = False if extendtraining: histstruct.exthistograms['training'] = {} print('generating artificial training data for '+histname) hists = histstruct.get_histograms( histname=histname, masknames=['dcson','highstat','training'] ) print(' original number of histograms: {}'.format(len(hists))) (exthists,_,_) = gdu.upsample_hist_set(hists, 5e4, doplot=True ) histstruct.add_exthistograms( 'training', histname, exthists ) print(' -> generated {} histograms'.format(len(histstruct.exthistograms['training'][histname]))) ### define and train an NMF model modelname = 'nmfclassifier' model = ModelInterface.ModelInterface( histstruct.histnames ) if training_mode=='local': training_masks = ['dcson','highstat','training'] hists_train = histstruct.get_histograms( histname=histname, masknames=training_masks ) elif training_mode=='global': training_masks = ['dcson','highstat'] # use all available data for training (with DCS-on and statistics selection) #hists_train = histstruct.get_histograms( histname=histname, masknames=training_masks ) # this can however take a long time... alternatively, use averaged histograms for training hists_train = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=training_masks ), 1000 ) if extendtraining: hists_train = histstruct.get_exthistograms( 'training', histname=histname ) classifier = NMFClassifier.NMFClassifier( ncomponents=3 ) classifier.train( hists_train ) classifier.set_nmax( 10 ) classifier.set_loss_type( 'chi2' ) model.set_classifiers( {histname: classifier} ) histstruct.add_model( modelname, model ) /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26). warnings.warn((\"The 'init' value, when 'init=None' and \" /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence. warnings.warn(\"Maximum number of iterations %d reached. Increase it to\" adding model \"nmfclassifier\" to the HistStruct ### plot the NMF components components = classifier.get_components() _ = pu.plot_hists_multi( components, colorlist=list(range(len(components))), xaxtitle='bin number', yaxtitle='arbitrary units', title='NMF components' ) ### evaluate the models on all histograms in the (non-extended) histstruct print('evaluating model for '+histname) histstruct.evaluate_classifier( modelname, histname ) evaluating model for chargeInner_PXLayer_2 ### train a density fitter on the scores obtained by the NMF model histstruct.set_fitter( modelname, IdentityFitter.IdentityFitter() ) histstruct.train_fitter( modelname, masknames=training_masks ) histstruct.evaluate_fitter( modelname ) ### extend the test set using artificial data generation and evaluate the model on the extended test set # make the extra data print('generating data for '+histname) if 'good' in histstruct.masks.keys(): goodhists = histstruct.get_histograms( histname=histname,masknames=['dcson','highstat','good'] ) else: goodhists = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['dcson','highstat'] ), 15 ) (goodexthists,_,_) = gdu.upsample_hist_set( goodhists,ntarget=nbadruns*5e3,fourierstdfactor=20., doplot=False) histstruct.add_exthistograms( 'good', histname, goodexthists, overwrite=True ) # alternative: copy original good set (e.g. for using resampled bad but original good) #histstruct.add_exthistograms( 'good', histname, goodhists ) for i in range(nbadruns): badhists = histstruct.get_histograms( histname=histname,masknames=['dcson','highstat','bad{}'.format(i)] ) (badexthists,_,_) = gdu.upsample_hist_set( badhists,ntarget=5e3,fourierstdfactor=20., doplot=False) histstruct.add_exthistograms( 'bad{}'.format(i), histname, badexthists, overwrite=True ) # evaluate the classifiers print('evaluating: '+histname) histstruct.evaluate_classifier( modelname, histname, setnames=['good'] ) for i in range(nbadruns): histstruct.evaluate_classifier( modelname, histname, setnames=['bad{}'.format(i)]) # evaluate the fitter print('evaluating fitter') histstruct.evaluate_fitter( modelname, setnames=['good'] ) for i in range(nbadruns): histstruct.evaluate_fitter( modelname, setnames=['bad{}'.format(i)] ) generating data for chargeInner_PXLayer_2 evaluating: chargeInner_PXLayer_2 evaluating fitter ### get the scores # get the log probability for good set prob_good = histstruct.get_globalscores( modelname, setnames=['good'] ) logprob_good = np.log(prob_good) # get the log probability for bad set prob_bad = histstruct.get_globalscores( modelname, setnames=['bad{}'.format(i) for i in range(nbadruns)] ) logprob_bad = np.log(prob_bad) ### make a roc curve based on the test results above labels_good = np.zeros(len(logprob_good)) # background: label = 0 labels_bad = np.ones(len(logprob_bad)) # signal: label = 1 labels = np.concatenate(tuple([labels_good,labels_bad])) scores = np.concatenate(tuple([-logprob_good,-logprob_bad])) pu.plot_score_dist(scores, labels, nbins=1000, normalize=True) auc = aeu.get_roc(scores, labels, mode='geom', bootstrap_samples=100, doprint=False) aeu.get_confusion_matrix(scores, labels, wp='maxauc') calculating ROC curve on 100 bootstrap samples of size 49958 (-4.156618406017773, <Figure size 432x288 with 2 Axes>, <AxesSubplot:>) ### plot some random examples # define reference histograms refhists = {} if( 'good' in histstruct.masks.keys() ): refhists[histname] = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['highstat','dcson','good']), 15 ) else: refhists[histname] = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['dcson','highstat'] ), 15 ) # define number of plots to make nplot = 10 # make plots for good histograms print('example histograms from the good test set:') runnbs_good = histstruct.get_runnbs( masknames=['highstat','dcson'] ) lsnbs_good = histstruct.get_lsnbs( masknames=['highstat','dcson'] ) indices = np.random.choice( np.arange(len(runnbs_good)), size=nplot, replace=False ) for i in indices: _ = histstruct.plot_ls( runnbs_good[i], lsnbs_good[i], recohist='nmfclassifier', refhists=refhists ) plt.show() # make plots for bad histograms print('example histograms from the bad test set:') runnbs_bad = histstruct.get_runnbs( masknames=['dcson','bad'] ) lsnbs_bad = histstruct.get_lsnbs( masknames=['dcson','bad'] ) indices = np.random.choice( np.arange(len(runnbs_bad)), size=nplot, replace=False ) for i in indices: _ = histstruct.plot_ls( runnbs_bad[i], lsnbs_bad[i], recohist='nmfclassifier', refhists=refhists ) plt.show() example histograms from the good test set: example histograms from the bad test set: ### investigate particular lumisections # initialization: general mode = 'run' run = 299316 # for mode 'ls' (ignored if mode is 'run'): ls = 70 # for mode 'run' (ignored if mode is 'ls'): run_masknames = ['dcson','highstat'] # initialization: reference scores plot_refscores = True refscore_masknames = ['dcson','highstat'] # initialization: reference histograms refhists = {} for histname in histstruct.histnames: refhists[histname] = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['highstat','dcson']), 50 ) if mode=='ls': # plot this particular run/ls fig,axs = histstruct.plot_ls( run, ls, recohist=None, refhists=refhists, opaque_legend=True, ncols = 3, physicalxax = True, ymaxfactor = 1.3, legendsize = 13 ) # print the mses msepoint = histstruct.get_scores_ls( modelname, run, ls ) logprob = np.log( histstruct.evaluate_fitter_on_point( modelname, msepoint ) ) print('-------------') print('MSE values:') for histname in histstruct.histnames: print('{} : {}'.format(histname,msepoint[histname])) print('-------------') print('logprob: '+str(logprob)) # plot mse distribution if plot_refscores: fig,axs = histstruct.plot_ls_score( modelname, run, ls, masknames=refscore_masknames, nbins=100, normalize=True, siglabel='This lumisection', bcklabel='All lumisections', sigcolor='k', bckcolor='b', title=None, xaxtitle='MSE', xaxtitlesize=15, yaxtitle='Normalized number of lumisections', yaxtitlesize=15, doshow=False) if mode=='run': # plot given run runnbs = histstruct.get_runnbs( masknames=run_masknames ) lsnbs = histstruct.get_lsnbs( masknames=run_masknames ) runsel = np.where(runnbs==run) lsnbs = lsnbs[runsel] print('plotting {} lumisections...'.format(len(lsnbs))) for lsnb in lsnbs: fig,ax = histstruct.plot_ls(run, lsnb, recohist=None, refhists=refhists, opaque_legend=True ) plt.show() msepoint = histstruct.get_scores_ls( modelname, run, lsnb ) msepointarray = np.array([msepoint[histname] for histname in histstruct.histnames]) logprob = np.log( histstruct.evaluate_fitter_on_point( modelname, msepoint ) ) print('-------------') print('MSE values:') for histname in histstruct.histnames: print('{} : {}'.format(histname,msepoint[histname])) print('-------------') print('logprob: '+str(logprob)) plotting 43 lumisections... ------------- MSE values: chargeInner_PXLayer_2 : [0.11742876] ------------- logprob: [2.1419234] ------------- MSE values: chargeInner_PXLayer_2 : [0.11759748] ------------- logprob: [2.14048768] ------------- MSE values: chargeInner_PXLayer_2 : [0.11930701] ------------- logprob: [2.12605523] ------------- MSE values: chargeInner_PXLayer_2 : [0.11448997] ------------- logprob: [2.16726804] ------------- MSE values: chargeInner_PXLayer_2 : [0.11787179] ------------- logprob: [2.1381578] ------------- MSE values: chargeInner_PXLayer_2 : [0.11709544] ------------- logprob: [2.14476592] ------------- MSE values: chargeInner_PXLayer_2 : [0.11491754] ------------- logprob: [2.16354048] ------------- MSE values: chargeInner_PXLayer_2 : [0.11651301] ------------- logprob: [2.14975236] ------------- MSE values: chargeInner_PXLayer_2 : [0.11659569] ------------- logprob: [2.14904299] ------------- MSE values: chargeInner_PXLayer_2 : [0.11531861] ------------- logprob: [2.16005649] ------------- MSE values: chargeInner_PXLayer_2 : [0.11699337] ------------- logprob: [2.14563803] ------------- MSE values: chargeInner_PXLayer_2 : [0.11511875] ------------- logprob: [2.16179106] ------------- MSE values: chargeInner_PXLayer_2 : [0.11746122] ------------- logprob: [2.141647] ------------- MSE values: chargeInner_PXLayer_2 : [0.11491104] ------------- logprob: [2.16359701] ------------- MSE values: chargeInner_PXLayer_2 : [0.11559568] ------------- logprob: [2.15765668] ------------- MSE values: chargeInner_PXLayer_2 : [0.11706182] ------------- logprob: [2.14505314] ------------- MSE values: chargeInner_PXLayer_2 : [0.11533168] ------------- logprob: [2.15994317] ------------- MSE values: chargeInner_PXLayer_2 : [0.11316278] ------------- logprob: [2.178928] ------------- MSE values: chargeInner_PXLayer_2 : [0.11744022] ------------- logprob: [2.14182586] ------------- MSE values: chargeInner_PXLayer_2 : [0.11295069] ------------- logprob: [2.18080389] ------------- MSE values: chargeInner_PXLayer_2 : [0.11432042] ------------- logprob: [2.16875009] ------------- MSE values: chargeInner_PXLayer_2 : [0.11577394] ------------- logprob: [2.15611574] ------------- MSE values: chargeInner_PXLayer_2 : [0.11596593] ------------- logprob: [2.15445883] ------------- MSE values: chargeInner_PXLayer_2 : [0.11252853] ------------- logprob: [2.1845485] ------------- MSE values: chargeInner_PXLayer_2 : [0.11418823] ------------- logprob: [2.16990708] ------------- MSE values: chargeInner_PXLayer_2 : [0.11592322] ------------- logprob: [2.15482724] ------------- MSE values: chargeInner_PXLayer_2 : [0.11453416] ------------- logprob: [2.16688216] ------------- MSE values: chargeInner_PXLayer_2 : [0.11061984] ------------- logprob: [2.20165583] ------------- MSE values: chargeInner_PXLayer_2 : [0.11639396] ------------- logprob: [2.15077467] ------------- MSE values: chargeInner_PXLayer_2 : [0.11509686] ------------- logprob: [2.16198121] ------------- MSE values: chargeInner_PXLayer_2 : [0.11489376] ------------- logprob: [2.16374741] ------------- MSE values: chargeInner_PXLayer_2 : [0.11569846] ------------- logprob: [2.15676797] ------------- MSE values: chargeInner_PXLayer_2 : [0.11598947] ------------- logprob: [2.15425584] ------------- MSE values: chargeInner_PXLayer_2 : [0.11440008] ------------- logprob: [2.16805346] ------------- MSE values: chargeInner_PXLayer_2 : [0.11418719] ------------- logprob: [2.16991614] ------------- MSE values: chargeInner_PXLayer_2 : [0.11207464] ------------- logprob: [2.18859017] ------------- MSE values: chargeInner_PXLayer_2 : [0.11372892] ------------- logprob: [2.17393758] ------------- MSE values: chargeInner_PXLayer_2 : [0.11627835] ------------- logprob: [2.1517684] ------------- MSE values: chargeInner_PXLayer_2 : [0.11539924] ------------- logprob: [2.15935751] ------------- MSE values: chargeInner_PXLayer_2 : [0.11177484] ------------- logprob: [2.1912688] ------------- MSE values: chargeInner_PXLayer_2 : [0.11515268] ------------- logprob: [2.16149634] ------------- MSE values: chargeInner_PXLayer_2 : [0.11584298] ------------- logprob: [2.15551965] ------------- MSE values: chargeInner_PXLayer_2 : [0.11319422] ------------- logprob: [2.17865016]","title":"Train and test an NMF model for a type of 1D monitoring element"},{"location":"tutorials/nmf_2d/","text":"Two-dimensional histogram classification based on non-negative matrix factorization This notebook shows the proof-of-principle of an NMF model on 2D monitoring elements. It also shows how to retrieve and use information from OMS. ### imports # external modules import sys import os import numpy as np import matplotlib.pyplot as plt # local modules sys.path.append('../utils') import dataframe_utils as dfu import plot_utils as pu import hist_utils as hu import generate_data_2d_utils as g2u import json_utils as jsonu sys.path.append('../src') import DataLoader sys.path.append('../src/classifiers') import NMFClassifier sys.path.append('../omsapi') from get_oms_data import get_oms_api,get_oms_data,get_oms_response_attribute 2022-07-26 17:24:40.584465: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc8-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc8-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc8-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.30-e5b21/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc8-opt/lib64/R/library/readr/rcon 2022-07-26 17:24:40.584531: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. ### load the histograms # note: this cell assumes you have a csv file stored at the specified location, # containing only histograms of the specified type; # see the tutorial read_and_write_data for examples on how to create such files! histname = 'clusterposition_zphi_ontrack_PXLayer_1' datadir = '../data' dloader = DataLoader.DataLoader() # read training data filename = 'DF2017B_'+histname+'_subset.csv' histdf = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(histdf)[0].shape )) histdf = dfu.select_dcson(histdf) (hists_ref) = hu.preparedatafromdf(histdf, cropslices=[slice(1,-1,None),slice(81,221,None)], rebinningfactor=(2,2), donormalize=True, doplot=False) _ = pu.plot_hists_2d(hists_ref[:4], ncols=4, title='some example histograms for NMF model') print('number of lumisections: '+str(len(histdf))) # read good test data filename = 'DF2017B_'+histname+'_run297056.csv' histdf = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(histdf)[0].shape )) histdf = dfu.select_dcson(histdf) (hists_good, runnbs_good, lsnbs_good) = hu.preparedatafromdf(histdf, returnrunls=True, cropslices=[slice(1,-1,None),slice(81,221,None)], rebinningfactor=(2,2), donormalize=True, doplot=False) _ = pu.plot_hists_2d(hists_good[:4], ncols=4, title='some example histograms in good test set') print('number of lumisections: '+str(len(histdf))) # read bad test data filename = 'DF2017B_'+histname+'_run297289.csv' histdf = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(histdf)[0].shape )) histdf = dfu.select_dcson(histdf) (hists_bad, runnbs_bad, lsnbs_bad) = hu.preparedatafromdf(histdf, returnrunls=True, cropslices=[slice(1,-1,None),slice(81,221,None)], rebinningfactor=(2,2), donormalize=True, doplot=False) _ = pu.plot_hists_2d(hists_bad[:4], ncols=4, title='some example histograms in bad test set') print('number of lumisections: '+str(len(histdf))) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017B_clusterposition_zphi_ontrack_PXLayer_1_subset.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 627 rows and 13 columns. raw input data shape: (627, 202, 302) number of lumisections: 599 INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017B_clusterposition_zphi_ontrack_PXLayer_1_run297056.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 196 rows and 14 columns. raw input data shape: (196, 202, 302) number of lumisections: 185 INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017B_clusterposition_zphi_ontrack_PXLayer_1_run297289.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 15 rows and 14 columns. raw input data shape: (15, 202, 302) number of lumisections: 15 ### build an NMF model classifier = NMFClassifier.NMFClassifier( ncomponents=10, nmax=50 ) classifier.train( hists_ref ) /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26). warnings.warn((\"The 'init' value, when 'init=None' and \" /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence. warnings.warn(\"Maximum number of iterations %d reached. Increase it to\" ### optionally enlarge the test set using resampling do_resampling = True if do_resampling: hists_good_ext = g2u.fourier_noise_nd(hists_good, nresamples=4, nonnegative=True, stdfactor=10., kmaxscale=0.5, ncomponents=3) print('shape of resampled good test set: {}'.format(hists_good_ext.shape)) hists_bad_ext = g2u.fourier_noise_nd(hists_bad, nresamples=40, nonnegative=True, stdfactor=5., kmaxscale=0.5, ncomponents=3) print('shape of resampled bad test set: {}'.format(hists_bad_ext.shape)) shape of resampled good test set: (740, 100, 70) shape of resampled bad test set: (600, 100, 70) ### perform the classification classifier.set_nmax( 50 ) scores_good = classifier.evaluate( hists_good ) scores_bad = classifier.evaluate( hists_bad ) if do_resampling: scores_good_ext = classifier.evaluate( hists_good_ext ) scores_bad_ext = classifier.evaluate( hists_bad_ext ) hists_good_appl = hists_good hists_bad_appl = hists_bad scores_good_appl = scores_good scores_bad_appl = scores_bad if do_resampling: hists_good_appl = hists_good_ext hists_bad_appl = hists_bad_ext scores_good_appl = scores_good_ext scores_bad_appl = scores_bad_ext labels_good = np.zeros(len(scores_good_appl)) labels_bad = np.ones(len(scores_bad_appl)) scores = np.concatenate((scores_good_appl,scores_bad_appl)) labels = np.concatenate((labels_good,labels_bad)) _ = pu.plot_score_dist( scores, labels, nbins=100, normalize=True, siglabel='Anomalous', sigcolor='r', bcklabel='Training (mostly good)', bckcolor='g', title='output score distributions for signal and background', xaxtitle='output score', yaxtitle=None) ### check some random examples nplot = 5 inds_good = np.random.choice(np.array(list(range(len(hists_good)))),size=nplot) print('example histograms from good test set:') for i in inds_good: histlist = [hists_good[i],classifier.reconstruct(np.array([hists_good[i]]))[0]] subtitles = ['good test histogram','NMF reconstruction'] title = 'index: {}, run: {}, lumisection: {}, MSE: {:.2E}'.format(i, runnbs_good[i],lsnbs_good[i],scores_good[i]) pu.plot_hists_2d(histlist, ncols=2, title = title, titlesize=20, subtitles=subtitles, subtitlesize=15) plt.show() inds_bad = np.random.choice(np.array(range(len(hists_bad))),size=nplot) print('example histograms from bad test set:') for i in inds_bad: histlist = [hists_bad[i],classifier.reconstruct(np.array([hists_bad[i]]))[0]] subtitles = ['bad test histogram','NMF reconstruction'] title = 'index: {}, run: {}, lumisection: {}, MSE: {:.2E}'.format(i, runnbs_bad[i],lsnbs_bad[i],scores_bad[i]) pu.plot_hists_2d(histlist, ncols=2, title = title, titlesize=20, subtitles=subtitles, subtitlesize=15) plt.show() example histograms from good test set: example histograms from bad test set: ### check some examples in a given MSE range mserange = (0.1,0.25) test_histograms = hists_good test_scores = scores_good test_runnbs = runnbs_good test_lsnbs = lsnbs_good inds = np.where( (test_scores>mserange[0]) & (test_scores<mserange[1]) )[0] print('{} out of {} histograms fall within this MSE range'.format(len(inds),len(test_scores))) for indx in inds: hist = test_histograms[indx] title = 'index: {}, run: {}, lumisection: {}, MSE: {:.2E}'.format(indx, test_runnbs[indx], test_lsnbs[indx], test_scores[indx]) pu.plot_hist_2d(hist, title = title) plt.show() 4 out of 185 histograms fall within this MSE range ### extra: retrieve information from OMS on pileup, lumi and/or trigger rates omsapi = get_oms_api() # pileup and luminosity in good test set oms_target_run_good = 297056 ls_info_good = get_oms_data( omsapi, 'lumisections', oms_target_run_good, attributes=['lumisection_number','pileup','delivered_lumi','recorded_lumi'] ) # pileup and luminosity in bad test set oms_target_run_bad = 297289 ls_info_bad = get_oms_data( omsapi, 'lumisections', oms_target_run_bad, attributes=['lumisection_number','pileup','delivered_lumi','recorded_lumi']) https://vocms0185.cern.ch/agg/api/v1/lumisections/?fields=lumisection_number,delivered_lumi,pileup,recorded_lumi&filter[run_number][EQ]=297056&page[offset]=0&page[limit]=1000 https://vocms0185.cern.ch/agg/api/v1/lumisections/?fields=lumisection_number,delivered_lumi,pileup,recorded_lumi&filter[run_number][EQ]=297056&page[offset]=0&page[limit]=1000 https://vocms0185.cern.ch/agg/api/v1/lumisections/?fields=lumisection_number,delivered_lumi,pileup,recorded_lumi&filter[run_number][EQ]=297289&page[offset]=0&page[limit]=1000 https://vocms0185.cern.ch/agg/api/v1/lumisections/?fields=lumisection_number,delivered_lumi,pileup,recorded_lumi&filter[run_number][EQ]=297289&page[offset]=0&page[limit]=1000 ### make plots of OMS info # plots for good run ls_good = get_oms_response_attribute(ls_info_good,'lumisection_number') pileup_good = get_oms_response_attribute(ls_info_good,'pileup') lumi_del_good = get_oms_response_attribute(ls_info_good,'delivered_lumi') lumi_rec_good = get_oms_response_attribute(ls_info_good,'recorded_lumi') dcson_good = jsonu.isdcson( [oms_target_run_good]*len(ls_good), ls_good ) pu.plot_hists([pileup_good], colorlist=['b'], labellist=['pileup'], xlims=(ls_good[0],ls_good[-1]), title='pileup for good run', xaxtitle='lumisection number', yaxtitle='pileup', bkgcolor=dcson_good,bkgcmap='cool') pu.plot_hists([lumi_rec_good,lumi_del_good], colorlist=['g','b'], labellist=['recorded luminosity','delivered luminosity'], xlims=(ls_good[0],ls_good[-1]), title='luminosity for good run', xaxtitle='lumisection number', yaxtitle='luminosity', bkgcolor=dcson_good,bkgcmap='cool') ls_bad = get_oms_response_attribute(ls_info_bad,'lumisection_number') pileup_bad = get_oms_response_attribute(ls_info_bad,'pileup') lumi_del_bad = get_oms_response_attribute(ls_info_bad,'delivered_lumi') lumi_rec_bad = get_oms_response_attribute(ls_info_bad,'recorded_lumi') dcson_bad = jsonu.isdcson( [oms_target_run_bad]*len(ls_bad), ls_bad ) pu.plot_hists([pileup_bad], colorlist=['b'], labellist=['pileup'], xlims=(ls_bad[0],ls_bad[-1]), title='pileup for bad run', xaxtitle='lumisection number', yaxtitle='pileup', bkgcolor=dcson_bad,bkgcmap='cool') pu.plot_hists([lumi_rec_bad,lumi_del_bad], colorlist=['g','b'], labellist=['recorded luminosity','delivered luminosity'], xlims=(ls_bad[0],ls_bad[-1]), title='luminosity for bad run', xaxtitle='lumisection number', yaxtitle='luminosity', bkgcolor=dcson_bad,bkgcmap='cool') (<Figure size 432x288 with 2 Axes>, <AxesSubplot:title={'center':'luminosity for bad run'}, xlabel='lumisection number', ylabel='luminosity'>) ### plot particular lumisections based on the OMS info above lsnbs_tocheck = list(range(25,40)) idx_oms = [np.where(np.array(ls_good)==lsnb)[0][0] for lsnb in lsnbs_tocheck] idx_hist = [np.where(lsnbs_good==lsnb)[0][0] for lsnb in lsnbs_tocheck] histlist = [hists_good[i] for i in idx_hist] subtitles = ['lumisection: {}, pileup: {}'.format(lsnbs_tocheck[i],pileup_good[idx_oms[i]]) for i in range(len(lsnbs_tocheck))] _ = pu.plot_hists_2d(histlist, subtitles=subtitles)","title":"nmf_2d"},{"location":"tutorials/nmf_2d/#two-dimensional-histogram-classification-based-on-non-negative-matrix-factorization","text":"This notebook shows the proof-of-principle of an NMF model on 2D monitoring elements. It also shows how to retrieve and use information from OMS. ### imports # external modules import sys import os import numpy as np import matplotlib.pyplot as plt # local modules sys.path.append('../utils') import dataframe_utils as dfu import plot_utils as pu import hist_utils as hu import generate_data_2d_utils as g2u import json_utils as jsonu sys.path.append('../src') import DataLoader sys.path.append('../src/classifiers') import NMFClassifier sys.path.append('../omsapi') from get_oms_data import get_oms_api,get_oms_data,get_oms_response_attribute 2022-07-26 17:24:40.584465: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc8-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc8-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc8-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.30-e5b21/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc8-opt/lib64/R/library/readr/rcon 2022-07-26 17:24:40.584531: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. ### load the histograms # note: this cell assumes you have a csv file stored at the specified location, # containing only histograms of the specified type; # see the tutorial read_and_write_data for examples on how to create such files! histname = 'clusterposition_zphi_ontrack_PXLayer_1' datadir = '../data' dloader = DataLoader.DataLoader() # read training data filename = 'DF2017B_'+histname+'_subset.csv' histdf = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(histdf)[0].shape )) histdf = dfu.select_dcson(histdf) (hists_ref) = hu.preparedatafromdf(histdf, cropslices=[slice(1,-1,None),slice(81,221,None)], rebinningfactor=(2,2), donormalize=True, doplot=False) _ = pu.plot_hists_2d(hists_ref[:4], ncols=4, title='some example histograms for NMF model') print('number of lumisections: '+str(len(histdf))) # read good test data filename = 'DF2017B_'+histname+'_run297056.csv' histdf = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(histdf)[0].shape )) histdf = dfu.select_dcson(histdf) (hists_good, runnbs_good, lsnbs_good) = hu.preparedatafromdf(histdf, returnrunls=True, cropslices=[slice(1,-1,None),slice(81,221,None)], rebinningfactor=(2,2), donormalize=True, doplot=False) _ = pu.plot_hists_2d(hists_good[:4], ncols=4, title='some example histograms in good test set') print('number of lumisections: '+str(len(histdf))) # read bad test data filename = 'DF2017B_'+histname+'_run297289.csv' histdf = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(histdf)[0].shape )) histdf = dfu.select_dcson(histdf) (hists_bad, runnbs_bad, lsnbs_bad) = hu.preparedatafromdf(histdf, returnrunls=True, cropslices=[slice(1,-1,None),slice(81,221,None)], rebinningfactor=(2,2), donormalize=True, doplot=False) _ = pu.plot_hists_2d(hists_bad[:4], ncols=4, title='some example histograms in bad test set') print('number of lumisections: '+str(len(histdf))) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017B_clusterposition_zphi_ontrack_PXLayer_1_subset.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 627 rows and 13 columns. raw input data shape: (627, 202, 302) number of lumisections: 599 INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017B_clusterposition_zphi_ontrack_PXLayer_1_run297056.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 196 rows and 14 columns. raw input data shape: (196, 202, 302) number of lumisections: 185 INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017B_clusterposition_zphi_ontrack_PXLayer_1_run297289.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 15 rows and 14 columns. raw input data shape: (15, 202, 302) number of lumisections: 15 ### build an NMF model classifier = NMFClassifier.NMFClassifier( ncomponents=10, nmax=50 ) classifier.train( hists_ref ) /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26). warnings.warn((\"The 'init' value, when 'init=None' and \" /cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence. warnings.warn(\"Maximum number of iterations %d reached. Increase it to\" ### optionally enlarge the test set using resampling do_resampling = True if do_resampling: hists_good_ext = g2u.fourier_noise_nd(hists_good, nresamples=4, nonnegative=True, stdfactor=10., kmaxscale=0.5, ncomponents=3) print('shape of resampled good test set: {}'.format(hists_good_ext.shape)) hists_bad_ext = g2u.fourier_noise_nd(hists_bad, nresamples=40, nonnegative=True, stdfactor=5., kmaxscale=0.5, ncomponents=3) print('shape of resampled bad test set: {}'.format(hists_bad_ext.shape)) shape of resampled good test set: (740, 100, 70) shape of resampled bad test set: (600, 100, 70) ### perform the classification classifier.set_nmax( 50 ) scores_good = classifier.evaluate( hists_good ) scores_bad = classifier.evaluate( hists_bad ) if do_resampling: scores_good_ext = classifier.evaluate( hists_good_ext ) scores_bad_ext = classifier.evaluate( hists_bad_ext ) hists_good_appl = hists_good hists_bad_appl = hists_bad scores_good_appl = scores_good scores_bad_appl = scores_bad if do_resampling: hists_good_appl = hists_good_ext hists_bad_appl = hists_bad_ext scores_good_appl = scores_good_ext scores_bad_appl = scores_bad_ext labels_good = np.zeros(len(scores_good_appl)) labels_bad = np.ones(len(scores_bad_appl)) scores = np.concatenate((scores_good_appl,scores_bad_appl)) labels = np.concatenate((labels_good,labels_bad)) _ = pu.plot_score_dist( scores, labels, nbins=100, normalize=True, siglabel='Anomalous', sigcolor='r', bcklabel='Training (mostly good)', bckcolor='g', title='output score distributions for signal and background', xaxtitle='output score', yaxtitle=None) ### check some random examples nplot = 5 inds_good = np.random.choice(np.array(list(range(len(hists_good)))),size=nplot) print('example histograms from good test set:') for i in inds_good: histlist = [hists_good[i],classifier.reconstruct(np.array([hists_good[i]]))[0]] subtitles = ['good test histogram','NMF reconstruction'] title = 'index: {}, run: {}, lumisection: {}, MSE: {:.2E}'.format(i, runnbs_good[i],lsnbs_good[i],scores_good[i]) pu.plot_hists_2d(histlist, ncols=2, title = title, titlesize=20, subtitles=subtitles, subtitlesize=15) plt.show() inds_bad = np.random.choice(np.array(range(len(hists_bad))),size=nplot) print('example histograms from bad test set:') for i in inds_bad: histlist = [hists_bad[i],classifier.reconstruct(np.array([hists_bad[i]]))[0]] subtitles = ['bad test histogram','NMF reconstruction'] title = 'index: {}, run: {}, lumisection: {}, MSE: {:.2E}'.format(i, runnbs_bad[i],lsnbs_bad[i],scores_bad[i]) pu.plot_hists_2d(histlist, ncols=2, title = title, titlesize=20, subtitles=subtitles, subtitlesize=15) plt.show() example histograms from good test set: example histograms from bad test set: ### check some examples in a given MSE range mserange = (0.1,0.25) test_histograms = hists_good test_scores = scores_good test_runnbs = runnbs_good test_lsnbs = lsnbs_good inds = np.where( (test_scores>mserange[0]) & (test_scores<mserange[1]) )[0] print('{} out of {} histograms fall within this MSE range'.format(len(inds),len(test_scores))) for indx in inds: hist = test_histograms[indx] title = 'index: {}, run: {}, lumisection: {}, MSE: {:.2E}'.format(indx, test_runnbs[indx], test_lsnbs[indx], test_scores[indx]) pu.plot_hist_2d(hist, title = title) plt.show() 4 out of 185 histograms fall within this MSE range ### extra: retrieve information from OMS on pileup, lumi and/or trigger rates omsapi = get_oms_api() # pileup and luminosity in good test set oms_target_run_good = 297056 ls_info_good = get_oms_data( omsapi, 'lumisections', oms_target_run_good, attributes=['lumisection_number','pileup','delivered_lumi','recorded_lumi'] ) # pileup and luminosity in bad test set oms_target_run_bad = 297289 ls_info_bad = get_oms_data( omsapi, 'lumisections', oms_target_run_bad, attributes=['lumisection_number','pileup','delivered_lumi','recorded_lumi']) https://vocms0185.cern.ch/agg/api/v1/lumisections/?fields=lumisection_number,delivered_lumi,pileup,recorded_lumi&filter[run_number][EQ]=297056&page[offset]=0&page[limit]=1000 https://vocms0185.cern.ch/agg/api/v1/lumisections/?fields=lumisection_number,delivered_lumi,pileup,recorded_lumi&filter[run_number][EQ]=297056&page[offset]=0&page[limit]=1000 https://vocms0185.cern.ch/agg/api/v1/lumisections/?fields=lumisection_number,delivered_lumi,pileup,recorded_lumi&filter[run_number][EQ]=297289&page[offset]=0&page[limit]=1000 https://vocms0185.cern.ch/agg/api/v1/lumisections/?fields=lumisection_number,delivered_lumi,pileup,recorded_lumi&filter[run_number][EQ]=297289&page[offset]=0&page[limit]=1000 ### make plots of OMS info # plots for good run ls_good = get_oms_response_attribute(ls_info_good,'lumisection_number') pileup_good = get_oms_response_attribute(ls_info_good,'pileup') lumi_del_good = get_oms_response_attribute(ls_info_good,'delivered_lumi') lumi_rec_good = get_oms_response_attribute(ls_info_good,'recorded_lumi') dcson_good = jsonu.isdcson( [oms_target_run_good]*len(ls_good), ls_good ) pu.plot_hists([pileup_good], colorlist=['b'], labellist=['pileup'], xlims=(ls_good[0],ls_good[-1]), title='pileup for good run', xaxtitle='lumisection number', yaxtitle='pileup', bkgcolor=dcson_good,bkgcmap='cool') pu.plot_hists([lumi_rec_good,lumi_del_good], colorlist=['g','b'], labellist=['recorded luminosity','delivered luminosity'], xlims=(ls_good[0],ls_good[-1]), title='luminosity for good run', xaxtitle='lumisection number', yaxtitle='luminosity', bkgcolor=dcson_good,bkgcmap='cool') ls_bad = get_oms_response_attribute(ls_info_bad,'lumisection_number') pileup_bad = get_oms_response_attribute(ls_info_bad,'pileup') lumi_del_bad = get_oms_response_attribute(ls_info_bad,'delivered_lumi') lumi_rec_bad = get_oms_response_attribute(ls_info_bad,'recorded_lumi') dcson_bad = jsonu.isdcson( [oms_target_run_bad]*len(ls_bad), ls_bad ) pu.plot_hists([pileup_bad], colorlist=['b'], labellist=['pileup'], xlims=(ls_bad[0],ls_bad[-1]), title='pileup for bad run', xaxtitle='lumisection number', yaxtitle='pileup', bkgcolor=dcson_bad,bkgcmap='cool') pu.plot_hists([lumi_rec_bad,lumi_del_bad], colorlist=['g','b'], labellist=['recorded luminosity','delivered luminosity'], xlims=(ls_bad[0],ls_bad[-1]), title='luminosity for bad run', xaxtitle='lumisection number', yaxtitle='luminosity', bkgcolor=dcson_bad,bkgcmap='cool') (<Figure size 432x288 with 2 Axes>, <AxesSubplot:title={'center':'luminosity for bad run'}, xlabel='lumisection number', ylabel='luminosity'>) ### plot particular lumisections based on the OMS info above lsnbs_tocheck = list(range(25,40)) idx_oms = [np.where(np.array(ls_good)==lsnb)[0][0] for lsnb in lsnbs_tocheck] idx_hist = [np.where(lsnbs_good==lsnb)[0][0] for lsnb in lsnbs_tocheck] histlist = [hists_good[i] for i in idx_hist] subtitles = ['lumisection: {}, pileup: {}'.format(lsnbs_tocheck[i],pileup_good[idx_oms[i]]) for i in range(len(lsnbs_tocheck))] _ = pu.plot_hists_2d(histlist, subtitles=subtitles)","title":"Two-dimensional histogram classification based on non-negative matrix factorization"},{"location":"tutorials/plot_histograms/","text":"Plot the histograms for general investigation and visual inspection Functionality for selecting a single run and plotting all lumisections belonging to that run. Functionality for plotting the moments of the distributions as a function of LS number. ### imports # external modules import sys import os import numpy as np import importlib # local modules sys.path.append('../utils') import dataframe_utils as dfu import plot_utils as pu import hist_utils as hu importlib.reload(dfu) importlib.reload(pu) importlib.reload(hu) sys.path.append('../src') import DataLoader importlib.reload(DataLoader) <module 'DataLoader' from '/eos/home-l/llambrec/SWAN_projects/ML4DQM-DC/tutorials/../src/DataLoader.py'> ### read the data # note: this cell assumes you have a csv file stored at the specified location, # containing only histograms of the specified type; # see the tutorial read_and_write_data for examples on how to create such files! histname = 'chargeInner_PXLayer_2' filename = 'DF2017_'+histname+'.csv' datadir = '../data' dloader = DataLoader.DataLoader() df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) # select a single run runs = dfu.get_runs(df) print('number of runs: '+str(len(runs))) #print(runs) # uncomment this to see a printed list of available runs runnbs = [305351] # you can also add multiple runs to the list to plot them all together df = dfu.select_runs(df,runnbs) # select DCS-bit on data #df = dfu.select_dcson(df) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_2.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. raw input data shape: (225954, 102) number of runs: 599 # make a plot of the histograms xmin = df.at[0,'Xmin'] xmax = df.at[0,'Xmax'] nbins = df.at[0,'Xbins'] (values,_,ls) = dfu.get_hist_values(df) # (note: get_hist_values returns the raw histograms as stored in the dataframe; # check out utils/hist_utils.py/preparedatafromdf for more advanced data loading, e.g. normalizing) print('shape of histogram array: '+str(values.shape)) # just plot all the histograms: pu.plot_hists_multi(values,xlims=(xmin,xmax)) # plot the histograms with a color according to their lumisection number: pu.plot_hists_multi(values,colorlist=ls,xlims=(xmin,xmax)) # same as before but normalizing each histogram: pu.plot_hists_multi(hu.normalizehists(values),colorlist=ls,xlims=(xmin,xmax)) shape of histogram array: (874, 102) (<Figure size 432x288 with 2 Axes>, <AxesSubplot:>) # select a single lumisection and plot it on top of all the other lumisections lsnumber = 869 pu.plot_anomalous(values,ls,highlight=lsnumber,hrange=10) (<Figure size 432x288 with 2 Axes>, <AxesSubplot:>) # make a plot of the moments of the (normalized) histograms # use xmin = 0 and xmax = 1 as a kind of normalization hists = values[:,1:-1] bins = np.linspace(0,1,num=hists.shape[1],endpoint=True) moments = hu.histmoments(bins,hists,orders=[1,2,3]) _ = pu.plot_moments(moments,ls,(0,1))","title":"plot_histograms"},{"location":"tutorials/plot_histograms_2d/","text":"Plot the histograms for general investigation and visual inspection For 2D histograms (as opposed to plot_histograms.ipynb which is for 1D histograms only) ### imports # external modules import sys import os import numpy as np import matplotlib.pyplot as plt import importlib # local modules sys.path.append('../utils') import dataframe_utils as dfu import plot_utils as pu import hist_utils as hu importlib.reload(dfu) importlib.reload(pu) importlib.reload(hu) sys.path.append('../src') import DataLoader importlib.reload(DataLoader) <module 'DataLoader' from '/eos/home-l/llambrec/SWAN_projects/ML4DQM-DC/tutorials/../src/DataLoader.py'> ### read the data # note: this cell assumes you have a csv file stored at the specified location, # containing only histograms of the specified type; # see the tutorial read_and_write_data for examples on how to create such files! histname = 'clusterposition_zphi_ontrack_PXLayer_1' filename = 'DF2017B_'+histname+'_run297289.csv' datadir = '../data' dloader = DataLoader.DataLoader() df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) # select a single run #runs = dfu.get_runs(df) #print('number of runs: '+str(len(runs))) #print(runs) # uncomment this to see a printed list of available runs #runnbs = [297056] # you can also add multiple runs to the list to plot them all together #df = dfu.select_runs(df, runnbs) # select DCS-bit on data df = dfu.select_dcson(df) print('number of selected lumisections: '+str(len(df))) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017B_clusterposition_zphi_ontrack_PXLayer_1_run297289.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 15 rows and 14 columns. raw input data shape: (15, 202, 302) number of selected lumisections: 15 # extract the histograms as a numpy array from the dataframe (histograms,runnbs,lsnbs) = dfu.get_hist_values(df) print('shape of histogram array: {}'.format(histograms.shape)) print('shape of run number array: {}'.format(runnbs.shape)) print('shape of lumisection number array: {}'.format(lsnbs.shape)) # additional info, not needed to get the histograms but may contain useful info xmin = df.at[0,'Xmin'] xmax = df.at[0,'Xmax'] nxbins = df.at[0,'Xbins'] ymin = df.at[0,'Ymin'] ymax = df.at[0,'Ymax'] nybins = df.at[0,'Ybins'] shape of histogram array: (15, 202, 302) shape of run number array: (15,) shape of lumisection number array: (15,) # make a plot of a single histogram index = 10 hist = histograms[index] pu.plot_hist_2d(hist,title='a histogram title',xaxtitle='x axis title',yaxtitle='y axis title') (<Figure size 432x288 with 2 Axes>, <AxesSubplot:title={'center':'a histogram title'}, xlabel='x axis title', ylabel='y axis title'>) # make a plot of multiple histograms next to each other _ = pu.plot_hists_2d(histograms[:9],ncols=3) # make a gif showing the evolution over time figname = 'temp_gif.gif' titles = [] for lsnb,runnb in zip(lsnbs,runnbs): titles.append('run {}, lumisection {}'.format(runnb,lsnb)) pu.plot_hists_2d_gif(histograms,titles=titles,figname=figname) # note: to view the result, you can directly open the file saved as figname in swan, it will show the animated gif # (or you can download it and view it with whatever gif viewer you normally use) # cropping, rebinning, normalization and averaging print('original histograms:') _ = pu.plot_hists_2d(histograms[:4],ncols=4) plt.show() # cropping: # (note: here you can also just manually slice the array, but the dedicated function shown here might be useful in more involved workflows) print(histograms.shape) modhists = hu.crophists(histograms,[slice(1,-1,None),slice(1,-1,None)]) # remove under- and overflow bins print(modhists.shape) modhists = hu.crophists(modhists,[slice(None,None,None),slice(80,220,None)]) # cut out uninteresting parts print(modhists.shape) print('cropped histograms:') _ = pu.plot_hists_2d(modhists[:4],ncols=4) plt.show() # rebinning: print(modhists.shape) modhists = hu.rebinhists(modhists,(2,2)) print(modhists.shape) print('rebinned histograms:') _ = pu.plot_hists_2d(modhists[:4],ncols=4) plt.show() # normalizing: modhists = hu.normalizehists(modhists) print('normalized histograms:') _ = pu.plot_hists_2d(modhists[:4],ncols=4) plt.show() # averaging avghists = hu.averagehists(modhists,1) print('average histogram(s):') _ = pu.plot_hists_2d(avghists[:min(4,len(avghists))],ncols=4) plt.show() original histograms: (15, 202, 302) (15, 200, 300) (15, 200, 140) cropped histograms: (15, 200, 140) (15, 100, 70) rebinned histograms: normalized histograms: average histogram(s): # equivalent way to obtain the same histograms but with less typing (histograms) = hu.preparedatafromdf(df, cropslices=[slice(1,-1,None),slice(81,221,None)], rebinningfactor=(2,2), donormalize=True, doplot=False) _ = pu.plot_hists_2d(histograms[:4],ncols=4)","title":"plot_histograms_2d"},{"location":"tutorials/plot_histograms_loop/","text":"Plot histograms in a loop, making a plot per run This allows to relatively quickly scan all runs or a collection of runs for either general characteristics of the histograms under investigation or spot anomalies by eye ### imports # external modules import sys import os import numpy as np import matplotlib.pyplot as plt import importlib # internal modules sys.path.append('../utils') import hist_utils as hu import dataframe_utils as dfu import plot_utils as pu import clustering_utils as cu importlib.reload(hu) importlib.reload(dfu) importlib.reload(pu) importlib.reload(cu) sys.path.append('../src') import DataLoader importlib.reload(DataLoader) <module 'DataLoader' from '/eos/home-l/llambrec/SWAN_projects/ML4DQM-DC/tutorials/../src/DataLoader.py'> # global settings plot_type1 = True plot_type2 = True plot_moment = True # read the data # note: this cell assumes you have a csv file stored at the specified location, # containing only histograms of the specified type; # see the tutorial read_and_write_data for examples on how to create such files! histname = 'chargeInner_PXLayer_2' filename = 'DF2017_'+histname+'.csv' datadir = '../data' dloader = DataLoader.DataLoader() df = dloader.get_dataframe_from_file( os.path.join(datadir, filename) ) print('raw input data shape: {}'.format( dfu.get_hist_values(df)[0].shape )) # first select a set of reference histograms (for plot type 2) # note: depending on the type of histogram you are looking at, the runs hard-coded below might not be good reference runs at all! # these runs are chosen just to show the principle of how to do a selection and make the plots shown below. refhists = hu.preparedatafromdf(dfu.select_dcson(dfu.select_runs(df,[297056,297177,301449])),donormalize=True) # filter the data #df = dfu.select_golden(df) #df = dfu.select_notgolden(df) df = dfu.select_dcson(df) #df = dfu.select_dcsoff(df) print('filtered number of LS: '+str(len(df))) # start loop over runs runs = dfu.get_runs(df) print('number of runs: '+str(len(runs))) runs = runs[:10] print('will loop over following runs: '+str(runs)) for run in runs: print('run '+str(run)) dfr = dfu.select_runs(df,[run]) # get histograms (hists,_,ls) = dfu.get_hist_values(dfr) # plot type 1 if plot_type1: pu.plot_hists_multi(hists.tolist(),colorlist=ls) plt.show() # plot type 2 if plot_type2: normhists = hu.normalizehists(hists) pu.plot_sets([refhists,normhists],colorlist=['blue','red'],labellist=['reference runs','this run'],transparencylist=[0.1,1.]) plt.show() # get moments if plot_moment: nmoments = 3 moments = np.zeros((len(hists),nmoments)) xmin = 0. # some sort of normalization xmax = 1. # some sort of normalization nbins = hists.shape[1] binwidth = (xmax-xmin)/nbins bins = np.linspace(xmin+binwidth/2,xmax-binwidth/2,num=nbins,endpoint=True) for i in range(1,nmoments+1): moments[:,i-1] = hu.moment(bins,hists,i) pu.plot_moments(moments,ls,(0,1)) dists = np.zeros(len(ls)) for i in range(len(ls)): dists[i] = cu.avgnndist(moments,i,2) pu.plot_distance(dists,ls) plt.show() INFO in DataLoader.get_dataframe_from_file: loading dataframe from file ../data/DF2017_chargeInner_PXLayer_2.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 225954 rows and 16 columns. raw input data shape: (225954, 102) filtered number of LS: 215144 number of runs: 594 will loop over following runs: [297047, 297048, 297049, 297050, 297056, 297057, 297099, 297100, 297101, 297113] run 297047 run 297048 run 297049 run 297050 run 297056 run 297057 run 297099 run 297100 run 297101 run 297113","title":"plot_histograms_loop"},{"location":"tutorials/read_and_write_data/","text":"Basic reading and writing of csv files as a first data processing This script has two functions: Serve as an example on the usage of the DataLoader class. Put the raw input files in a more workable format (see more info below). This script starts from the raw csv files provided by central DQM as an ultimate input. These files are difficult to work with since they contain a fixed number of lines, not grouped by e.g. run number, and they contain a large number of histogram types together. This script (of which basically all the functionality is in the 'utils' folder, interfaced by the DataLoader class) puts them into a more useful form, i.e. one file per histogram type and per year, containing all runs and lumisections for that type for that year. It might be a good idea to run this code, where you change the histogram types to the ones that you intend to use in your study. Options are also available (although not shown in this small tutorial) to make files per era instead of per year, if you prefer that. For more information, check the documentation of src/DataLoader, utils/csv_utils and utils/dataframe_utils! See also the comments in the code below for some more explanation. ### imports # external modules import sys import os import importlib # local modules sys.path.append('../utils') import csv_utils as csvu import dataframe_utils as dfu importlib.reload(csvu) importlib.reload(dfu) sys.path.append('../src') import DataLoader importlib.reload(DataLoader) <module 'DataLoader' from '/eos/home-l/llambrec/SWAN_projects/ML4DQM-DC/tutorials/../src/DataLoader.py'> # find csv files for a given data-taking year and set of eras # settings year = '2017' # data-taking year eras = ['B'] # list of eras dim = 1 # dimension of histograms (1 or 2) # create a DataLoader instance dloader = DataLoader.DataLoader() # get the default directories where the data are stored # (this requires access to the /eos filesystem!) datadirs = dloader.get_default_data_dirs( year=year, eras=eras, dim=dim ) print('some example data directories:') print(datadirs[:10]) # get the csv files located in those directories csvfiles = dloader.get_csv_files_in_dirs( datadirs ) print('number of csv files: {}'.format(len(csvfiles))) # read an example csv file csvfile = csvfiles[0] df = dloader.get_dataframe_from_file(csvfile) # uncomment the following two lines to get a printout of the dataframe before any further processing. # comment them back again to have a better view of the rest of the printouts in this cell. print('example data frame:') print(df) some example data directories: ['/eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete'] number of csv files: 33 INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_1.csv... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 100000 rows and 15 columns. example data frame: fromrun fromlumi hname fromrun.1 \\ 0 297047 25 goodvtxNbr 297047 1 297047 25 adc_PXLayer_1 297047 2 297047 25 adc_PXLayer_2 297047 3 297047 25 adc_PXLayer_3 297047 4 297047 25 adc_PXLayer_4 297047 ... ... ... ... ... 99995 299318 46 NumberOfRecHitsPerTrack_lumiFlag_GenTk 299318 99996 299318 46 NumberOfTracks_lumiFlag_GenTk 299318 99997 299318 46 Chi2oNDF_lumiFlag_GenTk 299318 99998 299318 46 NumberOfRecHitsPerTrack_lumiFlag_GenTk 299318 99999 299318 46 NumberOfTracks_lumiFlag_GenTk 299318 fromlumi.1 metype hname.1 \\ 0 25 3 goodvtxNbr 1 25 3 adc_PXLayer_1 2 25 3 adc_PXLayer_2 3 25 3 adc_PXLayer_3 4 25 3 adc_PXLayer_4 ... ... ... ... 99995 46 3 NumberOfRecHitsPerTrack_lumiFlag_GenTk 99996 46 3 NumberOfTracks_lumiFlag_GenTk 99997 46 3 Chi2oNDF_lumiFlag_GenTk 99998 46 3 NumberOfRecHitsPerTrack_lumiFlag_GenTk 99999 46 3 NumberOfTracks_lumiFlag_GenTk histo entries Xmax \\ 0 [0, 142, 0, 0, 0, 0, 0, 1, 0, 0, 6, 9, 5, 13, ... 2658 79.5 1 [0, 976046, 94512, 59260, 184038, 96687, 28368... 26035355 255.5 2 [0, 7292, 95884, 227670, 754195, 1214688, 1830... 19290359 255.5 3 [0, 12915, 89363, 226436, 771693, 1181594, 169... 15075285 255.5 4 [0, 8729, 121957, 280897, 778806, 1076284, 138... 12271474 255.5 ... ... ... ... 99995 [0, 0, 0, 0, 16067, 17959, 11751, 9948, 5799, ... 181198 39.5 99996 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... 294 2999.5 99997 [0, 4625, 32264, 5932, 1242, 436, 254, 144, 11... 45450 79.5 99998 [0, 0, 0, 0, 3117, 1223, 1089, 1190, 660, 812,... 45450 39.5 99999 [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 4, 2, ... 294 2999.5 Xmin Xbins Ymax Ymin Ybins 0 -0.5 80 1 0 1 1 -0.5 32 1 0 1 2 -0.5 32 1 0 1 3 -0.5 32 1 0 1 4 -0.5 32 1 0 1 ... ... ... ... ... ... 99995 -0.5 40 1 0 1 99996 -0.5 600 1 0 1 99997 -0.5 80 1 0 1 99998 -0.5 40 1 0 1 99999 -0.5 600 1 0 1 [100000 rows x 15 columns] # select a specific type of histogram histname = 'chargeInner_PXLayer_1' # option 1: use the already loaded dataframe dftest = dfu.select_histnames(df, [histname]) # option 2: directly load only the needed histograms df = dloader.get_dataframe_from_file( csvfile, histnames=[histname]) # compare the output print(dftest) print(df) INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_1.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: sorting the dataframe... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. fromrun fromlumi hname fromrun.1 fromlumi.1 metype \\ 0 297047 25 chargeInner_PXLayer_1 297047 25 3 1 297047 26 chargeInner_PXLayer_1 297047 26 3 2 297047 27 chargeInner_PXLayer_1 297047 27 3 3 297047 28 chargeInner_PXLayer_1 297047 28 3 4 297050 166 chargeInner_PXLayer_1 297050 166 3 .. ... ... ... ... ... ... 857 299185 110 chargeInner_PXLayer_1 299185 110 3 858 299185 111 chargeInner_PXLayer_1 299185 111 3 859 299185 112 chargeInner_PXLayer_1 299185 112 3 860 299185 113 chargeInner_PXLayer_1 299185 113 3 861 299318 46 chargeInner_PXLayer_1 299318 46 3 hname.1 histo \\ 0 chargeInner_PXLayer_1 [0, 59, 360, 653, 1002, 1501, 1986, 2467, 2980... 1 chargeInner_PXLayer_1 [0, 70, 321, 691, 1110, 1523, 2083, 2632, 3093... 2 chargeInner_PXLayer_1 [0, 74, 356, 721, 1156, 1645, 2207, 2685, 3302... 3 chargeInner_PXLayer_1 [0, 51, 281, 611, 965, 1337, 1831, 2351, 2624,... 4 chargeInner_PXLayer_1 [0, 24, 190, 393, 495, 583, 635, 690, 720, 754... .. ... ... 857 chargeInner_PXLayer_1 [0, 38, 199, 368, 545, 680, 836, 917, 989, 117... 858 chargeInner_PXLayer_1 [0, 40, 195, 335, 489, 633, 777, 841, 1026, 11... 859 chargeInner_PXLayer_1 [0, 33, 213, 337, 571, 676, 785, 846, 1032, 11... 860 chargeInner_PXLayer_1 [0, 16, 50, 76, 109, 149, 159, 215, 236, 262, ... 861 chargeInner_PXLayer_1 [0, 26, 179, 315, 399, 519, 545, 616, 616, 721... entries Xmax Xmin Xbins Ymax Ymin Ybins 0 198872 80000.0 0.0 100 1 0 1 1 202908 80000.0 0.0 100 1 0 1 2 209451 80000.0 0.0 100 1 0 1 3 176162 80000.0 0.0 100 1 0 1 4 151349 80000.0 0.0 100 1 0 1 .. ... ... ... ... ... ... ... 857 177394 80000.0 0.0 100 1 0 1 858 171434 80000.0 0.0 100 1 0 1 859 175360 80000.0 0.0 100 1 0 1 860 40103 80000.0 0.0 100 1 0 1 861 81988 80000.0 0.0 100 1 0 1 [862 rows x 15 columns] fromrun fromlumi hname fromrun.1 fromlumi.1 metype \\ 0 297047 25 chargeInner_PXLayer_1 297047 25 3 1 297047 26 chargeInner_PXLayer_1 297047 26 3 2 297047 27 chargeInner_PXLayer_1 297047 27 3 3 297047 28 chargeInner_PXLayer_1 297047 28 3 4 297050 166 chargeInner_PXLayer_1 297050 166 3 .. ... ... ... ... ... ... 857 299185 110 chargeInner_PXLayer_1 299185 110 3 858 299185 111 chargeInner_PXLayer_1 299185 111 3 859 299185 112 chargeInner_PXLayer_1 299185 112 3 860 299185 113 chargeInner_PXLayer_1 299185 113 3 861 299318 46 chargeInner_PXLayer_1 299318 46 3 hname.1 histo \\ 0 chargeInner_PXLayer_1 [0, 59, 360, 653, 1002, 1501, 1986, 2467, 2980... 1 chargeInner_PXLayer_1 [0, 70, 321, 691, 1110, 1523, 2083, 2632, 3093... 2 chargeInner_PXLayer_1 [0, 74, 356, 721, 1156, 1645, 2207, 2685, 3302... 3 chargeInner_PXLayer_1 [0, 51, 281, 611, 965, 1337, 1831, 2351, 2624,... 4 chargeInner_PXLayer_1 [0, 24, 190, 393, 495, 583, 635, 690, 720, 754... .. ... ... 857 chargeInner_PXLayer_1 [0, 38, 199, 368, 545, 680, 836, 917, 989, 117... 858 chargeInner_PXLayer_1 [0, 40, 195, 335, 489, 633, 777, 841, 1026, 11... 859 chargeInner_PXLayer_1 [0, 33, 213, 337, 571, 676, 785, 846, 1032, 11... 860 chargeInner_PXLayer_1 [0, 16, 50, 76, 109, 149, 159, 215, 236, 262, ... 861 chargeInner_PXLayer_1 [0, 26, 179, 315, 399, 519, 545, 616, 616, 721... entries Xmax Xmin Xbins Ymax Ymin Ybins 0 198872 80000.0 0.0 100 1 0 1 1 202908 80000.0 0.0 100 1 0 1 2 209451 80000.0 0.0 100 1 0 1 3 176162 80000.0 0.0 100 1 0 1 4 151349 80000.0 0.0 100 1 0 1 .. ... ... ... ... ... ... ... 857 177394 80000.0 0.0 100 1 0 1 858 171434 80000.0 0.0 100 1 0 1 859 175360 80000.0 0.0 100 1 0 1 860 40103 80000.0 0.0 100 1 0 1 861 81988 80000.0 0.0 100 1 0 1 [862 rows x 15 columns] # do some printouts for the example dataframe loaded in previous cell print('--- available runs present in this file: ---') for r in dfu.get_runs(df): print(r) print('--- available histogram types in this file ---') for h in dfu.get_histnames(df): print(h) --- available runs present in this file: --- 297047 297050 297056 297057 297113 297114 297169 297170 297175 297176 297177 297178 297282 297283 297424 297425 297429 297432 297435 297467 299149 299178 299180 299183 299184 299185 299318 --- available histogram types in this file --- chargeInner_PXLayer_1 # main reformatting of input csv files into smaller files, # with one type of histogram each (for a full data-taking year) # note: this cell can take quite a while to run! importlib.reload(DataLoader) # settings outputdir = '../data_test' histnames = ([ 'chargeInner_PXLayer_1' ]) year = '2017' dim = 1 # load all input files dloader = DataLoader.DataLoader() csvfiles = dloader.get_default_csv_files( year=year, dim=dim ) df = dloader.get_dataframe_from_files( csvfiles, histnames=histnames ) # loop over histnames and write one file per histogram type for histname in histnames: thisdf = dfu.select_histnames(df, [histname]) outputfile = 'DF_'+year+'_'+histname+'.csv' dloader.write_dataframe_to_file( thisdf, os.path.join(outputdir,outputfile) ) INFO in DataLoader.get_dataframe_from_files: reading and merging 264 files... INFO in DataLoader.get_dataframe_from_files: now processing file 1 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_1.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 2 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_2.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 3 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_3.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 4 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_4.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 5 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_5.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 6 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_6.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 7 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_7.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 870 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 8 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_8.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 9 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_9.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 10 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_10.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 11 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_11.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 857 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 12 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_12.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 852 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 13 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_13.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 14 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_14.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 15 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_15.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 854 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 16 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_16.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 17 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_17.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 18 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_18.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 857 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 19 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_19.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 20 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_20.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 21 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_21.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 857 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 22 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_22.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 857 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 23 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_23.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 24 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_24.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 25 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_25.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 26 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_26.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 856 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 27 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_27.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 861 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 28 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_28.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 29 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_29.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 30 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_30.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 857 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 31 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_31.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 857 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 32 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_32.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 33 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_33.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 799 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 34 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_1.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 864 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 35 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_2.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 36 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_3.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 37 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_4.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 38 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_5.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 39 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_6.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 40 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_7.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 41 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_8.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 861 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 42 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_9.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 861 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 43 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_10.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 44 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_11.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 45 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_12.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 46 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_13.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 47 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_14.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 48 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_15.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 864 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 49 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_16.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 50 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_17.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 51 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_18.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 52 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_19.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 864 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 53 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_20.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 54 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_21.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 861 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 55 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_22.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 864 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 56 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_23.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 57 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_24.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 58 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_25.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 870 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 59 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_26.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 60 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_27.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 61 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_28.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 62 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_29.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 63 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_30.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 64 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_31.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 65 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_32.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 66 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_33.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 67 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_34.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 68 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_35.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 861 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 69 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_36.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 70 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_37.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 71 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_38.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 72 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_39.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 73 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_40.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 74 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_41.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 861 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 75 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_42.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 76 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_43.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 77 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_44.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 861 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 78 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_45.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 79 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_46.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 80 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_47.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 81 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_48.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 864 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 82 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_49.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 83 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_50.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 856 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 84 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_51.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 85 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_52.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 86 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_53.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 87 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_54.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 88 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_55.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 864 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 89 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_56.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 864 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 90 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_57.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 91 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_58.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 870 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 92 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_59.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 93 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_60.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 94 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_61.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 871 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 95 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_62.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 857 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 96 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_63.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 97 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_64.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 98 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_65.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 99 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_66.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 100 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_67.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 101 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_68.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 102 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_69.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 103 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_70.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 104 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_71.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 864 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 105 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017C_1D_Complete/ZeroBias_2017C_DataFrame_1D_72.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 191 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 106 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_1.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 107 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_2.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 108 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_3.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 109 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_4.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 110 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_5.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 871 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 111 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_6.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 872 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 112 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_7.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 873 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 113 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_8.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 873 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 114 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_9.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 872 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 115 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_10.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 116 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_11.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 870 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 117 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_12.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 872 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 118 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_13.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 870 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 119 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_14.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 120 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_15.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 121 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_16.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 122 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_17.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 864 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 123 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_18.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 124 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_19.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 125 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_20.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 126 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_21.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 871 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 127 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_22.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 870 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 128 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_23.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 129 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_24.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 130 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_25.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 131 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_26.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 870 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 132 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_27.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 133 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_28.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 134 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_29.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 135 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_30.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 136 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_31.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 137 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_32.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 138 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_33.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 870 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 139 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017D_1D_Complete/ZeroBias_2017D_DataFrame_1D_34.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 98 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 140 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_1.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 871 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 141 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_2.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 872 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 142 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_3.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 875 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 143 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_4.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 871 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 144 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_5.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 872 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 145 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_6.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 146 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_7.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 147 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_8.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 148 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_9.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 149 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_10.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 871 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 150 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_11.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 151 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_12.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 152 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_13.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 870 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 153 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_14.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 870 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 154 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_15.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 872 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 155 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_16.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 872 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 156 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_17.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 157 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_18.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 871 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 158 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_19.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 159 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_20.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 160 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_21.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 161 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_22.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 870 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 162 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_23.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 163 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_24.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 164 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_25.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 872 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 165 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_26.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 166 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_27.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 167 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_28.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 168 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_29.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 169 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_30.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 170 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_31.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 171 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_32.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 871 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 172 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_33.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 871 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 173 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_34.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 871 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 174 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_35.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 873 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 175 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_36.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 176 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_37.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 177 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_38.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 178 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_39.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 179 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_40.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 180 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_41.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 870 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 181 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_42.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 874 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 182 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_43.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 873 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 183 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_44.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 870 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 184 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_45.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 185 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_46.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 186 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_47.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 187 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_48.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 188 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_49.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 189 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_50.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 190 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_51.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 191 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017E_1D_Complete/ZeroBias_2017E_DataFrame_1D_52.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 563 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 192 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_1.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 853 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 193 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_2.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 873 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 194 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_3.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 872 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 195 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_4.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 196 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_5.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 197 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_6.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 198 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_7.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 199 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_8.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 200 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_9.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 872 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 201 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_10.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 202 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_11.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 203 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_12.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 204 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_13.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 864 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 205 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_14.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 857 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 206 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_15.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 207 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_16.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 208 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_17.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 209 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_18.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 210 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_19.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 857 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 211 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_20.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 212 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_21.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 213 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_22.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 214 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_23.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 215 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_24.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 216 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_25.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 217 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_26.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 218 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_27.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 219 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_28.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 220 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_29.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 221 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_30.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 222 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_31.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 223 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_32.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 224 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_33.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 225 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_34.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 226 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_35.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 227 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_36.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 228 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_37.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 869 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 229 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_38.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 230 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_39.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 231 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_40.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 861 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 232 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_41.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 856 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 233 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_42.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 861 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 234 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_43.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 861 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 235 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_44.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 236 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_45.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 237 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_46.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 238 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_47.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 856 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 239 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_48.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 856 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 240 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_49.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 241 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_50.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 242 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_51.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 243 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_52.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 864 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 244 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_53.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 245 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_54.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 246 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_55.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 864 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 247 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_56.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 248 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_57.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 249 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_58.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 250 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_59.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 251 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_60.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 252 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_61.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 253 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_62.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 254 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_63.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 255 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_64.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 256 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_65.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 257 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_66.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 864 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 258 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_67.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 259 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_68.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 864 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 260 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_69.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 861 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 261 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_70.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 864 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 262 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_71.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 861 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 263 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_72.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 264 of 264... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017F_1D_Complete/ZeroBias_2017F_DataFrame_1D_73.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 414 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: merging the dataframes... INFO in DataLoader.get_dataframe_from_files: sorting the dataframe... INFO in DataLoader.get_dataframe_from_files: loaded a dataframe from 264 files with 225954 rows and 15 columns. WARNING in DataLoader.write_dataframe_to_file: the output directory ../data_test does not exist yet; will try to create it. ### same as cell above, but now writing one file per era and per histogram type # settings outputdir = '../data_test' histnames = ([ 'chargeInner_PXLayer_1' ]) year = '2017' eras = ['B'] dim = 1 for era in eras: # load all input files dloader = DataLoader.DataLoader() csvfiles = dloader.get_default_csv_files( year=year, eras=[era], dim=dim ) df = dloader.get_dataframe_from_files( csvfiles, histnames=histnames ) # loop over histnames and write one file per histogram type for histname in histnames: thisdf = dfu.select_histnames(df, [histname]) outputfile = 'DF_'+year+era+'_'+histname+'.csv' dloader.write_dataframe_to_file( thisdf, os.path.join(outputdir,outputfile) ) INFO in DataLoader.get_dataframe_from_files: reading and merging 33 files... INFO in DataLoader.get_dataframe_from_files: now processing file 1 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_1.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 2 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_2.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 867 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 3 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_3.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 4 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_4.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 863 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 5 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_5.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 6 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_6.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 7 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_7.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 870 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 8 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_8.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 866 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 9 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_9.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 868 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 10 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_10.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 11 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_11.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 857 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 12 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_12.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 852 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 13 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_13.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 14 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_14.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 15 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_15.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 854 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 16 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_16.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 17 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_17.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 18 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_18.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 857 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 19 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_19.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 20 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_20.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 21 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_21.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 857 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 22 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_22.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 857 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 23 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_23.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 860 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 24 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_24.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 865 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 25 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_25.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 862 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 26 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_26.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 856 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 27 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_27.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 861 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 28 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_28.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 29 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_29.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 858 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 30 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_30.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 857 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 31 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_31.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 857 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 32 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_32.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 859 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: now processing file 33 of 33... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_1D_Complete/ZeroBias_2017B_DataFrame_1D_33.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['chargeInner_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 799 rows and 15 columns. INFO in DataLoader.get_dataframe_from_files: merging the dataframes... INFO in DataLoader.get_dataframe_from_files: sorting the dataframe... INFO in DataLoader.get_dataframe_from_files: loaded a dataframe from 33 files with 28335 rows and 15 columns. # extra: for 2D histograms, even the files per histogram type and per era might be too big to easily work with. # this cell writes even smaller files for quicker testing # settings outputdir = '../data_test' histname = 'clusterposition_zphi_ontrack_PXLayer_1' year = '2017' era = 'B' dim = 2 dloader = DataLoader.DataLoader() csvfiles = dloader.get_default_csv_files( year=year, eras=[era], dim=dim) # just pick one (or a few) csv file(s) csvfiles = [csvfiles[0]] df = dloader.get_dataframe_from_files( csvfiles, histnames=[histname] ) outputfile = 'DF'+year+era+'subset_'+histname+'.csv' dloader.write_dataframe_to_file( thisdf, os.path.join(outputdir,outputfile) ) INFO in DataLoader.get_dataframe_from_files: reading and merging 1 files... INFO in DataLoader.get_dataframe_from_files: now processing file 1 of 1... INFO in DataLoader.get_dataframe_from_file: loading dataframe from file /eos/project/c/cmsml4dc/ML_2020/UL2017_Data/DF2017B_2D_Complete/ZeroBias_2017B_DataFrame_2D_1.csv... INFO in DataLoader.get_dataframe_from_file: selecting histograms ['clusterposition_zphi_ontrack_PXLayer_1']... INFO in DataLoader.get_dataframe_from_file: loaded a dataframe with 627 rows and 12 columns. INFO in DataLoader.get_dataframe_from_files: merging the dataframes... INFO in DataLoader.get_dataframe_from_files: sorting the dataframe... INFO in DataLoader.get_dataframe_from_files: loaded a dataframe from 1 files with 627 rows and 12 columns.","title":"read_and_write_data"},{"location":"utils/","text":"Collection of utility functions","title":"README"},{"location":"utils/autoencoder_utils/","text":"autoencoder utils Utilities related to the training and evaluation of autoencoder models with keras The functionality in this script includes: - definition of loss functions (several flavours of MSE or chi-squared) - calculating and plotting ROC curves and confusion matrices - definition of very simple ready-to-use keras model architectures mseTop10 full signature: def mseTop10(y_true, y_pred) comments: MSE top 10 loss function for autoencoder training input arguments: - y_true and y_pred: two numpy arrays of equal shape, typically a histogram and its autoencoder reconstruction. if two-dimensional, the arrays are assumed to have shape (nhists,nbins)! output: - mean squared error between y_true and y_pred, where only the 10 bins with largest squared error are taken into account. if y_true and y_pred are 2D arrays, this function returns 1D array (mseTop10 for each histogram) mseTop10Raw full signature: def mseTop10Raw(y_true, y_pred) comments: same as mseTop10 but without using tf or K the version including tf or K seemed to cause randomly dying kernels, no clear reason could be found, but it was solved using this loss function instead. verified that it gives exactly the same output as the function above on some random arrays. contrary to mseTop10, this function only works for arrays with 2D shapes (so shape (nhists,nbins)), not for (nbins,). mseTopNRaw full signature: def mseTopNRaw(y_true, y_pred, n=10) comments: generalization of mseTop10Raw to any number of bins to take into account note: now generalized to also work for 2D histograms, i.e. arrays of shape (nhists,nybins,nxbins)! hence this is the most general method and preferred above mseTop10 and mseTop10Raw, which are only kept for reference input arguments: - y_true, y_pred: numpy arrays between which to calculate the mean square difference, of shape (nhists,nbins) or (nhists,nybins,nxbins) - n: number of largest elements to keep for averaging output: numpy array of shape (nhists) chiSquared full signature: def chiSquared(y_true, y_pred) comments: chi2 loss function for autoencoder training input arguments: - y_true and y_pred: two numpy arrays of equal shape, typically a histogram and its autoencoder reconstruction. if two-dimensional, the arrays are assumed to have shape (nhists,nbins)! output: - relative mean squared error between y_true and y_pred, if y_true and y_pred are 2D arrays, this function returns 1D array (chiSquared for each histogram) chiSquaredTopNRaw full signature: def chiSquaredTopNRaw(y_true, y_pred, n=10) comments: generalization of chiSquared to any number of bins to take into account note: should work for 2D histograms as well (i.e. arrays of shape (nhistograms,nybins,nxbins)), but not yet tested! input arguments: - y_true, y_pred: numpy arrays between which to calculate the mean square difference, of shape (nhists,nbins) or (nhists,nybins,nxbins) - n: number of largest elements to keep for summing output: numpy array of shape (nhists) calculate_roc full signature: def calculate_roc(scores, labels, scoreax) comments: calculate a roc curve input arguments: - scores is a 1D numpy array containing output scores of any algorithm - labels is a 1D numpy array (equally long as scores) containing labels note that 1 for signal and 0 for background is assumed! this convention is only used to define what scores belong to signal or background; the scores itself can be anything (not limited to (0,1)), as long as the target for signal is higher than the target for background - scoreax is an array of score thresholds for which to compute the signal and background efficiency, assumed to be sorted in increasing order (i.e. from loose to tight) output: - tuple of two np arrays (signal efficiency and background efficiency) get_roc full signature: def get_roc(scores, labels, mode='lin', npoints=100, doprint=False, doplot=True, doshow=True, bootstrap_samples=None, bootstrap_size=None, returneffs=False ) comments: make a ROC curve input arguments: - scores is a 1D numpy array containing output scores of any algorithm - labels is a 1D numpy array (equally long as scores) containing labels note that 1 for signal and 0 for background is assumed! this convention is only used to define what scores belong to signal or background; the scores itself can be anything (not limited to (0,1)), as long as the target for signal is higher than the target for background - mode: how to determine the points where to calculate signal and background efficiencies; options are: - 'lin': np.linspace between min and max score - 'geom': np. geomspace between min and max score - 'full': one point per score instance - npoints: number of points where to calculate the signal and background efficiencies (ignored if mode is 'full') - doprint: boolean whether to print score thresholds and corresponding signal and background efficiencies - doplot: boolean whether to make a plot - doshow: boolean whether to call plt.show - bootstrap_samples: number of bootstrap samples to assess uncertainty on ROC curve (default: no bootstrapping) - bootstrap_size: size of each bootstrap sample (default: same size as scores, i.e. full sample size) note: the bootstrapping method can be used to assess the uncertainty on the ROC curve, by recalculating it several times on samples drawn from the test set with replacement; the resulting uncertainty as calculated here does not include contributions from varying the training set! - returneffs: boolean whether to return the signal and background efficiencies returns: - if returneffs is False, only the AUC value is returned - if returneffs is True, the return type is a tuple of the form (auc,sigeff,bckeff) get_roc_from_hists full signature: def get_roc_from_hists(hists, labels, predicted_hists, mode='lin', npoints=100, doprint=False, doplot=True, plotmode='classic') comments: make a ROC curve without manually calculating the scores the output score is the mseTop10Raw between the histograms and their reconstruction - input arguments: - hists and predicted_hists are 2D numpy arrays of shape (nhistograms,nbins) - other arguments: see get_roc get_confusion_matrix full signature: def get_confusion_matrix(scores, labels, wp='maxauc', plotwp=True, true_positive_label='Good', true_negative_label='Anomalous', pred_positive_label='Predicted good', pred_negative_label='Predicted anomalous', xaxlabelsize=None, yaxlabelsize=None, textsize=None, colormap='Blues', colortitle=None) comments: plot a confusion matrix input arguments: - scores and labels: defined in the same way as for get_roc - wp: the chosen working point (i.e. any score above wp is flagged as signal, any below is flagged as background) note: wp can be a integer or float, in which case that value will be used directly, or it can be a string in which case it will be used as the 'method' argument in get_wp! - plotwp: only relevant if wp is a string (see above), in which case plotwp will be used as the 'doplot' argument in get_wp get_confusion_matrix_from_hists full signature: def get_confusion_matrix_from_hists(hists, labels, predicted_hists, msewp=None) comments: plot a confusion matrix without manually calculating the scores the output score is the mse between the histograms and their reconstruction get_wp full signature: def get_wp(scores, labels, method='maxauc', doplot=False) comments: automatically calculate a suitable working point input arguments: - scores, labels: equally long 1d numpy arrays of predictions and true labels respectively note: in all methods, the labels are assumed to be 0 (for background) or 1 (for signal)! - method: method to calculate the working point currently supported: 'maxauc' - doplot: make a plot (if a plotting method exists for the chosen method) get_wp_maxauc full signature: def get_wp_maxauc(scores, labels, doplot=False) comments: calculate the working point corresponding to maximum pseudo-AUC (i.e. maximize the rectangular area enclosed by the working point) getautoencoder full signature: def getautoencoder(input_size,arch,act=[],opt='adam',loss=mseTop10) comments: get a trainable autoencoder model input args: - input_size: size of vector that autoencoder will operate on - arch: list of number of nodes per hidden layer (excluding input and output layer) - act: list of activations per layer (default: tanh) - opt: optimizer to use (default: adam) - loss: loss function to use (defualt: mseTop10) train_simple_autoencoder full signature: def train_simple_autoencoder(hists, nepochs=-1, modelname='', batch_size=500, shuffle=False, verbose=1, validation_split=0.1, returnhistory=False ) comments: create and train a very simple keras model the model consists of one hidden layer (with half as many units as there are input bins), tanh activation, adam optimizer and mseTop10 loss. input args: - hists is a 2D numpy array of shape (nhistograms, nbins) - nepochs is the number of epochs to use (has a default value if left unspecified) - modelname is a file name to save the model in (default: model is not saved to a file) - batch_size, shuffle, verbose, validation_split: passed to keras .fit method - returnhistory: boolean whether to return the training history (e.g. for making plots) returns - if returnhistory is False, only the trained keras model is returned - if returnhistory is True, the return type is a tuple of the form (model, history) clip_scores full signature: def clip_scores( scores, margin=1., hard_thresholds=None ) comments: clip +-inf values in scores +inf values in scores will be replaced by the maximum value (exclucing +inf) plus one -inf values in scores will be replaced by the minimim value (exclucing -inf) minus one input arguments: - scores: 1D numpy array - margin: margin between maximum value (excluding inf) and where to put inf. - hard_thresholds: tuple of values for -inf, +inf (in case the min or max cannot be determined) returns - array with same length as scores with elements replaced as explained above","title":"autoencoder_utils"},{"location":"utils/autoencoder_utils/#autoencoder-utils","text":"Utilities related to the training and evaluation of autoencoder models with keras The functionality in this script includes: - definition of loss functions (several flavours of MSE or chi-squared) - calculating and plotting ROC curves and confusion matrices - definition of very simple ready-to-use keras model architectures","title":"autoencoder utils"},{"location":"utils/autoencoder_utils/#msetop10","text":"full signature: def mseTop10(y_true, y_pred) comments: MSE top 10 loss function for autoencoder training input arguments: - y_true and y_pred: two numpy arrays of equal shape, typically a histogram and its autoencoder reconstruction. if two-dimensional, the arrays are assumed to have shape (nhists,nbins)! output: - mean squared error between y_true and y_pred, where only the 10 bins with largest squared error are taken into account. if y_true and y_pred are 2D arrays, this function returns 1D array (mseTop10 for each histogram)","title":"mseTop10"},{"location":"utils/autoencoder_utils/#msetop10raw","text":"full signature: def mseTop10Raw(y_true, y_pred) comments: same as mseTop10 but without using tf or K the version including tf or K seemed to cause randomly dying kernels, no clear reason could be found, but it was solved using this loss function instead. verified that it gives exactly the same output as the function above on some random arrays. contrary to mseTop10, this function only works for arrays with 2D shapes (so shape (nhists,nbins)), not for (nbins,).","title":"mseTop10Raw"},{"location":"utils/autoencoder_utils/#msetopnraw","text":"full signature: def mseTopNRaw(y_true, y_pred, n=10) comments: generalization of mseTop10Raw to any number of bins to take into account note: now generalized to also work for 2D histograms, i.e. arrays of shape (nhists,nybins,nxbins)! hence this is the most general method and preferred above mseTop10 and mseTop10Raw, which are only kept for reference input arguments: - y_true, y_pred: numpy arrays between which to calculate the mean square difference, of shape (nhists,nbins) or (nhists,nybins,nxbins) - n: number of largest elements to keep for averaging output: numpy array of shape (nhists)","title":"mseTopNRaw"},{"location":"utils/autoencoder_utils/#chisquared","text":"full signature: def chiSquared(y_true, y_pred) comments: chi2 loss function for autoencoder training input arguments: - y_true and y_pred: two numpy arrays of equal shape, typically a histogram and its autoencoder reconstruction. if two-dimensional, the arrays are assumed to have shape (nhists,nbins)! output: - relative mean squared error between y_true and y_pred, if y_true and y_pred are 2D arrays, this function returns 1D array (chiSquared for each histogram)","title":"chiSquared"},{"location":"utils/autoencoder_utils/#chisquaredtopnraw","text":"full signature: def chiSquaredTopNRaw(y_true, y_pred, n=10) comments: generalization of chiSquared to any number of bins to take into account note: should work for 2D histograms as well (i.e. arrays of shape (nhistograms,nybins,nxbins)), but not yet tested! input arguments: - y_true, y_pred: numpy arrays between which to calculate the mean square difference, of shape (nhists,nbins) or (nhists,nybins,nxbins) - n: number of largest elements to keep for summing output: numpy array of shape (nhists)","title":"chiSquaredTopNRaw"},{"location":"utils/autoencoder_utils/#calculate95roc","text":"full signature: def calculate_roc(scores, labels, scoreax) comments: calculate a roc curve input arguments: - scores is a 1D numpy array containing output scores of any algorithm - labels is a 1D numpy array (equally long as scores) containing labels note that 1 for signal and 0 for background is assumed! this convention is only used to define what scores belong to signal or background; the scores itself can be anything (not limited to (0,1)), as long as the target for signal is higher than the target for background - scoreax is an array of score thresholds for which to compute the signal and background efficiency, assumed to be sorted in increasing order (i.e. from loose to tight) output: - tuple of two np arrays (signal efficiency and background efficiency)","title":"calculate_roc"},{"location":"utils/autoencoder_utils/#get95roc","text":"full signature: def get_roc(scores, labels, mode='lin', npoints=100, doprint=False, doplot=True, doshow=True, bootstrap_samples=None, bootstrap_size=None, returneffs=False ) comments: make a ROC curve input arguments: - scores is a 1D numpy array containing output scores of any algorithm - labels is a 1D numpy array (equally long as scores) containing labels note that 1 for signal and 0 for background is assumed! this convention is only used to define what scores belong to signal or background; the scores itself can be anything (not limited to (0,1)), as long as the target for signal is higher than the target for background - mode: how to determine the points where to calculate signal and background efficiencies; options are: - 'lin': np.linspace between min and max score - 'geom': np. geomspace between min and max score - 'full': one point per score instance - npoints: number of points where to calculate the signal and background efficiencies (ignored if mode is 'full') - doprint: boolean whether to print score thresholds and corresponding signal and background efficiencies - doplot: boolean whether to make a plot - doshow: boolean whether to call plt.show - bootstrap_samples: number of bootstrap samples to assess uncertainty on ROC curve (default: no bootstrapping) - bootstrap_size: size of each bootstrap sample (default: same size as scores, i.e. full sample size) note: the bootstrapping method can be used to assess the uncertainty on the ROC curve, by recalculating it several times on samples drawn from the test set with replacement; the resulting uncertainty as calculated here does not include contributions from varying the training set! - returneffs: boolean whether to return the signal and background efficiencies returns: - if returneffs is False, only the AUC value is returned - if returneffs is True, the return type is a tuple of the form (auc,sigeff,bckeff)","title":"get_roc"},{"location":"utils/autoencoder_utils/#get95roc95from95hists","text":"full signature: def get_roc_from_hists(hists, labels, predicted_hists, mode='lin', npoints=100, doprint=False, doplot=True, plotmode='classic') comments: make a ROC curve without manually calculating the scores the output score is the mseTop10Raw between the histograms and their reconstruction - input arguments: - hists and predicted_hists are 2D numpy arrays of shape (nhistograms,nbins) - other arguments: see get_roc","title":"get_roc_from_hists"},{"location":"utils/autoencoder_utils/#get95confusion95matrix","text":"full signature: def get_confusion_matrix(scores, labels, wp='maxauc', plotwp=True, true_positive_label='Good', true_negative_label='Anomalous', pred_positive_label='Predicted good', pred_negative_label='Predicted anomalous', xaxlabelsize=None, yaxlabelsize=None, textsize=None, colormap='Blues', colortitle=None) comments: plot a confusion matrix input arguments: - scores and labels: defined in the same way as for get_roc - wp: the chosen working point (i.e. any score above wp is flagged as signal, any below is flagged as background) note: wp can be a integer or float, in which case that value will be used directly, or it can be a string in which case it will be used as the 'method' argument in get_wp! - plotwp: only relevant if wp is a string (see above), in which case plotwp will be used as the 'doplot' argument in get_wp","title":"get_confusion_matrix"},{"location":"utils/autoencoder_utils/#get95confusion95matrix95from95hists","text":"full signature: def get_confusion_matrix_from_hists(hists, labels, predicted_hists, msewp=None) comments: plot a confusion matrix without manually calculating the scores the output score is the mse between the histograms and their reconstruction","title":"get_confusion_matrix_from_hists"},{"location":"utils/autoencoder_utils/#get95wp","text":"full signature: def get_wp(scores, labels, method='maxauc', doplot=False) comments: automatically calculate a suitable working point input arguments: - scores, labels: equally long 1d numpy arrays of predictions and true labels respectively note: in all methods, the labels are assumed to be 0 (for background) or 1 (for signal)! - method: method to calculate the working point currently supported: 'maxauc' - doplot: make a plot (if a plotting method exists for the chosen method)","title":"get_wp"},{"location":"utils/autoencoder_utils/#get95wp95maxauc","text":"full signature: def get_wp_maxauc(scores, labels, doplot=False) comments: calculate the working point corresponding to maximum pseudo-AUC (i.e. maximize the rectangular area enclosed by the working point)","title":"get_wp_maxauc"},{"location":"utils/autoencoder_utils/#getautoencoder","text":"full signature: def getautoencoder(input_size,arch,act=[],opt='adam',loss=mseTop10) comments: get a trainable autoencoder model input args: - input_size: size of vector that autoencoder will operate on - arch: list of number of nodes per hidden layer (excluding input and output layer) - act: list of activations per layer (default: tanh) - opt: optimizer to use (default: adam) - loss: loss function to use (defualt: mseTop10)","title":"getautoencoder"},{"location":"utils/autoencoder_utils/#train95simple95autoencoder","text":"full signature: def train_simple_autoencoder(hists, nepochs=-1, modelname='', batch_size=500, shuffle=False, verbose=1, validation_split=0.1, returnhistory=False ) comments: create and train a very simple keras model the model consists of one hidden layer (with half as many units as there are input bins), tanh activation, adam optimizer and mseTop10 loss. input args: - hists is a 2D numpy array of shape (nhistograms, nbins) - nepochs is the number of epochs to use (has a default value if left unspecified) - modelname is a file name to save the model in (default: model is not saved to a file) - batch_size, shuffle, verbose, validation_split: passed to keras .fit method - returnhistory: boolean whether to return the training history (e.g. for making plots) returns - if returnhistory is False, only the trained keras model is returned - if returnhistory is True, the return type is a tuple of the form (model, history)","title":"train_simple_autoencoder"},{"location":"utils/autoencoder_utils/#clip95scores","text":"full signature: def clip_scores( scores, margin=1., hard_thresholds=None ) comments: clip +-inf values in scores +inf values in scores will be replaced by the maximum value (exclucing +inf) plus one -inf values in scores will be replaced by the minimim value (exclucing -inf) minus one input arguments: - scores: 1D numpy array - margin: margin between maximum value (excluding inf) and where to put inf. - hard_thresholds: tuple of values for -inf, +inf (in case the min or max cannot be determined) returns - array with same length as scores with elements replaced as explained above","title":"clip_scores"},{"location":"utils/clustering_utils/","text":"clustering utils A collection of functions used for performing clustering tasks This collection of tools is a little deprecated at this moment but kept for reference; it contains functionality for pre-filtering the histograms in the training set based on their moments (e.g. mean, rms). Note that the functions here have not been used in a long time and might need some maintenance before they work properly again. vecdist full signature: def vecdist(moments, index) comments: calculate the vectorial distance between a set of moments input arguments: - moments: 2D numpy array of shape (ninstances,nmoments) - index: index for which instance to calculate the distance relative to the other instances returns: - a distance measure for the given index w.r.t. the other instances in 'moments' notes: - for this distance measure, the points are considered as vectors and the point at index is the origin. with respect to this origin, the average vector before index and the average vector after index are calculated. the distance is then defined as the norm of the difference of these vectors, normalized by the norms of the individual vectors. costhetadist full signature: def costhetadist(moments, index) comments: calculate the costheta distance between a set of moments input arguments: - moments: 2D numpy array of shape (ninstances,nmoments) - index: index for which instance to calculate the distance relative to the other instances returns: - a distance measure for the given index w.r.t. the other instances in 'moments' notes: - this distance measure takes the cosine of the angle between the point at index and the one at index-1 (interpreted as vectors from the origin). avgnndist full signature: def avgnndist(moments, index, nn) comments: calculate average euclidean distance to neighbouring points input arguments: - moments: 2D numpy array of shape (ninstances,nmoments) - index: index for which instance to calculate the distance relative to the other instances - nn: (half-) window size returns: - a distance measure for the given index w.r.t. the other instances in 'moments' notes: - for this distance measure, the average euclidean distance is calculated between the point at 'index' and the points at index-nn and index+nn (e.g. the nn previous and next lumisections). getavgnndist full signature: def getavgnndist(hists, nmoments, xmin, xmax, nbins, nneighbours) comments: apply avgnndist to a set of histograms filteranomalous full signature: def filteranomalous(df, nmoments=3, rmouterflow=True, rmlargest=0., doplot=True) comments: do a pre-filtering, removing the histograms with anomalous moments","title":"clustering_utils"},{"location":"utils/clustering_utils/#clustering-utils","text":"A collection of functions used for performing clustering tasks This collection of tools is a little deprecated at this moment but kept for reference; it contains functionality for pre-filtering the histograms in the training set based on their moments (e.g. mean, rms). Note that the functions here have not been used in a long time and might need some maintenance before they work properly again.","title":"clustering utils"},{"location":"utils/clustering_utils/#vecdist","text":"full signature: def vecdist(moments, index) comments: calculate the vectorial distance between a set of moments input arguments: - moments: 2D numpy array of shape (ninstances,nmoments) - index: index for which instance to calculate the distance relative to the other instances returns: - a distance measure for the given index w.r.t. the other instances in 'moments' notes: - for this distance measure, the points are considered as vectors and the point at index is the origin. with respect to this origin, the average vector before index and the average vector after index are calculated. the distance is then defined as the norm of the difference of these vectors, normalized by the norms of the individual vectors.","title":"vecdist"},{"location":"utils/clustering_utils/#costhetadist","text":"full signature: def costhetadist(moments, index) comments: calculate the costheta distance between a set of moments input arguments: - moments: 2D numpy array of shape (ninstances,nmoments) - index: index for which instance to calculate the distance relative to the other instances returns: - a distance measure for the given index w.r.t. the other instances in 'moments' notes: - this distance measure takes the cosine of the angle between the point at index and the one at index-1 (interpreted as vectors from the origin).","title":"costhetadist"},{"location":"utils/clustering_utils/#avgnndist","text":"full signature: def avgnndist(moments, index, nn) comments: calculate average euclidean distance to neighbouring points input arguments: - moments: 2D numpy array of shape (ninstances,nmoments) - index: index for which instance to calculate the distance relative to the other instances - nn: (half-) window size returns: - a distance measure for the given index w.r.t. the other instances in 'moments' notes: - for this distance measure, the average euclidean distance is calculated between the point at 'index' and the points at index-nn and index+nn (e.g. the nn previous and next lumisections).","title":"avgnndist"},{"location":"utils/clustering_utils/#getavgnndist","text":"full signature: def getavgnndist(hists, nmoments, xmin, xmax, nbins, nneighbours) comments: apply avgnndist to a set of histograms","title":"getavgnndist"},{"location":"utils/clustering_utils/#filteranomalous","text":"full signature: def filteranomalous(df, nmoments=3, rmouterflow=True, rmlargest=0., doplot=True) comments: do a pre-filtering, removing the histograms with anomalous moments","title":"filteranomalous"},{"location":"utils/csv_utils/","text":"csv utils A collection of useful basic functions for reading and processing the input csv files. Functionality includes: - reading the raw input csv files and producing more manageable csv files (grouped per histogram type). - reading csv files into pandas dataframes and writing pandas dataframes back to csv files. Note: the functionality of these utils has been absorbed into the DataLoader class, which is now the recommended way to read the data! get_data_dirs full signature: def get_data_dirs(year='2017', eras=[], dim=1) comments: yield all data directories note that the location of the data is hard-coded; this function might break for newer or later reprocessings of the data. - year is a string, either '2017' or '2018' - era is a list containing a selection of era names (default empty list = all eras) - dim is either 1 or 2 (for 1D or 2D plots) get_csv_files full signature: def get_csv_files(inputdir) comments: yields paths to all csv files in input directory note that the output paths consist of input_dir/filename this function is only meant for 1-level down searching, i.e. the .csv files listed directly under input_dir. sort_filenames full signature: def sort_filenames(filelist) comments: sort filenames in numerical order (e.g. 2 before 10) note that the number is supposed to be in ..._<number>.<extension> format read_csv full signature: def read_csv(csv_file) comments: read csv file into pandas dataframe csv_file is the path to the csv file to be read DEPRECATED, this function might be removed in the future; use DataLoader.get_dataframe_from_file instead. write_csv full signature: def write_csv(dataframe, csvfilename) comments: write a dataframe to a csv file note: just a wrapper for builtin dataframe.to_csv DEPRECATED, this function might be removed in the future; use DataLoader.write_dataframe_to_file instead. read_and_merge_csv full signature: def read_and_merge_csv(csv_files, histnames=[], runnbs=[]) comments: read and merge list of csv files into a single df csv_files is a list of paths to files to merge into a df histnames is a list of the types of histograms to keep (default: all) runnbs is a list of run numbers to keep (default: all) DEPRECATED, this function might be removed in the future; use DataLoader.get_dataframe_from_files instead. write_skimmed_csv full signature: def write_skimmed_csv(histnames, year, eras=['all'], dim=1) comments: read all available data for a given year/era and make a file per histogram type DEPRECATED, this function might be removed in the future; see tutorial read_and_write_data.ipynb for equivalent functionality. input arguments: - histnames: list of histogram names for which to make a separate file - year: data-taking year (in string format) - eras: data-taking eras for which to make a separate file (in string format) use 'all' to make a file with all eras merged, i.e. a full data taking year - dim: dimension of histograms (1 or 2), needed to retrieve the correct folder containing input files output: - one csv file per year/era and per histogram type note: this function can take quite a while to run!","title":"csv_utils"},{"location":"utils/csv_utils/#csv-utils","text":"A collection of useful basic functions for reading and processing the input csv files. Functionality includes: - reading the raw input csv files and producing more manageable csv files (grouped per histogram type). - reading csv files into pandas dataframes and writing pandas dataframes back to csv files. Note: the functionality of these utils has been absorbed into the DataLoader class, which is now the recommended way to read the data!","title":"csv utils"},{"location":"utils/csv_utils/#get95data95dirs","text":"full signature: def get_data_dirs(year='2017', eras=[], dim=1) comments: yield all data directories note that the location of the data is hard-coded; this function might break for newer or later reprocessings of the data. - year is a string, either '2017' or '2018' - era is a list containing a selection of era names (default empty list = all eras) - dim is either 1 or 2 (for 1D or 2D plots)","title":"get_data_dirs"},{"location":"utils/csv_utils/#get95csv95files","text":"full signature: def get_csv_files(inputdir) comments: yields paths to all csv files in input directory note that the output paths consist of input_dir/filename this function is only meant for 1-level down searching, i.e. the .csv files listed directly under input_dir.","title":"get_csv_files"},{"location":"utils/csv_utils/#sort95filenames","text":"full signature: def sort_filenames(filelist) comments: sort filenames in numerical order (e.g. 2 before 10) note that the number is supposed to be in ..._<number>.<extension> format","title":"sort_filenames"},{"location":"utils/csv_utils/#read95csv","text":"full signature: def read_csv(csv_file) comments: read csv file into pandas dataframe csv_file is the path to the csv file to be read DEPRECATED, this function might be removed in the future; use DataLoader.get_dataframe_from_file instead.","title":"read_csv"},{"location":"utils/csv_utils/#write95csv","text":"full signature: def write_csv(dataframe, csvfilename) comments: write a dataframe to a csv file note: just a wrapper for builtin dataframe.to_csv DEPRECATED, this function might be removed in the future; use DataLoader.write_dataframe_to_file instead.","title":"write_csv"},{"location":"utils/csv_utils/#read95and95merge95csv","text":"full signature: def read_and_merge_csv(csv_files, histnames=[], runnbs=[]) comments: read and merge list of csv files into a single df csv_files is a list of paths to files to merge into a df histnames is a list of the types of histograms to keep (default: all) runnbs is a list of run numbers to keep (default: all) DEPRECATED, this function might be removed in the future; use DataLoader.get_dataframe_from_files instead.","title":"read_and_merge_csv"},{"location":"utils/csv_utils/#write95skimmed95csv","text":"full signature: def write_skimmed_csv(histnames, year, eras=['all'], dim=1) comments: read all available data for a given year/era and make a file per histogram type DEPRECATED, this function might be removed in the future; see tutorial read_and_write_data.ipynb for equivalent functionality. input arguments: - histnames: list of histogram names for which to make a separate file - year: data-taking year (in string format) - eras: data-taking eras for which to make a separate file (in string format) use 'all' to make a file with all eras merged, i.e. a full data taking year - dim: dimension of histograms (1 or 2), needed to retrieve the correct folder containing input files output: - one csv file per year/era and per histogram type note: this function can take quite a while to run!","title":"write_skimmed_csv"},{"location":"utils/dataframe_utils/","text":"dataframe utils A collection of useful basic functions for manipulating pandas dataframes. Functionality includes (among others): - selecting DCS-bit on data or golden json data. - selecting specific runs, lumisections, or types of histograms get_histnames full signature: def get_histnames(df) comments: get a list of (unique) histogram names present in a df df is a dataframe read from an input csv file. select_histnames full signature: def select_histnames(df, histnames) comments: keep only a subset of histograms in a df histnames is a list of histogram names to keep in the df. get_runs full signature: def get_runs(df) comments: return a list of (unique) run numbers present in a df df is a dataframe read from an input csv file. select_runs full signature: def select_runs(df, runnbs) comments: keep only a subset of runs in a df runnbs is a list of run numbers to keep in the df. get_ls full signature: def get_ls(df) comments: return a list of ls numbers present in a df note that the numbers are not required to be unique! note: no check is done on the run number! select_ls full signature: def select_ls(df, lsnbs) comments: keep only a subset of lumisection numbers in a df lsnbs is a list of lumisection numbers to keep in the df. note: no check is done on the run number! get_runsls full signature: def get_runsls(df) comments: return a dictionary with runs and lumisections in a dataframe (same format as e.g. golden json) select_json full signature: def select_json(df, jsonfile) comments: keep only lumisections that are in the given json file select_runsls full signature: def select_runsls(df, jsondict) comments: equivalent to select_json but using a pre-loaded json dict instead of a json file on disk select_golden full signature: def select_golden(df) comments: keep only golden lumisections in df select_notgolden full signature: def select_notgolden(df) comments: keep all but golden lumisections in df select_dcson full signature: def select_dcson(df) comments: keep only lumisections in df that have DCS-bit on select_dcsoff full signature: def select_dcsoff(df) comments: keep only lumisections in df that have DCS-bit off select_pixelgood full signature: def select_pixelgood(df) comments: keep only lumisections in df that are in good pixel json select_pixelbad full signature: def select_pixelbad(df) comments: keep only lumisections in df that are in bad pixel json get_highstat full signature: def get_highstat(df, entries_to_bins_ratio=100) comments: return a select object of runs and ls of histograms with high statistics select_highstat full signature: def select_highstat(df, entries_to_bins_ratio=100) comments: keep only lumisection in df with high statistics get_hist_values full signature: def get_hist_values(df) comments: same as builtin \"df['histo'].values\" but convert strings to np arrays input arguments: - df: a dataframe containing histograms (assumed to be of a single type!) note: this function works for both 1D and 2D histograms, the distinction is made based on whether or not 'Ybins' is present as a column in the dataframe update: 'Ybins' is also present for 1D histograms, but has value 1! output: a tuple containing the following elements: - np array of shape (nhists,nbins) (for 1D) or (nhists,nybins,nxbins) (for 2D) - np array of run numbers of length nhists - np array of lumisection numbers of length nhists warning: no check is done to assure that all histograms are of the same type!","title":"dataframe_utils"},{"location":"utils/dataframe_utils/#dataframe-utils","text":"A collection of useful basic functions for manipulating pandas dataframes. Functionality includes (among others): - selecting DCS-bit on data or golden json data. - selecting specific runs, lumisections, or types of histograms","title":"dataframe utils"},{"location":"utils/dataframe_utils/#get95histnames","text":"full signature: def get_histnames(df) comments: get a list of (unique) histogram names present in a df df is a dataframe read from an input csv file.","title":"get_histnames"},{"location":"utils/dataframe_utils/#select95histnames","text":"full signature: def select_histnames(df, histnames) comments: keep only a subset of histograms in a df histnames is a list of histogram names to keep in the df.","title":"select_histnames"},{"location":"utils/dataframe_utils/#get95runs","text":"full signature: def get_runs(df) comments: return a list of (unique) run numbers present in a df df is a dataframe read from an input csv file.","title":"get_runs"},{"location":"utils/dataframe_utils/#select95runs","text":"full signature: def select_runs(df, runnbs) comments: keep only a subset of runs in a df runnbs is a list of run numbers to keep in the df.","title":"select_runs"},{"location":"utils/dataframe_utils/#get95ls","text":"full signature: def get_ls(df) comments: return a list of ls numbers present in a df note that the numbers are not required to be unique! note: no check is done on the run number!","title":"get_ls"},{"location":"utils/dataframe_utils/#select95ls","text":"full signature: def select_ls(df, lsnbs) comments: keep only a subset of lumisection numbers in a df lsnbs is a list of lumisection numbers to keep in the df. note: no check is done on the run number!","title":"select_ls"},{"location":"utils/dataframe_utils/#get95runsls","text":"full signature: def get_runsls(df) comments: return a dictionary with runs and lumisections in a dataframe (same format as e.g. golden json)","title":"get_runsls"},{"location":"utils/dataframe_utils/#select95json","text":"full signature: def select_json(df, jsonfile) comments: keep only lumisections that are in the given json file","title":"select_json"},{"location":"utils/dataframe_utils/#select95runsls","text":"full signature: def select_runsls(df, jsondict) comments: equivalent to select_json but using a pre-loaded json dict instead of a json file on disk","title":"select_runsls"},{"location":"utils/dataframe_utils/#select95golden","text":"full signature: def select_golden(df) comments: keep only golden lumisections in df","title":"select_golden"},{"location":"utils/dataframe_utils/#select95notgolden","text":"full signature: def select_notgolden(df) comments: keep all but golden lumisections in df","title":"select_notgolden"},{"location":"utils/dataframe_utils/#select95dcson","text":"full signature: def select_dcson(df) comments: keep only lumisections in df that have DCS-bit on","title":"select_dcson"},{"location":"utils/dataframe_utils/#select95dcsoff","text":"full signature: def select_dcsoff(df) comments: keep only lumisections in df that have DCS-bit off","title":"select_dcsoff"},{"location":"utils/dataframe_utils/#select95pixelgood","text":"full signature: def select_pixelgood(df) comments: keep only lumisections in df that are in good pixel json","title":"select_pixelgood"},{"location":"utils/dataframe_utils/#select95pixelbad","text":"full signature: def select_pixelbad(df) comments: keep only lumisections in df that are in bad pixel json","title":"select_pixelbad"},{"location":"utils/dataframe_utils/#get95highstat","text":"full signature: def get_highstat(df, entries_to_bins_ratio=100) comments: return a select object of runs and ls of histograms with high statistics","title":"get_highstat"},{"location":"utils/dataframe_utils/#select95highstat","text":"full signature: def select_highstat(df, entries_to_bins_ratio=100) comments: keep only lumisection in df with high statistics","title":"select_highstat"},{"location":"utils/dataframe_utils/#get95hist95values","text":"full signature: def get_hist_values(df) comments: same as builtin \"df['histo'].values\" but convert strings to np arrays input arguments: - df: a dataframe containing histograms (assumed to be of a single type!) note: this function works for both 1D and 2D histograms, the distinction is made based on whether or not 'Ybins' is present as a column in the dataframe update: 'Ybins' is also present for 1D histograms, but has value 1! output: a tuple containing the following elements: - np array of shape (nhists,nbins) (for 1D) or (nhists,nybins,nxbins) (for 2D) - np array of run numbers of length nhists - np array of lumisection numbers of length nhists warning: no check is done to assure that all histograms are of the same type!","title":"get_hist_values"},{"location":"utils/generate_data_2d_utils/","text":"generate data 2d utils Extension of generate_data_utils.py towards 2D histograms goodnoise_nd full signature: def goodnoise_nd(shape, fstd=None, kmaxscale=0.25, ncomponents=3) comments: generate one sample of 'good' noise consisting of fourier components generalization of goodnoise (see generate_data_utils) to arbitrary number of dimensions input args: - shape: a tuple, shape of the noise array to be sampled note: in case of 1D, a comma is needed e.g. shape = (30,) else it will be automatically parsed to int and raise an error - fstd: an array of shape given by shape argument, used for scaling of the amplitude of the noise bin-by-bin (default: no scaling). - kmaxscale: scale factor to limit maximum frequency (lower kmaxscale means smoother noise) note: can be a tuple with same length as shape, to scale differently in different dimensions. - ncomponents: number of random sines to add per dimension note: can be a tuple with same length as shape, to use a different number of components in different dimensions. output: - numpy array of shape detailed by shape argument containing the noise whitenoise_nd full signature: def whitenoise_nd(shape, fstd=None) comments: generate one sample of white noise (standard normally distributed, uncorrelated between bins) generalization of whitenoise (see generate_data_utils) to arbitrary number of dimensions input args: - shape: a tuple, shape of the noise array to be sampled note: in case of 1D, a comma is needed e.g. shape = (30,) else it will be automatically parsed to int and raise an error - fstd: an array of shape given by shape argument, used for scaling of the amplitude of the noise bin-by-bin (default: no scaling). output: - numpy array of shape detailed by shape argument containing the noise random_lico_nd full signature: def random_lico_nd(hists) comments: generate one linear combination of histograms with random coefficients in (0,1) summing to 1. generalization of random_lico (see generate_data_utils) to arbitrary number of dimensions. input args: - numpy array of shape (nhists,<arbitrary number of additional dimensions>) output: - numpy array of shape (<same dimensions as input>), containing the new histogram fourier_noise_nd full signature: def fourier_noise_nd(hists, outfilename=None, doplot=False, ntarget=None, nresamples=1, nonnegative=True, stdfactor=15., kmaxscale=0.25, ncomponents=3) comments: apply fourier noise on random histograms with simple flat amplitude scaling. generalization of fourier_noise (see generate_data_utils) to arbitrary number of dimensions. input args: - hists: numpy array of shape (nhists,<arbitrary number of dimensions>) used for seeding - outfilename: path to csv file to write results to (default: no writing) - doplot: boolean whether to make a plot of some examples (only for 2D histograms!) - ntarget: total target number of histograms (default: use nresamples instead) - nresamples: number of samples to draw per input histogram (note: ignored if ntarget is not None) - nonnegative: boolean whether to set all bins to minimum zero after applying noise - stdfactor: factor to scale magnitude of noise (larger factor = smaller noise) - kmaxscale and ncomponents: see goodnoise_nd white_noise_nd full signature: def white_noise_nd(hists, doplot=False, ntarget=None, nresamples=1, nonnegative=True, stdfactor=15.) comments: apply white noise to the histograms in hists. generalization of white_noise (see generate_data_utils) to arbitrary number of dimensions. input args: - hists: np array (nhists,<arbitrary number of dimensions>) containing input histograms - doplot: boolean whether to plot some examples (only for 2D histograms!) - ntarget: total target number of histograms (default: use nresamples instead) - nresamples: number of samples to draw per input histogram (note: ignored if ntarget is not None) - nonnegative: boolean whether to set all bins to minimum zero after applying noise - stdfactor: scaling factor of white noise amplitude (higher factor = smaller noise) resample_lico_nd full signature: def resample_lico_nd(hists, doplot=False, ntarget=None, nonnegative=True) comments: take random linear combinations of input histograms generalization of fourier_noise (see generate_data_utils) to arbitrary number of dimensions. input args: - hists: numpy array of shape (nhists,<arbitrary number of dimensions>) used for seeding - doplot: boolean whether to plot some examples (only for 2D histograms!) - ntarget: total target number of histograms (default: same as number of input histograms) - nonnegative: boolean whether to set all bins to minimum zero after applying noise note: coefficients in linear combination are always nonnegative, so this setting is superfluous is input histograms are all nonnegative","title":"generate_data_2d_utils"},{"location":"utils/generate_data_2d_utils/#generate-data-2d-utils","text":"Extension of generate_data_utils.py towards 2D histograms","title":"generate data 2d utils"},{"location":"utils/generate_data_2d_utils/#goodnoise95nd","text":"full signature: def goodnoise_nd(shape, fstd=None, kmaxscale=0.25, ncomponents=3) comments: generate one sample of 'good' noise consisting of fourier components generalization of goodnoise (see generate_data_utils) to arbitrary number of dimensions input args: - shape: a tuple, shape of the noise array to be sampled note: in case of 1D, a comma is needed e.g. shape = (30,) else it will be automatically parsed to int and raise an error - fstd: an array of shape given by shape argument, used for scaling of the amplitude of the noise bin-by-bin (default: no scaling). - kmaxscale: scale factor to limit maximum frequency (lower kmaxscale means smoother noise) note: can be a tuple with same length as shape, to scale differently in different dimensions. - ncomponents: number of random sines to add per dimension note: can be a tuple with same length as shape, to use a different number of components in different dimensions. output: - numpy array of shape detailed by shape argument containing the noise","title":"goodnoise_nd"},{"location":"utils/generate_data_2d_utils/#whitenoise95nd","text":"full signature: def whitenoise_nd(shape, fstd=None) comments: generate one sample of white noise (standard normally distributed, uncorrelated between bins) generalization of whitenoise (see generate_data_utils) to arbitrary number of dimensions input args: - shape: a tuple, shape of the noise array to be sampled note: in case of 1D, a comma is needed e.g. shape = (30,) else it will be automatically parsed to int and raise an error - fstd: an array of shape given by shape argument, used for scaling of the amplitude of the noise bin-by-bin (default: no scaling). output: - numpy array of shape detailed by shape argument containing the noise","title":"whitenoise_nd"},{"location":"utils/generate_data_2d_utils/#random95lico95nd","text":"full signature: def random_lico_nd(hists) comments: generate one linear combination of histograms with random coefficients in (0,1) summing to 1. generalization of random_lico (see generate_data_utils) to arbitrary number of dimensions. input args: - numpy array of shape (nhists,<arbitrary number of additional dimensions>) output: - numpy array of shape (<same dimensions as input>), containing the new histogram","title":"random_lico_nd"},{"location":"utils/generate_data_2d_utils/#fourier95noise95nd","text":"full signature: def fourier_noise_nd(hists, outfilename=None, doplot=False, ntarget=None, nresamples=1, nonnegative=True, stdfactor=15., kmaxscale=0.25, ncomponents=3) comments: apply fourier noise on random histograms with simple flat amplitude scaling. generalization of fourier_noise (see generate_data_utils) to arbitrary number of dimensions. input args: - hists: numpy array of shape (nhists,<arbitrary number of dimensions>) used for seeding - outfilename: path to csv file to write results to (default: no writing) - doplot: boolean whether to make a plot of some examples (only for 2D histograms!) - ntarget: total target number of histograms (default: use nresamples instead) - nresamples: number of samples to draw per input histogram (note: ignored if ntarget is not None) - nonnegative: boolean whether to set all bins to minimum zero after applying noise - stdfactor: factor to scale magnitude of noise (larger factor = smaller noise) - kmaxscale and ncomponents: see goodnoise_nd","title":"fourier_noise_nd"},{"location":"utils/generate_data_2d_utils/#white95noise95nd","text":"full signature: def white_noise_nd(hists, doplot=False, ntarget=None, nresamples=1, nonnegative=True, stdfactor=15.) comments: apply white noise to the histograms in hists. generalization of white_noise (see generate_data_utils) to arbitrary number of dimensions. input args: - hists: np array (nhists,<arbitrary number of dimensions>) containing input histograms - doplot: boolean whether to plot some examples (only for 2D histograms!) - ntarget: total target number of histograms (default: use nresamples instead) - nresamples: number of samples to draw per input histogram (note: ignored if ntarget is not None) - nonnegative: boolean whether to set all bins to minimum zero after applying noise - stdfactor: scaling factor of white noise amplitude (higher factor = smaller noise)","title":"white_noise_nd"},{"location":"utils/generate_data_2d_utils/#resample95lico95nd","text":"full signature: def resample_lico_nd(hists, doplot=False, ntarget=None, nonnegative=True) comments: take random linear combinations of input histograms generalization of fourier_noise (see generate_data_utils) to arbitrary number of dimensions. input args: - hists: numpy array of shape (nhists,<arbitrary number of dimensions>) used for seeding - doplot: boolean whether to plot some examples (only for 2D histograms!) - ntarget: total target number of histograms (default: same as number of input histograms) - nonnegative: boolean whether to set all bins to minimum zero after applying noise note: coefficients in linear combination are always nonnegative, so this setting is superfluous is input histograms are all nonnegative","title":"resample_lico_nd"},{"location":"utils/generate_data_utils/","text":"generate data utils A collection of functions for artificially creating a labeled dataset. See the function documentation below for more details on the implemented methods. Also check the tutorial generate_data.ipynb for examples! goodnoise full signature: def goodnoise(nbins, fstd=None) comments: generate one sample of 'good' noise consisting of fourier components input args: - nbins: number of bins, length of noise array to be sampled - fstd: an array of length nbins used for scaling of the amplitude of the noise bin-by-bin. output: - numpy array of length nbins containing the noise badnoise full signature: def badnoise(nbins, fstd=None) comments: generate one sample of 'bad' noise consisting of fourier components (higher frequency and amplitude than 'good' noise) input args and output: simlar to goodnoise WARNING: NOT NECESSARILY REPRESENTATIVE OF ANOMALIES TO BE EXPECTED, DO NOT USE whitenoise full signature: def whitenoise(nbins, fstd=None) comments: generate one sample of white noise (uncorrelated between bins) input args and output: similar to goodnoise random_lico full signature: def random_lico(hists) comments: generate one linear combination of histograms with random coefficients in (0,1) summing to 1 input args: - numpy array of shape (nhists,nbins), the rows of which will be linearly combined output: - numpy array of shape (nbins), containing the new histogram smoother full signature: def smoother(inarray, halfwidth=1) comments: smooth the rows of a 2D array using the 2*halfwidth+1 surrounding values. mse_correlation_vector full signature: def mse_correlation_vector(hists, index) comments: calculate mse of a histogram at given index wrt all other histograms input args: - hists: numpy array of shape (nhists,nbins) containing the histograms - index: the index (must be in (0,len(hists)-1)) of the histogram in question output: - numpy array of length nhists containing mse of the indexed histogram with respect to all other histograms WARNING: can be slow if called many times on a large collection of histograms with many bins. moments_correlation_vector full signature: def moments_correlation_vector(moments, index) comments: calculate moment distance of hist at index wrt all other hists very similar to mse_correlation_vector but using histogram moments instead of full histograms for speed-up plot_data_and_gen full signature: def plot_data_and_gen(datahists, genhists, fig=None, axs=None, datacolor='b', gencolor='b', datalabel='Histograms from data', genlabel='Artificially generated histograms') comments: plot a couple of random examples from data and generated histograms note: both are plotted in different subplots of the same figure input arguments: - datahists, genhists: numpy arrays of shape (nhists,nbins) - fig, axs: a matplotlib figure object and a list of two axes objects (if either is None, a new figure with two subplots will be created) plot_seed_and_gen full signature: def plot_seed_and_gen(seedhists, genhists, fig=None, ax=None, seedcolor='b', gencolor='g', seedlabel='Histograms from data', genlabel='Artificially generated histograms') comments: plot seed and generated histograms note: both are plotted in the same subplot input arguments: - seedhists, genhists: numpy arrays of shape (nhists,nbins) - fig, ax: a matplotlib figure object and an axes object (if either is None, a new figure will be created) plot_noise full signature: def plot_noise(noise, fig=None, ax=None, noiselabel='Examples of noise', noisecolor='b', histstd=None, histstdlabel='Variation') comments: plot histograms in noise (numpy array of shape (nhists,nbins)) input arguments: - noise: 2D numpy array of shape (nexamples,nbins) - fig, ax: a matplotlib figure object and an axes object (if either is None, a new figure will be created) - noiselabel: label for noise examples (use None to not add a legend entry for noise) - noisecolor: color for noise examples on plot - histstd: 1D numpy array of shape (nbins) displaying some order-of-magnitude allowed variation (typically some measure of per-bin variation in the input histogram(s)) - histstdlabel: label for histstd (use None to not add a legend entry for histstd) fourier_noise_on_mean full signature: def fourier_noise_on_mean(hists, outfilename='', nresamples=0, nonnegative=True, doplot=True) comments: apply fourier noise on the bin-per-bin mean histogram, with amplitude scaling based on bin-per-bin std histogram. input args: - hists: numpy array of shape (nhists,nbins) used for determining mean and std - outfilename: path to csv file to write results to (default: no writing) - nresamples: number of samples to draw (default: number of input histograms / 10) - nonnegative: boolean whether to set all bins to minimum zero after applying noise - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False MOSTLY SUITABLE AS HELP FUNCTION FOR RESAMPLE_SIMILAR_FOURIER_NOISE, NOT AS GENERATOR IN ITSELF advantages: mean histogram is almost certainly 'good' because of averaging, eliminate bad histograms disadvantages: deviations from mean are small, does not model systematic shifts by lumi. fourier_noise full signature: def fourier_noise(hists, outfilename='', nresamples=1, nonnegative=True, stdfactor=15., doplot=True) comments: apply fourier noise on random histograms with simple flat amplitude scaling. input args: - hists: numpy array of shape (nhists,nbins) used for seeding - outfilename: path to csv file to write results to (default: no writing) - nresamples: number of samples to draw per input histogram - nonnegative: boolean whether to set all bins to minimum zero after applying noise - stdfactor: factor to scale magnitude of noise (larger factor = smaller noise) - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False advantages: resampled histograms will have statistically same features as original input set disadvantages: also 'bad' histograms will be resampled if included in hists upsample_hist_set full signature: def upsample_hist_set(hists, ntarget=-1, fourierstdfactor=15., doplot=True) comments: wrapper for fourier_noise allowing for a fixed target number of histograms instead of a fixed resampling factor. useful function for quickly generating a fixed number of resampled histograms, without bothering too much about what exact resampling technique or detailed settings would be most appropriate. input arguments: - hists: input histogram set - ntarget: targetted number of resampled histograms (default: equally many as in hists) - fourierstdfactor: see fourier_noise - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False white_noise full signature: def white_noise(hists, stdfactor=15., doplot=True) comments: apply white noise to the histograms in hists. input args: - hists: np array (nhists,nbins) containing input histograms - stdfactor: scaling factor of white noise amplitude (higher factor = smaller noise) - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False resample_bin_per_bin full signature: def resample_bin_per_bin(hists, outfilename='', nresamples=0, nonnegative=True, smoothinghalfwidth=2, doplot=True) comments: do resampling from bin-per-bin probability distributions input args: - hists: np array (nhists,nbins) containing the histograms to draw new samples from - outfilename: path to csv file to write results to (default: no writing) - nresamples: number of samples to draw (default: 1/10 of number of input histograms) - nonnegative: boolean whether or not to put all bins to minimum zero after applying noise - smoothinghalfwidth: halfwidth of smoothing procedure to apply on the result (default: no smoothing) - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False advantages: no arbitrary noise modeling disadvantages: bins are considered independent, shape of historams not taken into account, does not work well on small number of input histograms, does not work well on histograms with systematic shifts resample_similar_bin_per_bin full signature: def resample_similar_bin_per_bin( allhists, selhists, outfilename='', nresamples=1, nonnegative=True, keeppercentage=1., doplot=True) comments: resample from bin-per-bin probability distributions, but only from similar looking histograms. input args: - allhists: np array (nhists,nbins) containing all available histograms (to determine mean) - selhists: np array (nhists,nbins) conataining selected histograms used as seeds (e.g. 'good' histograms) - outfilename: path of csv file to write results to (default: no writing) - nresamples: number of samples per input histogram in selhists - nonnegative: boolean whether or not to put all bins to minimum zero after applying noise - keeppercentage: percentage (between 1 and 100) of histograms in allhists to use per input histogram - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False advantages: no assumptions on shape of noise, can handle systematic shifts in histograms disadvantages: bins are treated independently from each other resample_similar_fourier_noise full signature: def resample_similar_fourier_noise( allhists, selhists, outfilename='', nresamples=1, nonnegative=True, keeppercentage=1., doplot=True) comments: apply fourier noise on mean histogram, where the mean is determined from a set of similar-looking histograms input args: - allhists: np array (nhists,nbins) containing all available histograms (to determine mean) - selhists: np array (nhists,nbins) conataining selected histograms used as seeds (e.g. 'good' histograms) - outfilename: path of csv file to write results to (default: no writing) - nresamples: number of samples per input histogram in selhists - nonnegative: boolean whether or not to put all bins to minimum zero after applying noise - keeppercentage: percentage (between 1 and 100) of histograms in allhists to use per input histogram - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False advantages: most of fourier_noise_on_mean but can additionally handle shifting histograms, apart from fourier noise, also white noise can be applied. disadvantages: does not filter out odd histograms as long as enough other odd histograms look more or less similar resample_similar_lico full signature: def resample_similar_lico( allhists, selhists, outfilename='', nresamples=1, nonnegative=True, keeppercentage=1., doplot=True) comments: take linear combinations of similar histograms input arguments: - allhists: 2D np array (nhists,nbins) with all available histograms, used to take linear combinations - selhists: 2D np array (nhists,nbins) with selected hists used for seeding (e.g. 'good' histograms) - outfilename: path to csv file to write result to (default: no writing) - nresamples: number of combinations to make per input histogram - nonnegative: boolean whether to make all final histograms nonnegative - keeppercentage: percentage (between 0. and 100.) of histograms in allhists to use per input histogram - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False advantages: no assumptions on noise disadvantages: sensitive to outlying histograms (more than with averaging) mc_sampling full signature: def mc_sampling(hists, nMC=10000 , nresamples=10, doplot=True) comments: resampling of a histogram using MC methods Drawing random points from a space defined by the range of the histogram in all axes. Points are \"accepted\" if the fall under the sampled histogram: f(x) - sampled distribution x_r, y_r -> randomly sampled point if y_r<=f(x_r), fill the new distribution at bin corresponding to x_r with weight: weight = (sum of input hist)/(#mc points accepted) this is equal to weight = (MC space volume)/(all MC points)","title":"generate_data_utils"},{"location":"utils/generate_data_utils/#generate-data-utils","text":"A collection of functions for artificially creating a labeled dataset. See the function documentation below for more details on the implemented methods. Also check the tutorial generate_data.ipynb for examples!","title":"generate data utils"},{"location":"utils/generate_data_utils/#goodnoise","text":"full signature: def goodnoise(nbins, fstd=None) comments: generate one sample of 'good' noise consisting of fourier components input args: - nbins: number of bins, length of noise array to be sampled - fstd: an array of length nbins used for scaling of the amplitude of the noise bin-by-bin. output: - numpy array of length nbins containing the noise","title":"goodnoise"},{"location":"utils/generate_data_utils/#badnoise","text":"full signature: def badnoise(nbins, fstd=None) comments: generate one sample of 'bad' noise consisting of fourier components (higher frequency and amplitude than 'good' noise) input args and output: simlar to goodnoise WARNING: NOT NECESSARILY REPRESENTATIVE OF ANOMALIES TO BE EXPECTED, DO NOT USE","title":"badnoise"},{"location":"utils/generate_data_utils/#whitenoise","text":"full signature: def whitenoise(nbins, fstd=None) comments: generate one sample of white noise (uncorrelated between bins) input args and output: similar to goodnoise","title":"whitenoise"},{"location":"utils/generate_data_utils/#random95lico","text":"full signature: def random_lico(hists) comments: generate one linear combination of histograms with random coefficients in (0,1) summing to 1 input args: - numpy array of shape (nhists,nbins), the rows of which will be linearly combined output: - numpy array of shape (nbins), containing the new histogram","title":"random_lico"},{"location":"utils/generate_data_utils/#smoother","text":"full signature: def smoother(inarray, halfwidth=1) comments: smooth the rows of a 2D array using the 2*halfwidth+1 surrounding values.","title":"smoother"},{"location":"utils/generate_data_utils/#mse95correlation95vector","text":"full signature: def mse_correlation_vector(hists, index) comments: calculate mse of a histogram at given index wrt all other histograms input args: - hists: numpy array of shape (nhists,nbins) containing the histograms - index: the index (must be in (0,len(hists)-1)) of the histogram in question output: - numpy array of length nhists containing mse of the indexed histogram with respect to all other histograms WARNING: can be slow if called many times on a large collection of histograms with many bins.","title":"mse_correlation_vector"},{"location":"utils/generate_data_utils/#moments95correlation95vector","text":"full signature: def moments_correlation_vector(moments, index) comments: calculate moment distance of hist at index wrt all other hists very similar to mse_correlation_vector but using histogram moments instead of full histograms for speed-up","title":"moments_correlation_vector"},{"location":"utils/generate_data_utils/#plot95data95and95gen","text":"full signature: def plot_data_and_gen(datahists, genhists, fig=None, axs=None, datacolor='b', gencolor='b', datalabel='Histograms from data', genlabel='Artificially generated histograms') comments: plot a couple of random examples from data and generated histograms note: both are plotted in different subplots of the same figure input arguments: - datahists, genhists: numpy arrays of shape (nhists,nbins) - fig, axs: a matplotlib figure object and a list of two axes objects (if either is None, a new figure with two subplots will be created)","title":"plot_data_and_gen"},{"location":"utils/generate_data_utils/#plot95seed95and95gen","text":"full signature: def plot_seed_and_gen(seedhists, genhists, fig=None, ax=None, seedcolor='b', gencolor='g', seedlabel='Histograms from data', genlabel='Artificially generated histograms') comments: plot seed and generated histograms note: both are plotted in the same subplot input arguments: - seedhists, genhists: numpy arrays of shape (nhists,nbins) - fig, ax: a matplotlib figure object and an axes object (if either is None, a new figure will be created)","title":"plot_seed_and_gen"},{"location":"utils/generate_data_utils/#plot95noise","text":"full signature: def plot_noise(noise, fig=None, ax=None, noiselabel='Examples of noise', noisecolor='b', histstd=None, histstdlabel='Variation') comments: plot histograms in noise (numpy array of shape (nhists,nbins)) input arguments: - noise: 2D numpy array of shape (nexamples,nbins) - fig, ax: a matplotlib figure object and an axes object (if either is None, a new figure will be created) - noiselabel: label for noise examples (use None to not add a legend entry for noise) - noisecolor: color for noise examples on plot - histstd: 1D numpy array of shape (nbins) displaying some order-of-magnitude allowed variation (typically some measure of per-bin variation in the input histogram(s)) - histstdlabel: label for histstd (use None to not add a legend entry for histstd)","title":"plot_noise"},{"location":"utils/generate_data_utils/#fourier95noise95on95mean","text":"full signature: def fourier_noise_on_mean(hists, outfilename='', nresamples=0, nonnegative=True, doplot=True) comments: apply fourier noise on the bin-per-bin mean histogram, with amplitude scaling based on bin-per-bin std histogram. input args: - hists: numpy array of shape (nhists,nbins) used for determining mean and std - outfilename: path to csv file to write results to (default: no writing) - nresamples: number of samples to draw (default: number of input histograms / 10) - nonnegative: boolean whether to set all bins to minimum zero after applying noise - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False MOSTLY SUITABLE AS HELP FUNCTION FOR RESAMPLE_SIMILAR_FOURIER_NOISE, NOT AS GENERATOR IN ITSELF advantages: mean histogram is almost certainly 'good' because of averaging, eliminate bad histograms disadvantages: deviations from mean are small, does not model systematic shifts by lumi.","title":"fourier_noise_on_mean"},{"location":"utils/generate_data_utils/#fourier95noise","text":"full signature: def fourier_noise(hists, outfilename='', nresamples=1, nonnegative=True, stdfactor=15., doplot=True) comments: apply fourier noise on random histograms with simple flat amplitude scaling. input args: - hists: numpy array of shape (nhists,nbins) used for seeding - outfilename: path to csv file to write results to (default: no writing) - nresamples: number of samples to draw per input histogram - nonnegative: boolean whether to set all bins to minimum zero after applying noise - stdfactor: factor to scale magnitude of noise (larger factor = smaller noise) - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False advantages: resampled histograms will have statistically same features as original input set disadvantages: also 'bad' histograms will be resampled if included in hists","title":"fourier_noise"},{"location":"utils/generate_data_utils/#upsample95hist95set","text":"full signature: def upsample_hist_set(hists, ntarget=-1, fourierstdfactor=15., doplot=True) comments: wrapper for fourier_noise allowing for a fixed target number of histograms instead of a fixed resampling factor. useful function for quickly generating a fixed number of resampled histograms, without bothering too much about what exact resampling technique or detailed settings would be most appropriate. input arguments: - hists: input histogram set - ntarget: targetted number of resampled histograms (default: equally many as in hists) - fourierstdfactor: see fourier_noise - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False","title":"upsample_hist_set"},{"location":"utils/generate_data_utils/#white95noise","text":"full signature: def white_noise(hists, stdfactor=15., doplot=True) comments: apply white noise to the histograms in hists. input args: - hists: np array (nhists,nbins) containing input histograms - stdfactor: scaling factor of white noise amplitude (higher factor = smaller noise) - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False","title":"white_noise"},{"location":"utils/generate_data_utils/#resample95bin95per95bin","text":"full signature: def resample_bin_per_bin(hists, outfilename='', nresamples=0, nonnegative=True, smoothinghalfwidth=2, doplot=True) comments: do resampling from bin-per-bin probability distributions input args: - hists: np array (nhists,nbins) containing the histograms to draw new samples from - outfilename: path to csv file to write results to (default: no writing) - nresamples: number of samples to draw (default: 1/10 of number of input histograms) - nonnegative: boolean whether or not to put all bins to minimum zero after applying noise - smoothinghalfwidth: halfwidth of smoothing procedure to apply on the result (default: no smoothing) - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False advantages: no arbitrary noise modeling disadvantages: bins are considered independent, shape of historams not taken into account, does not work well on small number of input histograms, does not work well on histograms with systematic shifts","title":"resample_bin_per_bin"},{"location":"utils/generate_data_utils/#resample95similar95bin95per95bin","text":"full signature: def resample_similar_bin_per_bin( allhists, selhists, outfilename='', nresamples=1, nonnegative=True, keeppercentage=1., doplot=True) comments: resample from bin-per-bin probability distributions, but only from similar looking histograms. input args: - allhists: np array (nhists,nbins) containing all available histograms (to determine mean) - selhists: np array (nhists,nbins) conataining selected histograms used as seeds (e.g. 'good' histograms) - outfilename: path of csv file to write results to (default: no writing) - nresamples: number of samples per input histogram in selhists - nonnegative: boolean whether or not to put all bins to minimum zero after applying noise - keeppercentage: percentage (between 1 and 100) of histograms in allhists to use per input histogram - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False advantages: no assumptions on shape of noise, can handle systematic shifts in histograms disadvantages: bins are treated independently from each other","title":"resample_similar_bin_per_bin"},{"location":"utils/generate_data_utils/#resample95similar95fourier95noise","text":"full signature: def resample_similar_fourier_noise( allhists, selhists, outfilename='', nresamples=1, nonnegative=True, keeppercentage=1., doplot=True) comments: apply fourier noise on mean histogram, where the mean is determined from a set of similar-looking histograms input args: - allhists: np array (nhists,nbins) containing all available histograms (to determine mean) - selhists: np array (nhists,nbins) conataining selected histograms used as seeds (e.g. 'good' histograms) - outfilename: path of csv file to write results to (default: no writing) - nresamples: number of samples per input histogram in selhists - nonnegative: boolean whether or not to put all bins to minimum zero after applying noise - keeppercentage: percentage (between 1 and 100) of histograms in allhists to use per input histogram - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False advantages: most of fourier_noise_on_mean but can additionally handle shifting histograms, apart from fourier noise, also white noise can be applied. disadvantages: does not filter out odd histograms as long as enough other odd histograms look more or less similar","title":"resample_similar_fourier_noise"},{"location":"utils/generate_data_utils/#resample95similar95lico","text":"full signature: def resample_similar_lico( allhists, selhists, outfilename='', nresamples=1, nonnegative=True, keeppercentage=1., doplot=True) comments: take linear combinations of similar histograms input arguments: - allhists: 2D np array (nhists,nbins) with all available histograms, used to take linear combinations - selhists: 2D np array (nhists,nbins) with selected hists used for seeding (e.g. 'good' histograms) - outfilename: path to csv file to write result to (default: no writing) - nresamples: number of combinations to make per input histogram - nonnegative: boolean whether to make all final histograms nonnegative - keeppercentage: percentage (between 0. and 100.) of histograms in allhists to use per input histogram - doplot: boolean whether to make a plot returns: a tuple of the form (resulting histograms, maplotlib figure, matplotlib axes), figure and axes are None if doplot was set to False advantages: no assumptions on noise disadvantages: sensitive to outlying histograms (more than with averaging)","title":"resample_similar_lico"},{"location":"utils/generate_data_utils/#mc95sampling","text":"full signature: def mc_sampling(hists, nMC=10000 , nresamples=10, doplot=True) comments: resampling of a histogram using MC methods Drawing random points from a space defined by the range of the histogram in all axes. Points are \"accepted\" if the fall under the sampled histogram: f(x) - sampled distribution x_r, y_r -> randomly sampled point if y_r<=f(x_r), fill the new distribution at bin corresponding to x_r with weight: weight = (sum of input hist)/(#mc points accepted) this is equal to weight = (MC space volume)/(all MC points)","title":"mc_sampling"},{"location":"utils/hist_utils/","text":"hist utils A collection of useful basic functions for processing histograms. Functionality includes: - rebinning, cropping and normalization - moment calculation - averaging over neighbouring histograms - smoothing over neighbouring bins - higher-level functions preparing data for ML training, starting from a dataframe or input csv file. crophists full signature: def crophists(hists, slices=None) comments: perform cropping on a set of histograms input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - slices: a slice object (builtin python type) or a list of two slices (for 2D) notes: - a slice can be created using the builtin python syntax 'slice(start,stop,step)', and the syntax 'list[slice]' is equivalent to 'list[start:stop:step]'. use 'None' to ignore one of the arguments for slice creation (equivalent to ':' in direct slicing) - for 1D histograms, slices can be either a slice object or a list of length 1 containing a single slice. example usage: - see tutorials/plot_histograms_2d.ipynb returns: - a numpy array containing the same histograms as input but cropped according to the slices argument get_cropslices_from_str full signature: def get_cropslices_from_str(slicestr) comments: get a collection of slices from a string (e.g. argument in gui) note: the resulting slices are typically passed to crophists (see above) input arguments: - slicestr: string representation of slices e.g. '0:6:2' for slice(0,6,2) e.g. '0:6:2,1:5:2' for [slice(0,6,2),slice(1,5,2)] rebinhists full signature: def rebinhists(hists, factor=None) comments: perform rebinning on a set of histograms input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - factor: the rebinning factor (for 1D), or a tuple of (y axis rebinning factor, x axis rebinning factor) (for 2D) note: the rebinning applied here is simple summing of bin contents, and the rebinning factors must be divisors of the respective number of bins! example usage: - see tutorials/plot_histograms_2d.ipynb returns: - a numpy array containing the same histograms as input but rebinned according to the factor argument get_rebinningfactor_from_str full signature: def get_rebinningfactor_from_str(factstr) comments: get a valid rebinning factor (int or tuple) from a string (e.g. argument in gui) note: the resulting factor is typically passed to rebinhists (see above) input arguments: - factstr: string representation of rebinning factor e.g. '4' for 4 (for 1D histograms) e.g. '4,4' for (4,4) (for 2D histograms) normalizehists full signature: def normalizehists(hists) comments: perform normalization on a set of histograms note: - for 1D histograms, the sum of bin contents is set equal one for each histogram - for 2D histograms, the bin contents are scaled so the maximum is 1 for each histogram - maybe later make more flexible by adding normalization stragy as argument... input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D returns: - a numpy array containing the same histograms as input but normalized averagehists full signature: def averagehists(hists, nout=None) comments: partition a set of histograms into equal parts and take the average histogram of each part input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - nout: number of partitions, i.e. number of output histograms note: nout=1 corresponds to simply taking the average of all histograms in hists. note: if nout is negative or if nout is larger than number of input histograms, the original set of histograms is returned. returns: - a numpy array of shape (nout,nbins) running_average_hists full signature: def running_average_hists(hists, window=None, weights=None) comments: replace each histogram in a collection of histograms by its running average input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - window: number of histograms to consider for the averaging if window is an integer, it is the number of previous histograms in hists used for averaging (so window=0 would correspond to no averaging) if window is a tuple, it corresponds to (nprevious,nnext), and the nprevious previous and nnext next histograms in hists are used for averaging (so window=(0,0) would correspond to no averaging) - weights: a list or numpy array containing the relative weights of the histograms in the averaging procedure. note: the weights can be any number, but they will be normalized to have unit sum. note: weights must have length nwindow+1 or nprevious+1+nnext. note: the default behaviour is a uniform array with values 1./(window+1) (or 1./(nprevious+1+nnext)) returns: - a numpy array with same shape as input but where each histogram is replaced by its running average notes: - at the edges, the weights are cropped to match the input array and renormalized - this function will throw an error when the length of the set of histograms is smaller than the total window length, maybe extend later (although this is not normally needed) select_random full signature: def select_random(hists, nselect=10) comments: select nselect random examples from a set of histograms input arguments: - hists: a numpy array of shape (nhistograms, nbins) for 1D or (nhistograms, nybins, nxbins) for 2D. - nselect: number of random instances to draw smoothhists full signature: def smoothhists(hists, halfwindow=None, weights=None) comments: perform histogram smoothing by averaging over neighbouring bins input arguments: - hists: a numpy array of shape (nhistograms, nbins) for 1D or (nhistograms, nybins, nxbins) for 2D. - halfwindow: number of bins to consider for the averaging; for 1D histograms, must be an int, corresponding to the number of bins before and after the current bin to average over; for 2D histograms, must be a tuple of (halfwindow_y, halfwindow_x). - weights: numpy array containing the relative weights of the bins for the averaging; for 1D histograms, must have length 2*halfwindow+1; for 2D histograms, must have shape (2*halfwindow_y+1, 2*halfwindow_x+1). note: the weights can be any number, but they will be normalized to have unit sum. note: the default behaviour is a uniform array returns: - a numpy array with same shape as input but where each histogram is replaced by its smoothed version get_smoothinghalfwindow_from_str full signature: def get_smoothinghalfwindow_from_str(windowstr) comments: get a valid smoothing half window (int or tuple) from a string (e.g. argument in gui) note: the resulting factor is typically passed to smoothhists (see above) input arguments: - windowstr: string representation of smoothing window e.g. '4' for 4 (for 1D histograms) e.g. '4,4' for (4,4) (for 2D histograms) moment full signature: def moment(bins, counts, order) comments: get n-th central moment of a histogram input arguments: - bins: a 1D or 2D np array holding the bin centers (shape (nbins) or (nhistograms,nbins)) - counts: a 2D np array containing the bin counts (shape (nhistograms,nbins)) - order: the order of the moment to calculate (0 = maximum value, 1 = mean value) returns: - an array of shape (nhistograms) holding the requested moment per histogram notes: - for now only 1D histograms are supported! histmean full signature: def histmean(bins, counts) comments: special case of moment calculation (with order=1) histrms full signature: def histrms(bins, counts) comments: special case of moment calculation histmoments full signature: def histmoments(bins, counts, orders) comments: apply moment calculation for a list of orders input arguments: - see function moment(bins, counts, order), the only difference being that orders is a list instead of a single number returns: - a numpy array of shape (nhistograms,nmoments) preparedatafromnpy full signature: def preparedatafromnpy(dataname, cropslices=None, rebinningfactor=None, smoothinghalfwindow=None, smoothingweights=None, averagewindow=None, averageweights=None, donormalize=True, doplot=False) comments: read a .npy file and output the histograms input arguments: - see e.g. preparedatafromdf notes: - not yet tested for 2D histograms, but is expected to work... preparedatafromdf full signature: def preparedatafromdf(df, returnrunls=False, cropslices=None, rebinningfactor=None, smoothinghalfwindow=None, smoothingweights=None, averagewindow=None, averageweights=None, donormalize=False, doplot=False) comments: prepare the data contained in a dataframe in the form of a numpy array input arguments: - returnrunls: boolean whether to return a tuple of (histograms, run numbers, lumisection numbers). (default: return only histograms) - cropslices: list of slices (one per dimension) by which to crop the historams (default: no cropping) - rebinningfactor: an integer (or tuple of integers for 2D histograms) to downsample/rebin the histograms (default: no rebinning) - smoothinghalfwindow: int or tuple (for 1D/2D histograms) used for smoothing the histograms - smoothingweights: 1D or 2D array (for 1D/2D histograms) with weights for smoothing - donormalize: boolean whether to normalize the data - doplot: if True, some example plots are made showing the histograms preparedatafromcsv full signature: def preparedatafromcsv(dataname, returnrunls=False, cropslices=None, rebinningfactor=None, smoothinghalfwindow=None, smoothingweights=None, averagewindow=None, averageweights=None, donormalize=True, doplot=False) comments: prepare the data contained in a dataframe csv file in the form of a numpy array input arguments: - returnrunls: boolean whether to return a tuple of (histograms, run numbers, lumisection numbers). (default: return only histograms) - cropslices: list of slices (one per dimension) by which to crop the historams (default: no cropping) - rebinningfactor: an integer (or tuple of integers for 2D histograms) to downsample/rebin the histograms (default: no rebinning) - smoothinghalfwindow: int or tuple (for 1D/2D histograms) used for smoothing the histograms - smoothingweights: 1D or 2D array (for 1D/2D histograms) with weights for smoothing - donormalize: boolean whether to normalize the data - doplot: if True, some example plots are made showing the histograms","title":"hist_utils"},{"location":"utils/hist_utils/#hist-utils","text":"A collection of useful basic functions for processing histograms. Functionality includes: - rebinning, cropping and normalization - moment calculation - averaging over neighbouring histograms - smoothing over neighbouring bins - higher-level functions preparing data for ML training, starting from a dataframe or input csv file.","title":"hist utils"},{"location":"utils/hist_utils/#crophists","text":"full signature: def crophists(hists, slices=None) comments: perform cropping on a set of histograms input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - slices: a slice object (builtin python type) or a list of two slices (for 2D) notes: - a slice can be created using the builtin python syntax 'slice(start,stop,step)', and the syntax 'list[slice]' is equivalent to 'list[start:stop:step]'. use 'None' to ignore one of the arguments for slice creation (equivalent to ':' in direct slicing) - for 1D histograms, slices can be either a slice object or a list of length 1 containing a single slice. example usage: - see tutorials/plot_histograms_2d.ipynb returns: - a numpy array containing the same histograms as input but cropped according to the slices argument","title":"crophists"},{"location":"utils/hist_utils/#get95cropslices95from95str","text":"full signature: def get_cropslices_from_str(slicestr) comments: get a collection of slices from a string (e.g. argument in gui) note: the resulting slices are typically passed to crophists (see above) input arguments: - slicestr: string representation of slices e.g. '0:6:2' for slice(0,6,2) e.g. '0:6:2,1:5:2' for [slice(0,6,2),slice(1,5,2)]","title":"get_cropslices_from_str"},{"location":"utils/hist_utils/#rebinhists","text":"full signature: def rebinhists(hists, factor=None) comments: perform rebinning on a set of histograms input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - factor: the rebinning factor (for 1D), or a tuple of (y axis rebinning factor, x axis rebinning factor) (for 2D) note: the rebinning applied here is simple summing of bin contents, and the rebinning factors must be divisors of the respective number of bins! example usage: - see tutorials/plot_histograms_2d.ipynb returns: - a numpy array containing the same histograms as input but rebinned according to the factor argument","title":"rebinhists"},{"location":"utils/hist_utils/#get95rebinningfactor95from95str","text":"full signature: def get_rebinningfactor_from_str(factstr) comments: get a valid rebinning factor (int or tuple) from a string (e.g. argument in gui) note: the resulting factor is typically passed to rebinhists (see above) input arguments: - factstr: string representation of rebinning factor e.g. '4' for 4 (for 1D histograms) e.g. '4,4' for (4,4) (for 2D histograms)","title":"get_rebinningfactor_from_str"},{"location":"utils/hist_utils/#normalizehists","text":"full signature: def normalizehists(hists) comments: perform normalization on a set of histograms note: - for 1D histograms, the sum of bin contents is set equal one for each histogram - for 2D histograms, the bin contents are scaled so the maximum is 1 for each histogram - maybe later make more flexible by adding normalization stragy as argument... input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D returns: - a numpy array containing the same histograms as input but normalized","title":"normalizehists"},{"location":"utils/hist_utils/#averagehists","text":"full signature: def averagehists(hists, nout=None) comments: partition a set of histograms into equal parts and take the average histogram of each part input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - nout: number of partitions, i.e. number of output histograms note: nout=1 corresponds to simply taking the average of all histograms in hists. note: if nout is negative or if nout is larger than number of input histograms, the original set of histograms is returned. returns: - a numpy array of shape (nout,nbins)","title":"averagehists"},{"location":"utils/hist_utils/#running95average95hists","text":"full signature: def running_average_hists(hists, window=None, weights=None) comments: replace each histogram in a collection of histograms by its running average input arguments: - hists: a numpy array of shape (nhistograms,nbins) for 1D or (nhistograms,nybins,nxbins) for 2D - window: number of histograms to consider for the averaging if window is an integer, it is the number of previous histograms in hists used for averaging (so window=0 would correspond to no averaging) if window is a tuple, it corresponds to (nprevious,nnext), and the nprevious previous and nnext next histograms in hists are used for averaging (so window=(0,0) would correspond to no averaging) - weights: a list or numpy array containing the relative weights of the histograms in the averaging procedure. note: the weights can be any number, but they will be normalized to have unit sum. note: weights must have length nwindow+1 or nprevious+1+nnext. note: the default behaviour is a uniform array with values 1./(window+1) (or 1./(nprevious+1+nnext)) returns: - a numpy array with same shape as input but where each histogram is replaced by its running average notes: - at the edges, the weights are cropped to match the input array and renormalized - this function will throw an error when the length of the set of histograms is smaller than the total window length, maybe extend later (although this is not normally needed)","title":"running_average_hists"},{"location":"utils/hist_utils/#select95random","text":"full signature: def select_random(hists, nselect=10) comments: select nselect random examples from a set of histograms input arguments: - hists: a numpy array of shape (nhistograms, nbins) for 1D or (nhistograms, nybins, nxbins) for 2D. - nselect: number of random instances to draw","title":"select_random"},{"location":"utils/hist_utils/#smoothhists","text":"full signature: def smoothhists(hists, halfwindow=None, weights=None) comments: perform histogram smoothing by averaging over neighbouring bins input arguments: - hists: a numpy array of shape (nhistograms, nbins) for 1D or (nhistograms, nybins, nxbins) for 2D. - halfwindow: number of bins to consider for the averaging; for 1D histograms, must be an int, corresponding to the number of bins before and after the current bin to average over; for 2D histograms, must be a tuple of (halfwindow_y, halfwindow_x). - weights: numpy array containing the relative weights of the bins for the averaging; for 1D histograms, must have length 2*halfwindow+1; for 2D histograms, must have shape (2*halfwindow_y+1, 2*halfwindow_x+1). note: the weights can be any number, but they will be normalized to have unit sum. note: the default behaviour is a uniform array returns: - a numpy array with same shape as input but where each histogram is replaced by its smoothed version","title":"smoothhists"},{"location":"utils/hist_utils/#get95smoothinghalfwindow95from95str","text":"full signature: def get_smoothinghalfwindow_from_str(windowstr) comments: get a valid smoothing half window (int or tuple) from a string (e.g. argument in gui) note: the resulting factor is typically passed to smoothhists (see above) input arguments: - windowstr: string representation of smoothing window e.g. '4' for 4 (for 1D histograms) e.g. '4,4' for (4,4) (for 2D histograms)","title":"get_smoothinghalfwindow_from_str"},{"location":"utils/hist_utils/#moment","text":"full signature: def moment(bins, counts, order) comments: get n-th central moment of a histogram input arguments: - bins: a 1D or 2D np array holding the bin centers (shape (nbins) or (nhistograms,nbins)) - counts: a 2D np array containing the bin counts (shape (nhistograms,nbins)) - order: the order of the moment to calculate (0 = maximum value, 1 = mean value) returns: - an array of shape (nhistograms) holding the requested moment per histogram notes: - for now only 1D histograms are supported!","title":"moment"},{"location":"utils/hist_utils/#histmean","text":"full signature: def histmean(bins, counts) comments: special case of moment calculation (with order=1)","title":"histmean"},{"location":"utils/hist_utils/#histrms","text":"full signature: def histrms(bins, counts) comments: special case of moment calculation","title":"histrms"},{"location":"utils/hist_utils/#histmoments","text":"full signature: def histmoments(bins, counts, orders) comments: apply moment calculation for a list of orders input arguments: - see function moment(bins, counts, order), the only difference being that orders is a list instead of a single number returns: - a numpy array of shape (nhistograms,nmoments)","title":"histmoments"},{"location":"utils/hist_utils/#preparedatafromnpy","text":"full signature: def preparedatafromnpy(dataname, cropslices=None, rebinningfactor=None, smoothinghalfwindow=None, smoothingweights=None, averagewindow=None, averageweights=None, donormalize=True, doplot=False) comments: read a .npy file and output the histograms input arguments: - see e.g. preparedatafromdf notes: - not yet tested for 2D histograms, but is expected to work...","title":"preparedatafromnpy"},{"location":"utils/hist_utils/#preparedatafromdf","text":"full signature: def preparedatafromdf(df, returnrunls=False, cropslices=None, rebinningfactor=None, smoothinghalfwindow=None, smoothingweights=None, averagewindow=None, averageweights=None, donormalize=False, doplot=False) comments: prepare the data contained in a dataframe in the form of a numpy array input arguments: - returnrunls: boolean whether to return a tuple of (histograms, run numbers, lumisection numbers). (default: return only histograms) - cropslices: list of slices (one per dimension) by which to crop the historams (default: no cropping) - rebinningfactor: an integer (or tuple of integers for 2D histograms) to downsample/rebin the histograms (default: no rebinning) - smoothinghalfwindow: int or tuple (for 1D/2D histograms) used for smoothing the histograms - smoothingweights: 1D or 2D array (for 1D/2D histograms) with weights for smoothing - donormalize: boolean whether to normalize the data - doplot: if True, some example plots are made showing the histograms","title":"preparedatafromdf"},{"location":"utils/hist_utils/#preparedatafromcsv","text":"full signature: def preparedatafromcsv(dataname, returnrunls=False, cropslices=None, rebinningfactor=None, smoothinghalfwindow=None, smoothingweights=None, averagewindow=None, averageweights=None, donormalize=True, doplot=False) comments: prepare the data contained in a dataframe csv file in the form of a numpy array input arguments: - returnrunls: boolean whether to return a tuple of (histograms, run numbers, lumisection numbers). (default: return only histograms) - cropslices: list of slices (one per dimension) by which to crop the historams (default: no cropping) - rebinningfactor: an integer (or tuple of integers for 2D histograms) to downsample/rebin the histograms (default: no rebinning) - smoothinghalfwindow: int or tuple (for 1D/2D histograms) used for smoothing the histograms - smoothingweights: 1D or 2D array (for 1D/2D histograms) with weights for smoothing - donormalize: boolean whether to normalize the data - doplot: if True, some example plots are made showing the histograms","title":"preparedatafromcsv"},{"location":"utils/json_utils/","text":"json utils A collection of useful basic functions for manipulating json files. Functionality includes: - reading and writing json files for given sets of run numbers and lumisection numbers - checking if a given run number, lumisection number or combination is present in a given json file Note that the json files are always assumed to contain the following structure: - dict - run number (in string format) - list - list of two elements - starting lumisection number, ending lumisection number Example: { \"294927\": [ [ 55,85 ], [ 95,105] ] } There is one exception to this rule: instead of [ start, stop ], the lumisection list can also be [ -1 ], which is short for all lumisections within that run. loadjson full signature: def loadjson( jsonfile ) comments: load the content of a json file into a python object input arguments: - jsonfile: the name (or full path if needed) to the json file to be read output: - an dict object as specified in the note below note: the json file is supposed to contain an object like this example: { \"294927\": [ [ 55,85 ], [ 95,105] ], \"294928\": [ [1,33 ] ] } although no explicit checking is done in this function, objects that don't have this structure will probably lead to errors further in the code writejson full signature: def writejson( jsondict, outputfile, overwrite=False ) comments: inverse function of loadjson input arguments - jsondict: dict object to be written to a json file - outputfile: output file to be written, extension '.json' will be appended automatically - overwrite: boolean whether to overwrite outputfile if it exists (default: throw exception) injson_single full signature: def injson_single( run, lumi, jsondict ) comments: helper function for injson, only for internal use input arguments: - run and lumi are integers - jsondict is an object loaded from a json file output: - boolean whether the run/lumi combination is in the json dict injson full signature: def injson( run, lumi, jsonfile=None, jsondict=None ) comments: find if a run and lumi combination is in a given json file input arguments: - run and lumi: integers or (equally long) arrays of integers - jsonfile: a path to a json file - jsondict: a dict loaded from a json file note: either jsonfile or jsondict must not be None! output: boolean or array of booleans (depending on run and lumi) getjsondir full signature: def getjsondir() comments: internal helper function returning the path to where json files are stored isgolden full signature: def isgolden(run, lumi) comments: find if a run and lumi combination is in the golden json file input arguments: - run and lumi: either integers or (equally long) arrays of integers isdcson full signature: def isdcson(run, lumi) comments: find if a run and lumi combination is in DCS-only json file input arguments: - run and lumi: either integers or (equally long) arrays of integers plainlist_to_rangelist full signature: def plainlist_to_rangelist( plainlist ) comments: helper function for tuplelist_to_jsondict, only for internal use input arguments: - plainlist: a list of integers in increasing order, must have length >= 2 output: - a list lists representing ranges example: [1,2,3,5,6] -> [ [1,3], [5,6] ] rangelist_to_plainlist full signature: def rangelist_to_plainlist( rangelist ) comments: inverse function of plainlist_to_rangelist, for internal use only tuplelist_to_jsondict full signature: def tuplelist_to_jsondict( tuplelist ) comments: convert a list of tuples of format (run number, [lumisection numbers]) to json dict jsondict_to_tuplelist full signature: def jsondict_to_tuplelist( jsondict ) comments: inverse function of tuplelist_to_jsondict get_lcs full signature: def get_lcs( jsonlist ) comments: return a jsondict object that is the largest common subset (LCS) between the jsondict objects in jsonlist input arguments: - jsonlist: a list of dicts in the conventional json format, so each element in jsonlist must be e.g. { \"294927\": [ [ 55,85 ], [ 95,105] ], \"294928\": [ [1,33 ] ] } remark: this is probably not the most efficient implementation, open for improvement...","title":"json_utils"},{"location":"utils/json_utils/#json-utils","text":"A collection of useful basic functions for manipulating json files. Functionality includes: - reading and writing json files for given sets of run numbers and lumisection numbers - checking if a given run number, lumisection number or combination is present in a given json file Note that the json files are always assumed to contain the following structure: - dict - run number (in string format) - list - list of two elements - starting lumisection number, ending lumisection number Example: { \"294927\": [ [ 55,85 ], [ 95,105] ] } There is one exception to this rule: instead of [ start, stop ], the lumisection list can also be [ -1 ], which is short for all lumisections within that run.","title":"json utils"},{"location":"utils/json_utils/#loadjson","text":"full signature: def loadjson( jsonfile ) comments: load the content of a json file into a python object input arguments: - jsonfile: the name (or full path if needed) to the json file to be read output: - an dict object as specified in the note below note: the json file is supposed to contain an object like this example: { \"294927\": [ [ 55,85 ], [ 95,105] ], \"294928\": [ [1,33 ] ] } although no explicit checking is done in this function, objects that don't have this structure will probably lead to errors further in the code","title":"loadjson"},{"location":"utils/json_utils/#writejson","text":"full signature: def writejson( jsondict, outputfile, overwrite=False ) comments: inverse function of loadjson input arguments - jsondict: dict object to be written to a json file - outputfile: output file to be written, extension '.json' will be appended automatically - overwrite: boolean whether to overwrite outputfile if it exists (default: throw exception)","title":"writejson"},{"location":"utils/json_utils/#injson95single","text":"full signature: def injson_single( run, lumi, jsondict ) comments: helper function for injson, only for internal use input arguments: - run and lumi are integers - jsondict is an object loaded from a json file output: - boolean whether the run/lumi combination is in the json dict","title":"injson_single"},{"location":"utils/json_utils/#injson","text":"full signature: def injson( run, lumi, jsonfile=None, jsondict=None ) comments: find if a run and lumi combination is in a given json file input arguments: - run and lumi: integers or (equally long) arrays of integers - jsonfile: a path to a json file - jsondict: a dict loaded from a json file note: either jsonfile or jsondict must not be None! output: boolean or array of booleans (depending on run and lumi)","title":"injson"},{"location":"utils/json_utils/#getjsondir","text":"full signature: def getjsondir() comments: internal helper function returning the path to where json files are stored","title":"getjsondir"},{"location":"utils/json_utils/#isgolden","text":"full signature: def isgolden(run, lumi) comments: find if a run and lumi combination is in the golden json file input arguments: - run and lumi: either integers or (equally long) arrays of integers","title":"isgolden"},{"location":"utils/json_utils/#isdcson","text":"full signature: def isdcson(run, lumi) comments: find if a run and lumi combination is in DCS-only json file input arguments: - run and lumi: either integers or (equally long) arrays of integers","title":"isdcson"},{"location":"utils/json_utils/#plainlist95to95rangelist","text":"full signature: def plainlist_to_rangelist( plainlist ) comments: helper function for tuplelist_to_jsondict, only for internal use input arguments: - plainlist: a list of integers in increasing order, must have length >= 2 output: - a list lists representing ranges example: [1,2,3,5,6] -> [ [1,3], [5,6] ]","title":"plainlist_to_rangelist"},{"location":"utils/json_utils/#rangelist95to95plainlist","text":"full signature: def rangelist_to_plainlist( rangelist ) comments: inverse function of plainlist_to_rangelist, for internal use only","title":"rangelist_to_plainlist"},{"location":"utils/json_utils/#tuplelist95to95jsondict","text":"full signature: def tuplelist_to_jsondict( tuplelist ) comments: convert a list of tuples of format (run number, [lumisection numbers]) to json dict","title":"tuplelist_to_jsondict"},{"location":"utils/json_utils/#jsondict95to95tuplelist","text":"full signature: def jsondict_to_tuplelist( jsondict ) comments: inverse function of tuplelist_to_jsondict","title":"jsondict_to_tuplelist"},{"location":"utils/json_utils/#get95lcs","text":"full signature: def get_lcs( jsonlist ) comments: return a jsondict object that is the largest common subset (LCS) between the jsondict objects in jsonlist input arguments: - jsonlist: a list of dicts in the conventional json format, so each element in jsonlist must be e.g. { \"294927\": [ [ 55,85 ], [ 95,105] ], \"294928\": [ [1,33 ] ] } remark: this is probably not the most efficient implementation, open for improvement...","title":"get_lcs"},{"location":"utils/mask_utils/","text":"mask utils Utilities for working with HistStruct masks Mostly meant for internal use. get_combined_name full signature: def get_combined_name( masklist ) comments: concatenate all the masknames in masklist to a combined name input arguments: - masklist: list of strings output: string with contatenated name","title":"mask_utils"},{"location":"utils/mask_utils/#mask-utils","text":"Utilities for working with HistStruct masks Mostly meant for internal use.","title":"mask utils"},{"location":"utils/mask_utils/#get95combined95name","text":"full signature: def get_combined_name( masklist ) comments: concatenate all the masknames in masklist to a combined name input arguments: - masklist: list of strings output: string with contatenated name","title":"get_combined_name"},{"location":"utils/plot_utils/","text":"plot utils A collection of useful basic functions for plotting. make_legend_opaque full signature: def make_legend_opaque( leg ) comments: set the transparency of all entries in a legend to zero add_text full signature: def add_text( ax, text, pos, fontsize=10, horizontalalignment='left', verticalalignment='bottom', background_facecolor=None, background_alpha=None, background_edgecolor=None, **kwargs ) comments: add text to an axis at a specified position (in relative figure coordinates) input arguments: - ax: matplotlib axis object - text: string, can contain latex syntax such as /textbf{} and /textit{} - pos: tuple with relative x- and y-axis coordinates of bottom left corner add_cms_label full signature: def add_cms_label( ax, pos=(0.1,0.9), extratext=None, **kwargs ) comments: add the CMS label and extra text (e.g. 'Preliminary') to a plot special case of add_text, for convenience make_text_latex_safe full signature: def make_text_latex_safe( text ) comments: make a string safe to process with matplotlib's latex parser in case no tex parsing is wanted (e.g. escape underscores) to be extended when the need arises! plot_hists full signature: def plot_hists(histlist, fig=None, ax=None, colorlist=[], labellist=[], transparency=1, xlims=(-0.5,-1), title=None, titlesize=None, xaxtitle=None, xaxtitlesize=None, yaxtitle=None, yaxtitlesize=None, ymaxfactor=None, legendsize=None, opaque_legend=False, ticksize=None, bkgcolor=None, bkgcmap='spring', bkgrange=None, bkgtitle=None) comments: plot some histograms (in histlist) in one figure using specified colors and/or labels - histlist is a list of 1D arrays containing the histograms (or a 2D array of shape (nhistograms,nbins)) - colorlist is a list or array containing colors (in string format), of length nhistograms note: it can also be a single string representing a color (in pyplot), then all histograms will take this color - labellist is a list or array containing labels for in legend, of length nhistograms - xlims is a tuple of min and max for the x-axis labels, defaults to (-0.5,nbins-0.5) - title, xaxtitle, yaxtitle: strings for histogram title, x-axis title and y-axis title respectively - bkgcolor: 1D array representing background color for the plot (color axis will be scaled between min and max in bkgcolor) note: if bkgcolor does not have the same length as the x-axis, it will be compressed or stretched to fit the axis, but this might be meaningless, depending on what you are trying to visualize! - bkgmap: name of valid pyplot color map for plotting the background color output: tuple of figure and axis objects, that can be used to further tune the look of the figure or save it plot_hists_from_df full signature: def plot_hists_from_df(df, histtype, nhists) comments: plot a number of histograms in a dataframe - df is the dataframe from which to plot - histtype is the name of the histogram type (e.g. 'chargeInner_PXLayer_1') - nhists is the number of histograms to plot plot_hists_multi full signature: def plot_hists_multi(histlist, fig=None, ax=None, colorlist=[], labellist=[], transparency=1, xlims=(-0.5,-1), title=None, titlesize=None, xaxtitle=None, xaxtitlesize=None, yaxtitle=None, yaxtitlesize=None, caxtitle=None, caxtitlesize=None, caxtitleoffset=None, remove_underflow=False, remove_overflow=False, ylims=None, ymaxfactor=None, legendsize=None, opaque_legend=False, ticksize=None ) comments: plot many histograms (in histlist) in one figure using specified colors and/or labels - histlist is a list of 1D arrays containing the histograms (or a 2D array of shape (nhistograms,nbins)) - colorlist is a list or array containing numbers to be mapped to colors - labellist is a list or array containing labels for in legend output: tuple of figure and axis objects, that can be used to further tune the look of the figure or save it plot_sets full signature: def plot_sets(setlist, fig=None, ax=None, colorlist=[], labellist=[], transparencylist=[], title=None, titlesize=None, xaxtitle=None, xaxtitlesize=None, xlims=(-0.5,-1), remove_underflow=False, remove_overflow=False, yaxtitle=None, yaxtitlesize=None, ylims=None, ymaxfactor=None, legendsize=None, opaque_legend=False, ticksize=None) comments: plot multiple sets of 1D histograms to compare the shapes - setlist is a list of 2D numpy arrays containing histograms - fig and ax: a pyplot figure and axis object (if one of both is none a new figure is created) - title is a string that will be used as the title for the ax object other parameters are lists of which each element applies to one list of histograms plot_anomalous full signature: def plot_anomalous(histlist, ls, highlight=-1, hrange=-1) comments: plot a range of 1D histograms and highlight one of them input arguments: - histlist and ls: a list of histograms and corresponding lumisection numbers - highlight: the lumisection number of the histogram to highlight - hrange: the number of histograms before and after lsnumber to plot (default: whole run) plot_hist_2d full signature: def plot_hist_2d(hist, fig=None, ax=None, title=None, titlesize=None, xaxtitle=None, xaxtitlesize=None, yaxtitle=None, yaxtitlesize=None, ticklabelsize=None, colorticklabelsize=None, extent=None, caxrange=None) comments: plot a 2D histogram - hist is a 2D numpy array of shape (nxbins, nybins) notes: - if the histogram contains only nonnegative values, values below 1e-12 will not be plotted (i.e. they will be shown as white spots in the plot) to discriminate zero from small but nonzero - if the histogram contains negative values, the color axis will be symmetrized around zero plot_hists_2d full signature: def plot_hists_2d(hists, ncols=4, axsize=5, title=None, titlesize=None, subtitles=None, subtitlesize=None, xaxtitles=None, yaxtitles=None, **kwargs) comments: plot multiple 2D histograms next to each other input arguments - hists: list of 2D numpy arrays of shape (nxbins,nybins), or an equivalent 3D numpy array - ncols: number of columns to use - figsize: approximate size of a single axis in the figure (will be modified by aspect ratio) - title, titlesize: properties of the super title for the entire figure - subtitles, subtitlesize: properties of the individual histogram titles - xaxtitles, yaxtitles: properties of axis titles of individual histograms - kwargs: passed down to plot_hist_2d plot_hists_2d_gif full signature: def plot_hists_2d_gif(hists, titles=None, xaxtitle=None, yaxtitle=None, duration=0.3, figname='temp_gif.gif') comments: (no valid documentation found) plot_moments full signature: def plot_moments(moments, ls, dims=(0,1), fig=None, ax=None, markersize=10) comments: plot the moments of a set of histograms input arguments: - moments: a numpy array of shape (nhists,nmoments) - dims: a tuple of two or three values between 0 and nmoments-1 plot_distance full signature: def plot_distance(dists, ls=None, rmlargest=0., doplot=True, title=None, xaxtitle='lumisection number', yaxtitle='distance metric') comments: (no valid documentation found) plot_loss full signature: def plot_loss(data, loss_key='loss', val_loss_key='val_loss', title=None, titlesize=None, xaxtitle='Epoch', xaxtitlesize=None, yaxtitle='Loss', yaxtitlesize=None, xlims=None, yaxlog=True, legendsize=None, legendloc='best', doshow=True) comments: plot the training and validation loss of a keras/tensorflow model input arguments: - data: the object returned by the .fit method when called upon a keras model - other: plot layout options plot_mse full signature: def plot_mse(mse, rmlargest=0., doplot=True, title=None, xaxtitle='lumisection number', yaxtitle='mse') comments: plot the mse's and return the mean and std input args: - mse is a 1D numpy array of mse scores - doplot: boolean whether to make a plot or simply return mean and std - rmlargest: fraction of largest mse's to remove (to avoid being too sensitive to outliers) plot_score_dist full signature: def plot_score_dist( scores, labels, fig=None, ax=None, nbins=20, normalize=False, siglabel='Signal', sigcolor='g', bcklabel='Background', bckcolor='r', title=None, titlesize=12, xaxtitle=None, xaxtitlesize=12, yaxtitle=None, yaxtitlesize=12, legendsize=None, legendloc='best', ticksize=None, doshow=True) comments: make a plot showing the distributions of the output scores for signal and background plot_score_dist_multi full signature: def plot_score_dist_multi( scores, labels=None, colors=None, fig=None, ax=None, nbins=20, normalize=False, linestyle=None, linewidth=1, title=None, titlesize=12, xaxtitle=None, xaxtitlesize=12, yaxtitle=None, yaxtitlesize=12, legendsize=None, legendloc='best', ticksize=None, dolegend=True ) comments: plot the distribution of output scores for arbitrarily many sets (not limited to 'signal' and 'background') input arguments: - scores: list of numpy arrays of scores - labels: list of legend entries for the scores, must have same length as scores or be None (no legend) - colors: list of colors for the different score arrays, must have same length as scores or be None (default colors) plot_score_ls full signature: def plot_score_ls( thisscore, refscores, fig=None, ax=None, thislabel='This LS', thiscolor='black', reflabel='Reference LS', refcolor='dodgerblue', **kwargs ) comments: make a plot of the score for a single lumisection comparing to some reference distribution plot_metric full signature: def plot_metric( wprange, metric, label=None, color=None, sig_eff=None, sig_label=None, sig_color=None, bck_eff=None, bck_label=None, bck_color=None, title=None, xaxtitle='working point', yaxlog=False, ymaxfactor=1.3, yaxtitle='metric' ) comments: plot a metric based on signal and background efficiencies. along with the metric, the actual signal and background efficiencies can be plotted as well. input arguments: - wprange, metric: equally long 1D-numpy arrays, x- and y-data respectively - label: label for the metric to put in the legend - color: color for the metric (default: blue) - sig_eff: 1D-numpy array of signal efficiencies corresponding to wprange - sig_label: label for sig_eff in the legend - color: color for sig_eff (default: green) - bck_eff, bck_label, bck_color: same as for signal - title, xaxtitle and yaxtitle: titles for the plot and axes - yaxlog: boolean whether to put y axis in log scale - ymaxfactor: factor to add extra space on the top of the plot (for the legend) plot_roc full signature: def plot_roc( sig_eff, bkg_eff, auc=None, sig_eff_unc=None, color='b', title=None, titlesize=None, xaxtitle='Background efficiency', xaxtitlesize=None, yaxtitle='Signal efficiency', yaxtitlesize=None, xaxlog=True, yaxlog=False, xlims='auto', ylims='auto', dogrid=True, ticksize=None, doshow=True ) comments: note: automatic determination of xlims and ylims assumes log scale for x-axis and lin scale for y-axis; might not work properly in other cases and ranges should be provided manually. plot_confusion_matrix full signature: def plot_confusion_matrix( tp, tn, fp, fn, true_positive_label='Good', true_negative_label='Anomalous', pred_positive_label='Predicted good', pred_negative_label='Predicted anomalous', xaxlabelsize=None, yaxlabelsize=None, textsize=None, colormap='Blues', colortitle=None ) comments: (no valid documentation found) color full signature: def color(value) comments: (no valid documentation found) valstr full signature: def valstr(value) comments: (no valid documentation found) clip_scores full signature: def clip_scores( scores ) comments: clip +-inf values in scores local copy of the same functions in autoencoder_utils.py (need to copy here locally to use in plot_fit_2d and plot_fit_1d without circular import...) plot_fit_2d full signature: def plot_fit_2d( points, fitfunc=None, figsize=(10,8), logprob=False, clipprob=False, onlycontour=False, xlims=5, ylims=5, onlypositive=False, xaxtitle=None, xaxtitlesize=None, yaxtitle=None, yaxtitlesize=None, title=None, titlesize=None, caxtitle=None, caxtitlesize=None, caxrange=None, transparency=1, ticksize=None ) comments: make a 2D scatter plot of a point cloud with fitted contour input arguments: - points: a numpy array of shape (npoints,ndims), where ndims is supposed to be 2 - fitfunc: an object of type CloudFitter (see src/cloudfitters) or any other object that implements a pdf(points) method - logprob: boolean whether to plot log probability or normal probability - clipprob: boolean whether to replace +- inf values by (non-inf) max and min - onlycontour: a boolean whether to draw only the fit or include the data points - xlims and ylims: tuples of (low,high) note: can be an integer, in which case the range will be determined automatically from the formula low = mean-xlims*std, high = mean+xlims*std, where mean and std are determined from the points array. - onlypositive: overrides previous argument to set lower bound of plotting range at 0 in both dimensions. - xaxtitle and yaxtitle: titles for axes. plot_fit_2d_clusters full signature: def plot_fit_2d_clusters( points, clusters, figsize=(12,8), labels=None, colors=None, legendmarkerscale=1., legendsize=10, legendloc='best', legendbbox=None, **kwargs ) comments: make a 2D scatter plot of a fitted contour with point clouds superimposed input arguments: - points: numpy arrays of shape (npoints,ndims), where ndims is supposed to be 2, usually the points to which the fit was done note: only used to determine plotting range, these points are not plotted! - clusters: list of numpy arrays of shape (npoints,ndims), where ndims is supposed to be 2, clouds of points to plot - labels: list with legend entries (must be same length as clusters) - colors: list with colors (must be same length as clusters) - kwargs: passed down to plot_fit_2d note: onlycontour is set automatically and should not be in kwargs plot_fit_1d full signature: def plot_fit_1d( points, fitfunc=None, logprob=False, clipprob=False, onlycontour=False, xlims=5, onlypositive=False, xaxtitle=None, xaxtitlesize=None, yaxtitle=None, yaxtitlesize=None, title=None, titlesize=None ) comments: make a 1D scatter plot of a point cloud with fitted contour input arguments: - points: a numpy array of shape (npoints,ndims), where ndims is supposed to be 1 - fitfunc: an object of type CloudFitter (see src/cloudfitters) or any other object that implements a pdf(points) method - logprob: boolean whether to plot log probability or normal probability - clipprob: boolean whether to replace +- inf values by (non-inf) max and min - onlycontour: a boolean whether to draw only the fit or include the data points - xlims: tuple of the form (low,high) note: can be an integer, in which case the range will be determined automatically from the formula low = mean-xlims*std, high = mean+xlims*std, where mean and std are determined from the points array. - onlypositive: set lower bound of plotting range at 0 (overrides xlims) - xaxtitle and yaxtitle: titles for axes. plot_fit_1d_clusters full signature: def plot_fit_1d_clusters( points, clusters, labels=None, colors=None, **kwargs ) comments: make a 1D scatter plot of a fitted contour with point clouds superimposed input arguments: - points: numpy arrays of shape (npoints,ndims), where ndims is supposed to be 1, usually the points to which the fit was done note: only used to determine plotting range, these points are not plotted! - clusters: list of numpy arrays of shape (npoints,ndims), where ndims is supposed to be 1, clouds of points to plot - labels: list with legend entries (must be same length as clusters) - colors: list with colors (must be same length as clusters) - kwargs: passed down to plot_fit_1d note: onlycontour is set automatically and should not be in kwargs","title":"plot_utils"},{"location":"utils/plot_utils/#plot-utils","text":"A collection of useful basic functions for plotting.","title":"plot utils"},{"location":"utils/plot_utils/#make95legend95opaque","text":"full signature: def make_legend_opaque( leg ) comments: set the transparency of all entries in a legend to zero","title":"make_legend_opaque"},{"location":"utils/plot_utils/#add95text","text":"full signature: def add_text( ax, text, pos, fontsize=10, horizontalalignment='left', verticalalignment='bottom', background_facecolor=None, background_alpha=None, background_edgecolor=None, **kwargs ) comments: add text to an axis at a specified position (in relative figure coordinates) input arguments: - ax: matplotlib axis object - text: string, can contain latex syntax such as /textbf{} and /textit{} - pos: tuple with relative x- and y-axis coordinates of bottom left corner","title":"add_text"},{"location":"utils/plot_utils/#add95cms95label","text":"full signature: def add_cms_label( ax, pos=(0.1,0.9), extratext=None, **kwargs ) comments: add the CMS label and extra text (e.g. 'Preliminary') to a plot special case of add_text, for convenience","title":"add_cms_label"},{"location":"utils/plot_utils/#make95text95latex95safe","text":"full signature: def make_text_latex_safe( text ) comments: make a string safe to process with matplotlib's latex parser in case no tex parsing is wanted (e.g. escape underscores) to be extended when the need arises!","title":"make_text_latex_safe"},{"location":"utils/plot_utils/#plot95hists","text":"full signature: def plot_hists(histlist, fig=None, ax=None, colorlist=[], labellist=[], transparency=1, xlims=(-0.5,-1), title=None, titlesize=None, xaxtitle=None, xaxtitlesize=None, yaxtitle=None, yaxtitlesize=None, ymaxfactor=None, legendsize=None, opaque_legend=False, ticksize=None, bkgcolor=None, bkgcmap='spring', bkgrange=None, bkgtitle=None) comments: plot some histograms (in histlist) in one figure using specified colors and/or labels - histlist is a list of 1D arrays containing the histograms (or a 2D array of shape (nhistograms,nbins)) - colorlist is a list or array containing colors (in string format), of length nhistograms note: it can also be a single string representing a color (in pyplot), then all histograms will take this color - labellist is a list or array containing labels for in legend, of length nhistograms - xlims is a tuple of min and max for the x-axis labels, defaults to (-0.5,nbins-0.5) - title, xaxtitle, yaxtitle: strings for histogram title, x-axis title and y-axis title respectively - bkgcolor: 1D array representing background color for the plot (color axis will be scaled between min and max in bkgcolor) note: if bkgcolor does not have the same length as the x-axis, it will be compressed or stretched to fit the axis, but this might be meaningless, depending on what you are trying to visualize! - bkgmap: name of valid pyplot color map for plotting the background color output: tuple of figure and axis objects, that can be used to further tune the look of the figure or save it","title":"plot_hists"},{"location":"utils/plot_utils/#plot95hists95from95df","text":"full signature: def plot_hists_from_df(df, histtype, nhists) comments: plot a number of histograms in a dataframe - df is the dataframe from which to plot - histtype is the name of the histogram type (e.g. 'chargeInner_PXLayer_1') - nhists is the number of histograms to plot","title":"plot_hists_from_df"},{"location":"utils/plot_utils/#plot95hists95multi","text":"full signature: def plot_hists_multi(histlist, fig=None, ax=None, colorlist=[], labellist=[], transparency=1, xlims=(-0.5,-1), title=None, titlesize=None, xaxtitle=None, xaxtitlesize=None, yaxtitle=None, yaxtitlesize=None, caxtitle=None, caxtitlesize=None, caxtitleoffset=None, remove_underflow=False, remove_overflow=False, ylims=None, ymaxfactor=None, legendsize=None, opaque_legend=False, ticksize=None ) comments: plot many histograms (in histlist) in one figure using specified colors and/or labels - histlist is a list of 1D arrays containing the histograms (or a 2D array of shape (nhistograms,nbins)) - colorlist is a list or array containing numbers to be mapped to colors - labellist is a list or array containing labels for in legend output: tuple of figure and axis objects, that can be used to further tune the look of the figure or save it","title":"plot_hists_multi"},{"location":"utils/plot_utils/#plot95sets","text":"full signature: def plot_sets(setlist, fig=None, ax=None, colorlist=[], labellist=[], transparencylist=[], title=None, titlesize=None, xaxtitle=None, xaxtitlesize=None, xlims=(-0.5,-1), remove_underflow=False, remove_overflow=False, yaxtitle=None, yaxtitlesize=None, ylims=None, ymaxfactor=None, legendsize=None, opaque_legend=False, ticksize=None) comments: plot multiple sets of 1D histograms to compare the shapes - setlist is a list of 2D numpy arrays containing histograms - fig and ax: a pyplot figure and axis object (if one of both is none a new figure is created) - title is a string that will be used as the title for the ax object other parameters are lists of which each element applies to one list of histograms","title":"plot_sets"},{"location":"utils/plot_utils/#plot95anomalous","text":"full signature: def plot_anomalous(histlist, ls, highlight=-1, hrange=-1) comments: plot a range of 1D histograms and highlight one of them input arguments: - histlist and ls: a list of histograms and corresponding lumisection numbers - highlight: the lumisection number of the histogram to highlight - hrange: the number of histograms before and after lsnumber to plot (default: whole run)","title":"plot_anomalous"},{"location":"utils/plot_utils/#plot95hist952d","text":"full signature: def plot_hist_2d(hist, fig=None, ax=None, title=None, titlesize=None, xaxtitle=None, xaxtitlesize=None, yaxtitle=None, yaxtitlesize=None, ticklabelsize=None, colorticklabelsize=None, extent=None, caxrange=None) comments: plot a 2D histogram - hist is a 2D numpy array of shape (nxbins, nybins) notes: - if the histogram contains only nonnegative values, values below 1e-12 will not be plotted (i.e. they will be shown as white spots in the plot) to discriminate zero from small but nonzero - if the histogram contains negative values, the color axis will be symmetrized around zero","title":"plot_hist_2d"},{"location":"utils/plot_utils/#plot95hists952d","text":"full signature: def plot_hists_2d(hists, ncols=4, axsize=5, title=None, titlesize=None, subtitles=None, subtitlesize=None, xaxtitles=None, yaxtitles=None, **kwargs) comments: plot multiple 2D histograms next to each other input arguments - hists: list of 2D numpy arrays of shape (nxbins,nybins), or an equivalent 3D numpy array - ncols: number of columns to use - figsize: approximate size of a single axis in the figure (will be modified by aspect ratio) - title, titlesize: properties of the super title for the entire figure - subtitles, subtitlesize: properties of the individual histogram titles - xaxtitles, yaxtitles: properties of axis titles of individual histograms - kwargs: passed down to plot_hist_2d","title":"plot_hists_2d"},{"location":"utils/plot_utils/#plot95hists952d95gif","text":"full signature: def plot_hists_2d_gif(hists, titles=None, xaxtitle=None, yaxtitle=None, duration=0.3, figname='temp_gif.gif') comments: (no valid documentation found)","title":"plot_hists_2d_gif"},{"location":"utils/plot_utils/#plot95moments","text":"full signature: def plot_moments(moments, ls, dims=(0,1), fig=None, ax=None, markersize=10) comments: plot the moments of a set of histograms input arguments: - moments: a numpy array of shape (nhists,nmoments) - dims: a tuple of two or three values between 0 and nmoments-1","title":"plot_moments"},{"location":"utils/plot_utils/#plot95distance","text":"full signature: def plot_distance(dists, ls=None, rmlargest=0., doplot=True, title=None, xaxtitle='lumisection number', yaxtitle='distance metric') comments: (no valid documentation found)","title":"plot_distance"},{"location":"utils/plot_utils/#plot95loss","text":"full signature: def plot_loss(data, loss_key='loss', val_loss_key='val_loss', title=None, titlesize=None, xaxtitle='Epoch', xaxtitlesize=None, yaxtitle='Loss', yaxtitlesize=None, xlims=None, yaxlog=True, legendsize=None, legendloc='best', doshow=True) comments: plot the training and validation loss of a keras/tensorflow model input arguments: - data: the object returned by the .fit method when called upon a keras model - other: plot layout options","title":"plot_loss"},{"location":"utils/plot_utils/#plot95mse","text":"full signature: def plot_mse(mse, rmlargest=0., doplot=True, title=None, xaxtitle='lumisection number', yaxtitle='mse') comments: plot the mse's and return the mean and std input args: - mse is a 1D numpy array of mse scores - doplot: boolean whether to make a plot or simply return mean and std - rmlargest: fraction of largest mse's to remove (to avoid being too sensitive to outliers)","title":"plot_mse"},{"location":"utils/plot_utils/#plot95score95dist","text":"full signature: def plot_score_dist( scores, labels, fig=None, ax=None, nbins=20, normalize=False, siglabel='Signal', sigcolor='g', bcklabel='Background', bckcolor='r', title=None, titlesize=12, xaxtitle=None, xaxtitlesize=12, yaxtitle=None, yaxtitlesize=12, legendsize=None, legendloc='best', ticksize=None, doshow=True) comments: make a plot showing the distributions of the output scores for signal and background","title":"plot_score_dist"},{"location":"utils/plot_utils/#plot95score95dist95multi","text":"full signature: def plot_score_dist_multi( scores, labels=None, colors=None, fig=None, ax=None, nbins=20, normalize=False, linestyle=None, linewidth=1, title=None, titlesize=12, xaxtitle=None, xaxtitlesize=12, yaxtitle=None, yaxtitlesize=12, legendsize=None, legendloc='best', ticksize=None, dolegend=True ) comments: plot the distribution of output scores for arbitrarily many sets (not limited to 'signal' and 'background') input arguments: - scores: list of numpy arrays of scores - labels: list of legend entries for the scores, must have same length as scores or be None (no legend) - colors: list of colors for the different score arrays, must have same length as scores or be None (default colors)","title":"plot_score_dist_multi"},{"location":"utils/plot_utils/#plot95score95ls","text":"full signature: def plot_score_ls( thisscore, refscores, fig=None, ax=None, thislabel='This LS', thiscolor='black', reflabel='Reference LS', refcolor='dodgerblue', **kwargs ) comments: make a plot of the score for a single lumisection comparing to some reference distribution","title":"plot_score_ls"},{"location":"utils/plot_utils/#plot95metric","text":"full signature: def plot_metric( wprange, metric, label=None, color=None, sig_eff=None, sig_label=None, sig_color=None, bck_eff=None, bck_label=None, bck_color=None, title=None, xaxtitle='working point', yaxlog=False, ymaxfactor=1.3, yaxtitle='metric' ) comments: plot a metric based on signal and background efficiencies. along with the metric, the actual signal and background efficiencies can be plotted as well. input arguments: - wprange, metric: equally long 1D-numpy arrays, x- and y-data respectively - label: label for the metric to put in the legend - color: color for the metric (default: blue) - sig_eff: 1D-numpy array of signal efficiencies corresponding to wprange - sig_label: label for sig_eff in the legend - color: color for sig_eff (default: green) - bck_eff, bck_label, bck_color: same as for signal - title, xaxtitle and yaxtitle: titles for the plot and axes - yaxlog: boolean whether to put y axis in log scale - ymaxfactor: factor to add extra space on the top of the plot (for the legend)","title":"plot_metric"},{"location":"utils/plot_utils/#plot95roc","text":"full signature: def plot_roc( sig_eff, bkg_eff, auc=None, sig_eff_unc=None, color='b', title=None, titlesize=None, xaxtitle='Background efficiency', xaxtitlesize=None, yaxtitle='Signal efficiency', yaxtitlesize=None, xaxlog=True, yaxlog=False, xlims='auto', ylims='auto', dogrid=True, ticksize=None, doshow=True ) comments: note: automatic determination of xlims and ylims assumes log scale for x-axis and lin scale for y-axis; might not work properly in other cases and ranges should be provided manually.","title":"plot_roc"},{"location":"utils/plot_utils/#plot95confusion95matrix","text":"full signature: def plot_confusion_matrix( tp, tn, fp, fn, true_positive_label='Good', true_negative_label='Anomalous', pred_positive_label='Predicted good', pred_negative_label='Predicted anomalous', xaxlabelsize=None, yaxlabelsize=None, textsize=None, colormap='Blues', colortitle=None ) comments: (no valid documentation found)","title":"plot_confusion_matrix"},{"location":"utils/plot_utils/#color","text":"full signature: def color(value) comments: (no valid documentation found)","title":"color"},{"location":"utils/plot_utils/#valstr","text":"full signature: def valstr(value) comments: (no valid documentation found)","title":"valstr"},{"location":"utils/plot_utils/#clip95scores","text":"full signature: def clip_scores( scores ) comments: clip +-inf values in scores local copy of the same functions in autoencoder_utils.py (need to copy here locally to use in plot_fit_2d and plot_fit_1d without circular import...)","title":"clip_scores"},{"location":"utils/plot_utils/#plot95fit952d","text":"full signature: def plot_fit_2d( points, fitfunc=None, figsize=(10,8), logprob=False, clipprob=False, onlycontour=False, xlims=5, ylims=5, onlypositive=False, xaxtitle=None, xaxtitlesize=None, yaxtitle=None, yaxtitlesize=None, title=None, titlesize=None, caxtitle=None, caxtitlesize=None, caxrange=None, transparency=1, ticksize=None ) comments: make a 2D scatter plot of a point cloud with fitted contour input arguments: - points: a numpy array of shape (npoints,ndims), where ndims is supposed to be 2 - fitfunc: an object of type CloudFitter (see src/cloudfitters) or any other object that implements a pdf(points) method - logprob: boolean whether to plot log probability or normal probability - clipprob: boolean whether to replace +- inf values by (non-inf) max and min - onlycontour: a boolean whether to draw only the fit or include the data points - xlims and ylims: tuples of (low,high) note: can be an integer, in which case the range will be determined automatically from the formula low = mean-xlims*std, high = mean+xlims*std, where mean and std are determined from the points array. - onlypositive: overrides previous argument to set lower bound of plotting range at 0 in both dimensions. - xaxtitle and yaxtitle: titles for axes.","title":"plot_fit_2d"},{"location":"utils/plot_utils/#plot95fit952d95clusters","text":"full signature: def plot_fit_2d_clusters( points, clusters, figsize=(12,8), labels=None, colors=None, legendmarkerscale=1., legendsize=10, legendloc='best', legendbbox=None, **kwargs ) comments: make a 2D scatter plot of a fitted contour with point clouds superimposed input arguments: - points: numpy arrays of shape (npoints,ndims), where ndims is supposed to be 2, usually the points to which the fit was done note: only used to determine plotting range, these points are not plotted! - clusters: list of numpy arrays of shape (npoints,ndims), where ndims is supposed to be 2, clouds of points to plot - labels: list with legend entries (must be same length as clusters) - colors: list with colors (must be same length as clusters) - kwargs: passed down to plot_fit_2d note: onlycontour is set automatically and should not be in kwargs","title":"plot_fit_2d_clusters"},{"location":"utils/plot_utils/#plot95fit951d","text":"full signature: def plot_fit_1d( points, fitfunc=None, logprob=False, clipprob=False, onlycontour=False, xlims=5, onlypositive=False, xaxtitle=None, xaxtitlesize=None, yaxtitle=None, yaxtitlesize=None, title=None, titlesize=None ) comments: make a 1D scatter plot of a point cloud with fitted contour input arguments: - points: a numpy array of shape (npoints,ndims), where ndims is supposed to be 1 - fitfunc: an object of type CloudFitter (see src/cloudfitters) or any other object that implements a pdf(points) method - logprob: boolean whether to plot log probability or normal probability - clipprob: boolean whether to replace +- inf values by (non-inf) max and min - onlycontour: a boolean whether to draw only the fit or include the data points - xlims: tuple of the form (low,high) note: can be an integer, in which case the range will be determined automatically from the formula low = mean-xlims*std, high = mean+xlims*std, where mean and std are determined from the points array. - onlypositive: set lower bound of plotting range at 0 (overrides xlims) - xaxtitle and yaxtitle: titles for axes.","title":"plot_fit_1d"},{"location":"utils/plot_utils/#plot95fit951d95clusters","text":"full signature: def plot_fit_1d_clusters( points, clusters, labels=None, colors=None, **kwargs ) comments: make a 1D scatter plot of a fitted contour with point clouds superimposed input arguments: - points: numpy arrays of shape (npoints,ndims), where ndims is supposed to be 1, usually the points to which the fit was done note: only used to determine plotting range, these points are not plotted! - clusters: list of numpy arrays of shape (npoints,ndims), where ndims is supposed to be 1, clouds of points to plot - labels: list with legend entries (must be same length as clusters) - colors: list with colors (must be same length as clusters) - kwargs: passed down to plot_fit_1d note: onlycontour is set automatically and should not be in kwargs","title":"plot_fit_1d_clusters"},{"location":"utils/refruns_utils/","text":"refruns utils Tools for retrieving a reference run for a given run Preliminary implementation, based on a json file generated by the Tracker DQM group. Retrieved from here . Perhaps modify code later to fetch the up-to-date version at runtime instead of having to download a new version. Maybe also using an API to retrieve only the requested run instead of loading the entire file into memory. get_reference_run full signature: def get_reference_run( runnb, jsonlist=None, jsonfile='json_allRunsRefRuns.json' ) comments: get the reference run for a given run number input arguments: - runnb: integer representing a run number. - jsonlist: list matching run numbers to reference run numbers. note: the list is supposed to contain dicts with keys 'run_number' and 'reference_run_number', this convention is based on the json file provided by the tracker group. note: if jsonlist is None, jsonfile (see below) will be opened and a jsonlist read from it. - jsonfile: path to json file matching run numbers to reference run numbers. note: the json file must contain a list of dicts with keys 'run_number' and 'reference_run_number', as explained above. note: ignored if jsonlist is not None. output: integer representing the reference run number for the given run. if the given run is not in the json, -1 is returned.","title":"refruns_utils"},{"location":"utils/refruns_utils/#refruns-utils","text":"Tools for retrieving a reference run for a given run Preliminary implementation, based on a json file generated by the Tracker DQM group. Retrieved from here . Perhaps modify code later to fetch the up-to-date version at runtime instead of having to download a new version. Maybe also using an API to retrieve only the requested run instead of loading the entire file into memory.","title":"refruns utils"},{"location":"utils/refruns_utils/#get95reference95run","text":"full signature: def get_reference_run( runnb, jsonlist=None, jsonfile='json_allRunsRefRuns.json' ) comments: get the reference run for a given run number input arguments: - runnb: integer representing a run number. - jsonlist: list matching run numbers to reference run numbers. note: the list is supposed to contain dicts with keys 'run_number' and 'reference_run_number', this convention is based on the json file provided by the tracker group. note: if jsonlist is None, jsonfile (see below) will be opened and a jsonlist read from it. - jsonfile: path to json file matching run numbers to reference run numbers. note: the json file must contain a list of dicts with keys 'run_number' and 'reference_run_number', as explained above. note: ignored if jsonlist is not None. output: integer representing the reference run number for the given run. if the given run is not in the json, -1 is returned.","title":"get_reference_run"}]}